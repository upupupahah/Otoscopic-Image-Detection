2025-05-05T00:43:46,859 [DEBUG] main org.pytorch.serve.util.ConfigManager - xpu-smi not available or failed: Cannot run program "xpu-smi": error=2, No such file or directory
2025-05-05T00:43:46,859 [DEBUG] main org.pytorch.serve.util.ConfigManager - xpu-smi not available or failed: Cannot run program "xpu-smi": error=2, No such file or directory
2025-05-05T00:43:46,923 [WARN ] main org.pytorch.serve.util.ConfigManager - Your torchserve instance can access any URL to load models. When deploying to production, make sure to limit the set of allowed_urls in config.properties
2025-05-05T00:43:46,923 [WARN ] main org.pytorch.serve.util.ConfigManager - Your torchserve instance can access any URL to load models. When deploying to production, make sure to limit the set of allowed_urls in config.properties
2025-05-05T00:43:47,580 [INFO ] main org.pytorch.serve.servingsdk.impl.PluginsManager - Initializing plugins manager...
2025-05-05T00:43:47,580 [INFO ] main org.pytorch.serve.servingsdk.impl.PluginsManager - Initializing plugins manager...
2025-05-05T00:43:48,081 [INFO ] main org.pytorch.serve.metrics.configuration.MetricConfiguration - Successfully loaded metrics configuration from /home/pupu/Desktop/project/.venv/lib/python3.12/site-packages/ts/configs/metrics.yaml
2025-05-05T00:43:48,081 [INFO ] main org.pytorch.serve.metrics.configuration.MetricConfiguration - Successfully loaded metrics configuration from /home/pupu/Desktop/project/.venv/lib/python3.12/site-packages/ts/configs/metrics.yaml
2025-05-05T00:43:48,473 [INFO ] main org.pytorch.serve.ModelServer - 
Torchserve version: 0.12.0
TS Home: /home/pupu/Desktop/project/.venv/lib/python3.12/site-packages
Current directory: /home/pupu/Desktop/project
Temp directory: /tmp
Metrics config path: /home/pupu/Desktop/project/.venv/lib/python3.12/site-packages/ts/configs/metrics.yaml
Number of GPUs: 1
Number of CPUs: 8
Max heap size: 3990 M
Python executable: /home/pupu/Desktop/project/.venv/bin/python
Config file: N/A
Inference address: http://127.0.0.1:8080
Management address: http://127.0.0.1:8081
Metrics address: http://127.0.0.1:8082
Model Store: /home/pupu/Desktop/project/there_is_our_server
Initial Models: my_model=my_model.mar
Log dir: /home/pupu/Desktop/project/logs
Metrics dir: /home/pupu/Desktop/project/logs
Netty threads: 0
Netty client threads: 0
Default workers per model: 1
Blacklist Regex: N/A
Maximum Response Size: 6553500
Maximum Request Size: 6553500
Limit Maximum Image Pixels: true
Prefer direct buffer: false
Allowed Urls: [file://.*|http(s)?://.*]
Custom python dependency for model allowed: false
Enable metrics API: true
Metrics mode: LOG
Disable system metrics: false
Workflow Store: /home/pupu/Desktop/project/there_is_our_server
CPP log config: N/A
Model config: N/A
System metrics command: default
Model API enabled: false
2025-05-05T00:43:48,473 [INFO ] main org.pytorch.serve.ModelServer - 
Torchserve version: 0.12.0
TS Home: /home/pupu/Desktop/project/.venv/lib/python3.12/site-packages
Current directory: /home/pupu/Desktop/project
Temp directory: /tmp
Metrics config path: /home/pupu/Desktop/project/.venv/lib/python3.12/site-packages/ts/configs/metrics.yaml
Number of GPUs: 1
Number of CPUs: 8
Max heap size: 3990 M
Python executable: /home/pupu/Desktop/project/.venv/bin/python
Config file: N/A
Inference address: http://127.0.0.1:8080
Management address: http://127.0.0.1:8081
Metrics address: http://127.0.0.1:8082
Model Store: /home/pupu/Desktop/project/there_is_our_server
Initial Models: my_model=my_model.mar
Log dir: /home/pupu/Desktop/project/logs
Metrics dir: /home/pupu/Desktop/project/logs
Netty threads: 0
Netty client threads: 0
Default workers per model: 1
Blacklist Regex: N/A
Maximum Response Size: 6553500
Maximum Request Size: 6553500
Limit Maximum Image Pixels: true
Prefer direct buffer: false
Allowed Urls: [file://.*|http(s)?://.*]
Custom python dependency for model allowed: false
Enable metrics API: true
Metrics mode: LOG
Disable system metrics: false
Workflow Store: /home/pupu/Desktop/project/there_is_our_server
CPP log config: N/A
Model config: N/A
System metrics command: default
Model API enabled: false
2025-05-05T00:43:48,529 [INFO ] main org.pytorch.serve.servingsdk.impl.PluginsManager -  Loading snapshot serializer plugin...
2025-05-05T00:43:48,529 [INFO ] main org.pytorch.serve.servingsdk.impl.PluginsManager -  Loading snapshot serializer plugin...
2025-05-05T00:43:48,571 [INFO ] main org.pytorch.serve.ModelServer - Loading initial models: my_model.mar
2025-05-05T00:43:48,571 [INFO ] main org.pytorch.serve.ModelServer - Loading initial models: my_model.mar
2025-05-05T00:43:49,988 [DEBUG] main org.pytorch.serve.wlm.ModelVersionedRefs - Adding new version 1.0 for model my_model
2025-05-05T00:43:49,988 [DEBUG] main org.pytorch.serve.wlm.ModelVersionedRefs - Adding new version 1.0 for model my_model
2025-05-05T00:43:49,988 [DEBUG] main org.pytorch.serve.wlm.ModelVersionedRefs - Setting default version to 1.0 for model my_model
2025-05-05T00:43:49,988 [DEBUG] main org.pytorch.serve.wlm.ModelVersionedRefs - Setting default version to 1.0 for model my_model
2025-05-05T00:43:49,989 [INFO ] main org.pytorch.serve.wlm.ModelManager - Model my_model loaded.
2025-05-05T00:43:49,989 [INFO ] main org.pytorch.serve.wlm.ModelManager - Model my_model loaded.
2025-05-05T00:43:49,989 [DEBUG] main org.pytorch.serve.wlm.ModelManager - updateModel: my_model, count: 1
2025-05-05T00:43:49,989 [DEBUG] main org.pytorch.serve.wlm.ModelManager - updateModel: my_model, count: 1
2025-05-05T00:43:50,024 [DEBUG] W-9000-my_model_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/home/pupu/Desktop/project/.venv/bin/python, /home/pupu/Desktop/project/.venv/lib/python3.12/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9000, --metrics-config, /home/pupu/Desktop/project/.venv/lib/python3.12/site-packages/ts/configs/metrics.yaml]
2025-05-05T00:43:50,024 [DEBUG] W-9000-my_model_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/home/pupu/Desktop/project/.venv/bin/python, /home/pupu/Desktop/project/.venv/lib/python3.12/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9000, --metrics-config, /home/pupu/Desktop/project/.venv/lib/python3.12/site-packages/ts/configs/metrics.yaml]
2025-05-05T00:43:50,036 [INFO ] main org.pytorch.serve.ModelServer - Initialize Inference server with: EpollServerSocketChannel.
2025-05-05T00:43:50,036 [INFO ] main org.pytorch.serve.ModelServer - Initialize Inference server with: EpollServerSocketChannel.
2025-05-05T00:43:50,261 [INFO ] main org.pytorch.serve.ModelServer - Inference API bind to: http://127.0.0.1:8080
2025-05-05T00:43:50,261 [INFO ] main org.pytorch.serve.ModelServer - Inference API bind to: http://127.0.0.1:8080
2025-05-05T00:43:50,262 [INFO ] main org.pytorch.serve.ModelServer - Initialize Management server with: EpollServerSocketChannel.
2025-05-05T00:43:50,262 [INFO ] main org.pytorch.serve.ModelServer - Initialize Management server with: EpollServerSocketChannel.
2025-05-05T00:43:50,264 [INFO ] main org.pytorch.serve.ModelServer - Management API bind to: http://127.0.0.1:8081
2025-05-05T00:43:50,264 [INFO ] main org.pytorch.serve.ModelServer - Management API bind to: http://127.0.0.1:8081
2025-05-05T00:43:50,264 [INFO ] main org.pytorch.serve.ModelServer - Initialize Metrics server with: EpollServerSocketChannel.
2025-05-05T00:43:50,264 [INFO ] main org.pytorch.serve.ModelServer - Initialize Metrics server with: EpollServerSocketChannel.
2025-05-05T00:43:50,265 [INFO ] main org.pytorch.serve.ModelServer - Metrics API bind to: http://127.0.0.1:8082
2025-05-05T00:43:50,265 [INFO ] main org.pytorch.serve.ModelServer - Metrics API bind to: http://127.0.0.1:8082
2025-05-05T00:43:52,665 [WARN ] pool-3-thread-1 org.pytorch.serve.metrics.MetricCollector - worker pid is not available yet.
2025-05-05T00:43:52,665 [WARN ] pool-3-thread-1 org.pytorch.serve.metrics.MetricCollector - worker pid is not available yet.
2025-05-05T00:43:58,693 [INFO ] pool-3-thread-1 TS_METRICS - CPUUtilization.Percent:50.0|#Level:Host|#hostname:pupu-Stvullinc,timestamp:1746387838
2025-05-05T00:43:58,696 [INFO ] pool-3-thread-1 TS_METRICS - DiskAvailable.Gigabytes:75.47428131103516|#Level:Host|#hostname:pupu-Stvullinc,timestamp:1746387838
2025-05-05T00:43:58,696 [INFO ] pool-3-thread-1 TS_METRICS - DiskUsage.Gigabytes:62.70756530761719|#Level:Host|#hostname:pupu-Stvullinc,timestamp:1746387838
2025-05-05T00:43:58,696 [INFO ] pool-3-thread-1 TS_METRICS - DiskUtilization.Percent:45.4|#Level:Host|#hostname:pupu-Stvullinc,timestamp:1746387838
2025-05-05T00:43:58,697 [INFO ] pool-3-thread-1 TS_METRICS - GPUMemoryUtilization.Percent:4.8583984375|#Level:Host,DeviceId:0|#hostname:pupu-Stvullinc,timestamp:1746387838
2025-05-05T00:43:58,697 [INFO ] pool-3-thread-1 TS_METRICS - GPUMemoryUsed.Megabytes:398.0|#Level:Host,DeviceId:0|#hostname:pupu-Stvullinc,timestamp:1746387838
2025-05-05T00:43:58,697 [INFO ] pool-3-thread-1 TS_METRICS - GPUUtilization.Percent:14.0|#Level:Host,DeviceId:0|#hostname:pupu-Stvullinc,timestamp:1746387838
2025-05-05T00:43:58,698 [INFO ] pool-3-thread-1 TS_METRICS - MemoryAvailable.Megabytes:9775.0390625|#Level:Host|#hostname:pupu-Stvullinc,timestamp:1746387838
2025-05-05T00:43:58,698 [INFO ] pool-3-thread-1 TS_METRICS - MemoryUsed.Megabytes:5735.3125|#Level:Host|#hostname:pupu-Stvullinc,timestamp:1746387838
2025-05-05T00:43:58,698 [INFO ] pool-3-thread-1 TS_METRICS - MemoryUtilization.Percent:38.7|#Level:Host|#hostname:pupu-Stvullinc,timestamp:1746387838
2025-05-05T00:44:03,551 [INFO ] W-9000-my_model_1.0-stdout MODEL_LOG - s_name_part0=/tmp/.ts.sock, s_name_part1=9000, pid=8697
2025-05-05T00:44:03,555 [INFO ] W-9000-my_model_1.0-stdout MODEL_LOG - Listening on port: /tmp/.ts.sock.9000
2025-05-05T00:44:03,555 [INFO ] W-9000-my_model_1.0-stdout MODEL_LOG - Successfully loaded /home/pupu/Desktop/project/.venv/lib/python3.12/site-packages/ts/configs/metrics.yaml.
2025-05-05T00:44:03,556 [INFO ] W-9000-my_model_1.0-stdout MODEL_LOG - [PID]8697
2025-05-05T00:44:03,556 [INFO ] W-9000-my_model_1.0-stdout MODEL_LOG - Torch worker started.
2025-05-05T00:44:03,557 [INFO ] W-9000-my_model_1.0-stdout MODEL_LOG - Python runtime: 3.12.3
2025-05-05T00:44:03,557 [DEBUG] W-9000-my_model_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-my_model_1.0 State change null -> WORKER_STARTED
2025-05-05T00:44:03,557 [DEBUG] W-9000-my_model_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-my_model_1.0 State change null -> WORKER_STARTED
2025-05-05T00:44:03,587 [INFO ] W-9000-my_model_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9000
2025-05-05T00:44:03,587 [INFO ] W-9000-my_model_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9000
2025-05-05T00:44:03,597 [INFO ] W-9000-my_model_1.0-stdout MODEL_LOG - Connection accepted: /tmp/.ts.sock.9000.
2025-05-05T00:44:03,599 [DEBUG] W-9000-my_model_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req.cmd LOAD repeats 1 to backend at: 1746387843599
2025-05-05T00:44:03,599 [DEBUG] W-9000-my_model_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req.cmd LOAD repeats 1 to backend at: 1746387843599
2025-05-05T00:44:03,601 [INFO ] W-9000-my_model_1.0 org.pytorch.serve.wlm.WorkerThread - Looping backend response at: 1746387843601
2025-05-05T00:44:03,601 [INFO ] W-9000-my_model_1.0 org.pytorch.serve.wlm.WorkerThread - Looping backend response at: 1746387843601
2025-05-05T00:44:03,646 [INFO ] W-9000-my_model_1.0-stdout MODEL_LOG - model_name: my_model, batchSize: 1
2025-05-05T00:44:09,852 [INFO ] W-9000-my_model_1.0-stdout MODEL_LOG - OpenVINO is not enabled
2025-05-05T00:44:09,853 [INFO ] W-9000-my_model_1.0-stdout MODEL_LOG - proceeding without onnxruntime
2025-05-05T00:44:09,853 [INFO ] W-9000-my_model_1.0-stdout MODEL_LOG - Torch TensorRT not enabled
2025-05-05T00:44:09,909 [INFO ] W-9000-my_model_1.0-stdout MODEL_LOG - Failed to load model my_model, exception No model weights could be loaded
2025-05-05T00:44:09,910 [INFO ] W-9000-my_model_1.0-stdout MODEL_LOG - Traceback (most recent call last):
2025-05-05T00:44:09,910 [INFO ] W-9000-my_model_1.0-stdout MODEL_LOG -   File "/home/pupu/Desktop/project/.venv/lib/python3.12/site-packages/ts/model_service_worker.py", line 133, in load_model
2025-05-05T00:44:09,911 [INFO ] W-9000-my_model_1.0-stdout MODEL_LOG -     service = model_loader.load(
2025-05-05T00:44:09,911 [INFO ] W-9000-my_model_1.0-stdout MODEL_LOG -               ^^^^^^^^^^^^^^^^^^
2025-05-05T00:44:09,911 [INFO ] W-9000-my_model_1.0-stdout MODEL_LOG -   File "/home/pupu/Desktop/project/.venv/lib/python3.12/site-packages/ts/model_loader.py", line 143, in load
2025-05-05T00:44:09,911 [INFO ] W-9000-my_model_1.0-stdout MODEL_LOG -     initialize_fn(service.context)
2025-05-05T00:44:09,912 [INFO ] W-9000-my_model_1.0-stdout MODEL_LOG -   File "/tmp/models/1595b2e1bad84d53b7dc47d2998fd305/handler.py", line 19, in initialize
2025-05-05T00:44:09,912 [INFO ] W-9000-my_model_1.0-stdout MODEL_LOG -     super().initialize(context)
2025-05-05T00:44:09,913 [INFO ] W-9000-my_model_1.0-stdout MODEL_LOG -   File "/home/pupu/Desktop/project/.venv/lib/python3.12/site-packages/ts/torch_handler/base_handler.py", line 223, in initialize
2025-05-05T00:44:09,913 [INFO ] W-9000-my_model_1.0-stdout MODEL_LOG -     raise RuntimeError("No model weights could be loaded")
2025-05-05T00:44:09,913 [INFO ] W-9000-my_model_1.0-stdout MODEL_LOG - RuntimeError: No model weights could be loaded
2025-05-05T00:44:09,913 [INFO ] W-9000-my_model_1.0-stdout MODEL_LOG - Backend worker process died.
2025-05-05T00:44:09,914 [INFO ] W-9000-my_model_1.0-stdout MODEL_LOG - Traceback (most recent call last):
2025-05-05T00:44:09,914 [INFO ] W-9000-my_model_1.0-stdout MODEL_LOG -   File "/home/pupu/Desktop/project/.venv/lib/python3.12/site-packages/ts/model_service_worker.py", line 301, in <module>
2025-05-05T00:44:09,914 [INFO ] W-9000-my_model_1.0-stdout MODEL_LOG -     worker.run_server()
2025-05-05T00:44:09,914 [INFO ] W-9000-my_model_1.0-stdout MODEL_LOG -   File "/home/pupu/Desktop/project/.venv/lib/python3.12/site-packages/ts/model_service_worker.py", line 268, in run_server
2025-05-05T00:44:09,915 [INFO ] W-9000-my_model_1.0-stdout MODEL_LOG -     self.handle_connection(cl_socket)
2025-05-05T00:44:09,915 [INFO ] W-9000-my_model_1.0-stdout MODEL_LOG -   File "/home/pupu/Desktop/project/.venv/lib/python3.12/site-packages/ts/model_service_worker.py", line 201, in handle_connection
2025-05-05T00:44:09,915 [INFO ] W-9000-my_model_1.0-stdout MODEL_LOG -     raise RuntimeError("{} - {}".format(code, result))
2025-05-05T00:44:09,915 [INFO ] W-9000-my_model_1.0-stdout MODEL_LOG - RuntimeError: 500 - Unknown exception
2025-05-05T00:44:09,950 [INFO ] W-9000-my_model_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 6349
2025-05-05T00:44:09,950 [INFO ] W-9000-my_model_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 6349
2025-05-05T00:44:09,951 [INFO ] epollEventLoopGroup-5-1 org.pytorch.serve.wlm.WorkerThread - 9000 Worker disconnected. WORKER_STARTED
2025-05-05T00:44:09,951 [DEBUG] W-9000-my_model_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-my_model_1.0 State change WORKER_STARTED -> WORKER_ERROR
2025-05-05T00:44:09,951 [DEBUG] W-9000-my_model_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-my_model_1.0 State change WORKER_STARTED -> WORKER_ERROR
2025-05-05T00:44:09,951 [INFO ] epollEventLoopGroup-5-1 org.pytorch.serve.wlm.WorkerThread - 9000 Worker disconnected. WORKER_STARTED
2025-05-05T00:44:09,951 [INFO ] W-9000-my_model_1.0 TS_METRICS - WorkerThreadTime.Milliseconds:3.0|#Level:Host|#hostname:pupu-Stvullinc,timestamp:1746387849
2025-05-05T00:44:09,952 [DEBUG] W-9000-my_model_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_ERROR
2025-05-05T00:44:09,952 [DEBUG] W-9000-my_model_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_ERROR
2025-05-05T00:44:09,952 [DEBUG] W-9000-my_model_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died., responseTimeout:120sec
java.lang.InterruptedException: null
	at java.util.concurrent.locks.ReentrantLock$Sync.lockInterruptibly(ReentrantLock.java:159) ~[?:?]
	at java.util.concurrent.locks.ReentrantLock.lockInterruptibly(ReentrantLock.java:372) ~[?:?]
	at org.pytorch.serve.wlm.Model.pollBatch(Model.java:478) ~[model-server.jar:?]
	at org.pytorch.serve.wlm.BatchAggregator.pollBatch(BatchAggregator.java:189) ~[model-server.jar:?]
	at org.pytorch.serve.wlm.BatchAggregator.getRequest(BatchAggregator.java:36) ~[model-server.jar:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:195) ~[model-server.jar:?]
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:572) ~[?:?]
	at java.util.concurrent.FutureTask.run(FutureTask.java:317) ~[?:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144) ~[?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642) ~[?:?]
	at java.lang.Thread.run(Thread.java:1583) [?:?]
2025-05-05T00:44:09,952 [DEBUG] W-9000-my_model_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died., responseTimeout:120sec
java.lang.InterruptedException: null
	at java.util.concurrent.locks.ReentrantLock$Sync.lockInterruptibly(ReentrantLock.java:159) ~[?:?]
	at java.util.concurrent.locks.ReentrantLock.lockInterruptibly(ReentrantLock.java:372) ~[?:?]
	at org.pytorch.serve.wlm.Model.pollBatch(Model.java:478) ~[model-server.jar:?]
	at org.pytorch.serve.wlm.BatchAggregator.pollBatch(BatchAggregator.java:189) ~[model-server.jar:?]
	at org.pytorch.serve.wlm.BatchAggregator.getRequest(BatchAggregator.java:36) ~[model-server.jar:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:195) ~[model-server.jar:?]
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:572) ~[?:?]
	at java.util.concurrent.FutureTask.run(FutureTask.java:317) ~[?:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144) ~[?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642) ~[?:?]
	at java.lang.Thread.run(Thread.java:1583) [?:?]
2025-05-05T00:44:09,982 [DEBUG] W-9000-my_model_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-my_model_1.0 State change WORKER_ERROR -> WORKER_STOPPED
2025-05-05T00:44:09,982 [DEBUG] W-9000-my_model_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-my_model_1.0 State change WORKER_ERROR -> WORKER_STOPPED
2025-05-05T00:44:09,982 [INFO ] W-9000-my_model_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery start timestamp: 1746387849982
2025-05-05T00:44:09,982 [INFO ] W-9000-my_model_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery start timestamp: 1746387849982
2025-05-05T00:44:09,983 [INFO ] W-9000-my_model_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9000 in 1 seconds.
2025-05-05T00:44:09,983 [INFO ] W-9000-my_model_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9000 in 1 seconds.
2025-05-05T00:44:10,023 [INFO ] W-9000-my_model_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-my_model_1.0-stdout
2025-05-05T00:44:10,023 [INFO ] W-9000-my_model_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-my_model_1.0-stdout
2025-05-05T00:44:10,023 [INFO ] W-9000-my_model_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-my_model_1.0-stderr
2025-05-05T00:44:10,023 [INFO ] W-9000-my_model_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-my_model_1.0-stderr
2025-05-05T00:44:10,984 [DEBUG] W-9000-my_model_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/home/pupu/Desktop/project/.venv/bin/python, /home/pupu/Desktop/project/.venv/lib/python3.12/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9000, --metrics-config, /home/pupu/Desktop/project/.venv/lib/python3.12/site-packages/ts/configs/metrics.yaml]
2025-05-05T00:44:10,984 [DEBUG] W-9000-my_model_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/home/pupu/Desktop/project/.venv/bin/python, /home/pupu/Desktop/project/.venv/lib/python3.12/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9000, --metrics-config, /home/pupu/Desktop/project/.venv/lib/python3.12/site-packages/ts/configs/metrics.yaml]
2025-05-05T00:44:12,796 [INFO ] W-9000-my_model_1.0-stdout MODEL_LOG - s_name_part0=/tmp/.ts.sock, s_name_part1=9000, pid=8750
2025-05-05T00:44:12,797 [INFO ] W-9000-my_model_1.0-stdout MODEL_LOG - Listening on port: /tmp/.ts.sock.9000
2025-05-05T00:44:12,805 [INFO ] W-9000-my_model_1.0-stdout MODEL_LOG - Successfully loaded /home/pupu/Desktop/project/.venv/lib/python3.12/site-packages/ts/configs/metrics.yaml.
2025-05-05T00:44:12,805 [INFO ] W-9000-my_model_1.0-stdout MODEL_LOG - [PID]8750
2025-05-05T00:44:12,805 [DEBUG] W-9000-my_model_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-my_model_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2025-05-05T00:44:12,805 [INFO ] W-9000-my_model_1.0-stdout MODEL_LOG - Torch worker started.
2025-05-05T00:44:12,805 [DEBUG] W-9000-my_model_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-my_model_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2025-05-05T00:44:12,806 [INFO ] W-9000-my_model_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9000
2025-05-05T00:44:12,806 [INFO ] W-9000-my_model_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9000
2025-05-05T00:44:12,806 [INFO ] W-9000-my_model_1.0-stdout MODEL_LOG - Python runtime: 3.12.3
2025-05-05T00:44:12,808 [DEBUG] W-9000-my_model_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req.cmd LOAD repeats 1 to backend at: 1746387852808
2025-05-05T00:44:12,808 [INFO ] W-9000-my_model_1.0-stdout MODEL_LOG - Connection accepted: /tmp/.ts.sock.9000.
2025-05-05T00:44:12,808 [DEBUG] W-9000-my_model_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req.cmd LOAD repeats 1 to backend at: 1746387852808
2025-05-05T00:44:12,808 [INFO ] W-9000-my_model_1.0 org.pytorch.serve.wlm.WorkerThread - Looping backend response at: 1746387852808
2025-05-05T00:44:12,808 [INFO ] W-9000-my_model_1.0 org.pytorch.serve.wlm.WorkerThread - Looping backend response at: 1746387852808
2025-05-05T00:44:12,828 [INFO ] W-9000-my_model_1.0-stdout MODEL_LOG - model_name: my_model, batchSize: 1
2025-05-05T00:44:14,175 [INFO ] W-9000-my_model_1.0-stdout MODEL_LOG - OpenVINO is not enabled
2025-05-05T00:44:14,176 [INFO ] W-9000-my_model_1.0-stdout MODEL_LOG - proceeding without onnxruntime
2025-05-05T00:44:14,176 [INFO ] W-9000-my_model_1.0-stdout MODEL_LOG - Torch TensorRT not enabled
2025-05-05T00:44:14,177 [INFO ] W-9000-my_model_1.0-stdout MODEL_LOG - Failed to load model my_model, exception No model weights could be loaded
2025-05-05T00:44:14,177 [INFO ] W-9000-my_model_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 1369
2025-05-05T00:44:14,177 [INFO ] W-9000-my_model_1.0-stdout MODEL_LOG - Traceback (most recent call last):
2025-05-05T00:44:14,177 [INFO ] W-9000-my_model_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 1369
2025-05-05T00:44:14,177 [DEBUG] W-9000-my_model_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-my_model_1.0 State change WORKER_STARTED -> WORKER_ERROR
2025-05-05T00:44:14,177 [DEBUG] W-9000-my_model_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-my_model_1.0 State change WORKER_STARTED -> WORKER_ERROR
2025-05-05T00:44:14,177 [INFO ] W-9000-my_model_1.0-stdout MODEL_LOG -   File "/home/pupu/Desktop/project/.venv/lib/python3.12/site-packages/ts/model_service_worker.py", line 133, in load_model
2025-05-05T00:44:14,177 [INFO ] W-9000-my_model_1.0-stdout MODEL_LOG -     service = model_loader.load(
2025-05-05T00:44:14,178 [INFO ] W-9000-my_model_1.0 TS_METRICS - WorkerThreadTime.Milliseconds:0.0|#Level:Host|#hostname:pupu-Stvullinc,timestamp:1746387854
2025-05-05T00:44:14,177 [INFO ] epollEventLoopGroup-5-2 org.pytorch.serve.wlm.WorkerThread - 9000 Worker disconnected. WORKER_STARTED
2025-05-05T00:44:14,178 [INFO ] W-9000-my_model_1.0-stdout MODEL_LOG -               ^^^^^^^^^^^^^^^^^^
2025-05-05T00:44:14,177 [INFO ] epollEventLoopGroup-5-2 org.pytorch.serve.wlm.WorkerThread - 9000 Worker disconnected. WORKER_STARTED
2025-05-05T00:44:14,178 [INFO ] W-9000-my_model_1.0-stdout MODEL_LOG -   File "/home/pupu/Desktop/project/.venv/lib/python3.12/site-packages/ts/model_loader.py", line 143, in load
2025-05-05T00:44:14,178 [DEBUG] W-9000-my_model_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_ERROR
2025-05-05T00:44:14,178 [INFO ] W-9000-my_model_1.0-stdout MODEL_LOG -     initialize_fn(service.context)
2025-05-05T00:44:14,178 [DEBUG] W-9000-my_model_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_ERROR
2025-05-05T00:44:14,178 [INFO ] W-9000-my_model_1.0-stdout MODEL_LOG -   File "/tmp/models/1595b2e1bad84d53b7dc47d2998fd305/handler.py", line 19, in initialize
2025-05-05T00:44:14,178 [INFO ] W-9000-my_model_1.0-stdout MODEL_LOG -     super().initialize(context)
2025-05-05T00:44:14,178 [DEBUG] W-9000-my_model_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died., responseTimeout:120sec
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1770) ~[?:?]
	at java.util.concurrent.LinkedBlockingDeque.pollFirst(LinkedBlockingDeque.java:515) ~[?:?]
	at java.util.concurrent.LinkedBlockingDeque.poll(LinkedBlockingDeque.java:677) ~[?:?]
	at org.pytorch.serve.wlm.Model.pollBatch(Model.java:482) ~[model-server.jar:?]
	at org.pytorch.serve.wlm.BatchAggregator.pollBatch(BatchAggregator.java:189) ~[model-server.jar:?]
	at org.pytorch.serve.wlm.BatchAggregator.getRequest(BatchAggregator.java:36) ~[model-server.jar:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:195) ~[model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144) ~[?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642) ~[?:?]
	at java.lang.Thread.run(Thread.java:1583) [?:?]
2025-05-05T00:44:14,179 [INFO ] W-9000-my_model_1.0-stdout MODEL_LOG -   File "/home/pupu/Desktop/project/.venv/lib/python3.12/site-packages/ts/torch_handler/base_handler.py", line 223, in initialize
2025-05-05T00:44:14,178 [DEBUG] W-9000-my_model_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died., responseTimeout:120sec
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1770) ~[?:?]
	at java.util.concurrent.LinkedBlockingDeque.pollFirst(LinkedBlockingDeque.java:515) ~[?:?]
	at java.util.concurrent.LinkedBlockingDeque.poll(LinkedBlockingDeque.java:677) ~[?:?]
	at org.pytorch.serve.wlm.Model.pollBatch(Model.java:482) ~[model-server.jar:?]
	at org.pytorch.serve.wlm.BatchAggregator.pollBatch(BatchAggregator.java:189) ~[model-server.jar:?]
	at org.pytorch.serve.wlm.BatchAggregator.getRequest(BatchAggregator.java:36) ~[model-server.jar:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:195) ~[model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144) ~[?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642) ~[?:?]
	at java.lang.Thread.run(Thread.java:1583) [?:?]
2025-05-05T00:44:14,179 [INFO ] W-9000-my_model_1.0-stdout MODEL_LOG -     raise RuntimeError("No model weights could be loaded")
2025-05-05T00:44:14,179 [DEBUG] W-9000-my_model_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-my_model_1.0 State change WORKER_ERROR -> WORKER_STOPPED
2025-05-05T00:44:14,179 [INFO ] W-9000-my_model_1.0-stdout MODEL_LOG - RuntimeError: No model weights could be loaded
2025-05-05T00:44:14,179 [DEBUG] W-9000-my_model_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-my_model_1.0 State change WORKER_ERROR -> WORKER_STOPPED
2025-05-05T00:44:14,179 [WARN ] W-9000-my_model_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery failed again
2025-05-05T00:44:14,179 [INFO ] W-9000-my_model_1.0-stdout MODEL_LOG - Backend worker process died.
2025-05-05T00:44:14,179 [WARN ] W-9000-my_model_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery failed again
2025-05-05T00:44:14,179 [INFO ] W-9000-my_model_1.0-stdout MODEL_LOG - Traceback (most recent call last):
2025-05-05T00:44:14,180 [INFO ] W-9000-my_model_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9000 in 1 seconds.
2025-05-05T00:44:14,180 [INFO ] W-9000-my_model_1.0-stdout MODEL_LOG -   File "/home/pupu/Desktop/project/.venv/lib/python3.12/site-packages/ts/model_service_worker.py", line 301, in <module>
2025-05-05T00:44:14,180 [INFO ] W-9000-my_model_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9000 in 1 seconds.
2025-05-05T00:44:14,180 [INFO ] W-9000-my_model_1.0-stdout MODEL_LOG -     worker.run_server()
2025-05-05T00:44:14,180 [INFO ] W-9000-my_model_1.0-stdout MODEL_LOG -   File "/home/pupu/Desktop/project/.venv/lib/python3.12/site-packages/ts/model_service_worker.py", line 268, in run_server
2025-05-05T00:44:14,180 [INFO ] W-9000-my_model_1.0-stdout MODEL_LOG -     self.handle_connection(cl_socket)
2025-05-05T00:44:14,180 [INFO ] W-9000-my_model_1.0-stdout MODEL_LOG -   File "/home/pupu/Desktop/project/.venv/lib/python3.12/site-packages/ts/model_service_worker.py", line 201, in handle_connection
2025-05-05T00:44:14,181 [INFO ] W-9000-my_model_1.0-stdout MODEL_LOG -     raise RuntimeError("{} - {}".format(code, result))
2025-05-05T00:44:14,181 [INFO ] W-9000-my_model_1.0-stdout MODEL_LOG - RuntimeError: 500 - Unknown exception
2025-05-05T00:44:14,181 [INFO ] W-9000-my_model_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-my_model_1.0-stdout
2025-05-05T00:44:14,181 [INFO ] W-9000-my_model_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-my_model_1.0-stdout
2025-05-05T00:44:14,240 [INFO ] W-9000-my_model_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-my_model_1.0-stderr
2025-05-05T00:44:14,240 [INFO ] W-9000-my_model_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-my_model_1.0-stderr
2025-05-05T00:44:15,181 [DEBUG] W-9000-my_model_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/home/pupu/Desktop/project/.venv/bin/python, /home/pupu/Desktop/project/.venv/lib/python3.12/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9000, --metrics-config, /home/pupu/Desktop/project/.venv/lib/python3.12/site-packages/ts/configs/metrics.yaml]
2025-05-05T00:44:15,181 [DEBUG] W-9000-my_model_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/home/pupu/Desktop/project/.venv/bin/python, /home/pupu/Desktop/project/.venv/lib/python3.12/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9000, --metrics-config, /home/pupu/Desktop/project/.venv/lib/python3.12/site-packages/ts/configs/metrics.yaml]
2025-05-05T00:44:16,928 [INFO ] W-9000-my_model_1.0-stdout MODEL_LOG - s_name_part0=/tmp/.ts.sock, s_name_part1=9000, pid=8772
2025-05-05T00:44:16,929 [INFO ] W-9000-my_model_1.0-stdout MODEL_LOG - Listening on port: /tmp/.ts.sock.9000
2025-05-05T00:44:16,938 [INFO ] W-9000-my_model_1.0-stdout MODEL_LOG - Successfully loaded /home/pupu/Desktop/project/.venv/lib/python3.12/site-packages/ts/configs/metrics.yaml.
2025-05-05T00:44:16,938 [INFO ] W-9000-my_model_1.0-stdout MODEL_LOG - [PID]8772
2025-05-05T00:44:16,939 [DEBUG] W-9000-my_model_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-my_model_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2025-05-05T00:44:16,939 [INFO ] W-9000-my_model_1.0-stdout MODEL_LOG - Torch worker started.
2025-05-05T00:44:16,939 [DEBUG] W-9000-my_model_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-my_model_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2025-05-05T00:44:16,939 [INFO ] W-9000-my_model_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9000
2025-05-05T00:44:16,939 [INFO ] W-9000-my_model_1.0-stdout MODEL_LOG - Python runtime: 3.12.3
2025-05-05T00:44:16,939 [INFO ] W-9000-my_model_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9000
2025-05-05T00:44:16,940 [DEBUG] W-9000-my_model_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req.cmd LOAD repeats 1 to backend at: 1746387856940
2025-05-05T00:44:16,940 [INFO ] W-9000-my_model_1.0-stdout MODEL_LOG - Connection accepted: /tmp/.ts.sock.9000.
2025-05-05T00:44:16,940 [DEBUG] W-9000-my_model_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req.cmd LOAD repeats 1 to backend at: 1746387856940
2025-05-05T00:44:16,941 [INFO ] W-9000-my_model_1.0 org.pytorch.serve.wlm.WorkerThread - Looping backend response at: 1746387856940
2025-05-05T00:44:16,941 [INFO ] W-9000-my_model_1.0 org.pytorch.serve.wlm.WorkerThread - Looping backend response at: 1746387856940
2025-05-05T00:44:16,955 [INFO ] W-9000-my_model_1.0-stdout MODEL_LOG - model_name: my_model, batchSize: 1
2025-05-05T00:44:18,336 [INFO ] W-9000-my_model_1.0-stdout MODEL_LOG - OpenVINO is not enabled
2025-05-05T00:44:18,337 [INFO ] W-9000-my_model_1.0-stdout MODEL_LOG - proceeding without onnxruntime
2025-05-05T00:44:18,337 [INFO ] W-9000-my_model_1.0-stdout MODEL_LOG - Torch TensorRT not enabled
2025-05-05T00:44:18,338 [INFO ] W-9000-my_model_1.0-stdout MODEL_LOG - Failed to load model my_model, exception No model weights could be loaded
2025-05-05T00:44:18,339 [INFO ] W-9000-my_model_1.0-stdout MODEL_LOG - Traceback (most recent call last):
2025-05-05T00:44:18,339 [INFO ] W-9000-my_model_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 1398
2025-05-05T00:44:18,339 [INFO ] W-9000-my_model_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 1398
2025-05-05T00:44:18,339 [INFO ] W-9000-my_model_1.0-stdout MODEL_LOG -   File "/home/pupu/Desktop/project/.venv/lib/python3.12/site-packages/ts/model_service_worker.py", line 133, in load_model
2025-05-05T00:44:18,339 [DEBUG] W-9000-my_model_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-my_model_1.0 State change WORKER_STARTED -> WORKER_ERROR
2025-05-05T00:44:18,339 [DEBUG] W-9000-my_model_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-my_model_1.0 State change WORKER_STARTED -> WORKER_ERROR
2025-05-05T00:44:18,339 [INFO ] W-9000-my_model_1.0-stdout MODEL_LOG -     service = model_loader.load(
2025-05-05T00:44:18,339 [INFO ] W-9000-my_model_1.0 TS_METRICS - WorkerThreadTime.Milliseconds:1.0|#Level:Host|#hostname:pupu-Stvullinc,timestamp:1746387858
2025-05-05T00:44:18,339 [INFO ] W-9000-my_model_1.0-stdout MODEL_LOG -               ^^^^^^^^^^^^^^^^^^
2025-05-05T00:44:18,339 [INFO ] epollEventLoopGroup-5-3 org.pytorch.serve.wlm.WorkerThread - 9000 Worker disconnected. WORKER_ERROR
2025-05-05T00:44:18,339 [INFO ] W-9000-my_model_1.0-stdout MODEL_LOG -   File "/home/pupu/Desktop/project/.venv/lib/python3.12/site-packages/ts/model_loader.py", line 143, in load
2025-05-05T00:44:18,339 [INFO ] epollEventLoopGroup-5-3 org.pytorch.serve.wlm.WorkerThread - 9000 Worker disconnected. WORKER_ERROR
2025-05-05T00:44:18,340 [INFO ] W-9000-my_model_1.0-stdout MODEL_LOG -     initialize_fn(service.context)
2025-05-05T00:44:18,340 [INFO ] W-9000-my_model_1.0-stdout MODEL_LOG -   File "/tmp/models/1595b2e1bad84d53b7dc47d2998fd305/handler.py", line 19, in initialize
2025-05-05T00:44:18,340 [DEBUG] W-9000-my_model_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_ERROR
2025-05-05T00:44:18,340 [DEBUG] W-9000-my_model_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_ERROR
2025-05-05T00:44:18,340 [INFO ] W-9000-my_model_1.0-stdout MODEL_LOG -     super().initialize(context)
2025-05-05T00:44:18,340 [INFO ] W-9000-my_model_1.0-stdout MODEL_LOG -   File "/home/pupu/Desktop/project/.venv/lib/python3.12/site-packages/ts/torch_handler/base_handler.py", line 223, in initialize
2025-05-05T00:44:18,340 [DEBUG] W-9000-my_model_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died., responseTimeout:120sec
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1770) ~[?:?]
	at java.util.concurrent.LinkedBlockingDeque.pollFirst(LinkedBlockingDeque.java:515) ~[?:?]
	at java.util.concurrent.LinkedBlockingDeque.poll(LinkedBlockingDeque.java:677) ~[?:?]
	at org.pytorch.serve.wlm.Model.pollBatch(Model.java:482) ~[model-server.jar:?]
	at org.pytorch.serve.wlm.BatchAggregator.pollBatch(BatchAggregator.java:189) ~[model-server.jar:?]
	at org.pytorch.serve.wlm.BatchAggregator.getRequest(BatchAggregator.java:36) ~[model-server.jar:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:195) ~[model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144) ~[?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642) ~[?:?]
	at java.lang.Thread.run(Thread.java:1583) [?:?]
2025-05-05T00:44:18,341 [INFO ] W-9000-my_model_1.0-stdout MODEL_LOG -     raise RuntimeError("No model weights could be loaded")
2025-05-05T00:44:18,340 [DEBUG] W-9000-my_model_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died., responseTimeout:120sec
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1770) ~[?:?]
	at java.util.concurrent.LinkedBlockingDeque.pollFirst(LinkedBlockingDeque.java:515) ~[?:?]
	at java.util.concurrent.LinkedBlockingDeque.poll(LinkedBlockingDeque.java:677) ~[?:?]
	at org.pytorch.serve.wlm.Model.pollBatch(Model.java:482) ~[model-server.jar:?]
	at org.pytorch.serve.wlm.BatchAggregator.pollBatch(BatchAggregator.java:189) ~[model-server.jar:?]
	at org.pytorch.serve.wlm.BatchAggregator.getRequest(BatchAggregator.java:36) ~[model-server.jar:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:195) ~[model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144) ~[?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642) ~[?:?]
	at java.lang.Thread.run(Thread.java:1583) [?:?]
2025-05-05T00:44:18,342 [DEBUG] W-9000-my_model_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-my_model_1.0 State change WORKER_ERROR -> WORKER_STOPPED
2025-05-05T00:44:18,342 [INFO ] W-9000-my_model_1.0-stdout MODEL_LOG - RuntimeError: No model weights could be loaded
2025-05-05T00:44:18,342 [DEBUG] W-9000-my_model_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-my_model_1.0 State change WORKER_ERROR -> WORKER_STOPPED
2025-05-05T00:44:18,342 [WARN ] W-9000-my_model_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery failed again
2025-05-05T00:44:18,342 [WARN ] W-9000-my_model_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery failed again
2025-05-05T00:44:18,342 [INFO ] W-9000-my_model_1.0-stdout MODEL_LOG - Backend worker process died.
2025-05-05T00:44:18,342 [INFO ] W-9000-my_model_1.0-stdout MODEL_LOG - Traceback (most recent call last):
2025-05-05T00:44:18,342 [INFO ] W-9000-my_model_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9000 in 2 seconds.
2025-05-05T00:44:18,342 [INFO ] W-9000-my_model_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9000 in 2 seconds.
2025-05-05T00:44:18,343 [INFO ] W-9000-my_model_1.0-stdout MODEL_LOG -   File "/home/pupu/Desktop/project/.venv/lib/python3.12/site-packages/ts/model_service_worker.py", line 301, in <module>
2025-05-05T00:44:18,343 [INFO ] W-9000-my_model_1.0-stdout MODEL_LOG -     worker.run_server()
2025-05-05T00:44:18,343 [INFO ] W-9000-my_model_1.0-stdout MODEL_LOG -   File "/home/pupu/Desktop/project/.venv/lib/python3.12/site-packages/ts/model_service_worker.py", line 268, in run_server
2025-05-05T00:44:18,343 [INFO ] W-9000-my_model_1.0-stdout MODEL_LOG -     self.handle_connection(cl_socket)
2025-05-05T00:44:18,343 [INFO ] W-9000-my_model_1.0-stdout MODEL_LOG -   File "/home/pupu/Desktop/project/.venv/lib/python3.12/site-packages/ts/model_service_worker.py", line 201, in handle_connection
2025-05-05T00:44:18,344 [INFO ] W-9000-my_model_1.0-stdout MODEL_LOG -     raise RuntimeError("{} - {}".format(code, result))
2025-05-05T00:44:18,344 [INFO ] W-9000-my_model_1.0-stdout MODEL_LOG - RuntimeError: 500 - Unknown exception
2025-05-05T00:44:18,344 [INFO ] W-9000-my_model_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-my_model_1.0-stdout
2025-05-05T00:44:18,344 [INFO ] W-9000-my_model_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-my_model_1.0-stdout
2025-05-05T00:44:18,415 [INFO ] W-9000-my_model_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-my_model_1.0-stderr
2025-05-05T00:44:18,415 [INFO ] W-9000-my_model_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-my_model_1.0-stderr
2025-05-05T00:44:20,343 [DEBUG] W-9000-my_model_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/home/pupu/Desktop/project/.venv/bin/python, /home/pupu/Desktop/project/.venv/lib/python3.12/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9000, --metrics-config, /home/pupu/Desktop/project/.venv/lib/python3.12/site-packages/ts/configs/metrics.yaml]
2025-05-05T00:44:20,343 [DEBUG] W-9000-my_model_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/home/pupu/Desktop/project/.venv/bin/python, /home/pupu/Desktop/project/.venv/lib/python3.12/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9000, --metrics-config, /home/pupu/Desktop/project/.venv/lib/python3.12/site-packages/ts/configs/metrics.yaml]
2025-05-05T00:44:22,120 [INFO ] W-9000-my_model_1.0-stdout MODEL_LOG - s_name_part0=/tmp/.ts.sock, s_name_part1=9000, pid=8788
2025-05-05T00:44:22,121 [INFO ] W-9000-my_model_1.0-stdout MODEL_LOG - Listening on port: /tmp/.ts.sock.9000
2025-05-05T00:44:22,130 [INFO ] W-9000-my_model_1.0-stdout MODEL_LOG - Successfully loaded /home/pupu/Desktop/project/.venv/lib/python3.12/site-packages/ts/configs/metrics.yaml.
2025-05-05T00:44:22,131 [INFO ] W-9000-my_model_1.0-stdout MODEL_LOG - [PID]8788
2025-05-05T00:44:22,131 [DEBUG] W-9000-my_model_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-my_model_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2025-05-05T00:44:22,131 [INFO ] W-9000-my_model_1.0-stdout MODEL_LOG - Torch worker started.
2025-05-05T00:44:22,131 [DEBUG] W-9000-my_model_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-my_model_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2025-05-05T00:44:22,131 [INFO ] W-9000-my_model_1.0-stdout MODEL_LOG - Python runtime: 3.12.3
2025-05-05T00:44:22,131 [INFO ] W-9000-my_model_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9000
2025-05-05T00:44:22,131 [INFO ] W-9000-my_model_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9000
2025-05-05T00:44:22,134 [DEBUG] W-9000-my_model_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req.cmd LOAD repeats 1 to backend at: 1746387862134
2025-05-05T00:44:22,134 [INFO ] W-9000-my_model_1.0-stdout MODEL_LOG - Connection accepted: /tmp/.ts.sock.9000.
2025-05-05T00:44:22,134 [DEBUG] W-9000-my_model_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req.cmd LOAD repeats 1 to backend at: 1746387862134
2025-05-05T00:44:22,134 [INFO ] W-9000-my_model_1.0 org.pytorch.serve.wlm.WorkerThread - Looping backend response at: 1746387862134
2025-05-05T00:44:22,134 [INFO ] W-9000-my_model_1.0 org.pytorch.serve.wlm.WorkerThread - Looping backend response at: 1746387862134
2025-05-05T00:44:22,152 [INFO ] W-9000-my_model_1.0-stdout MODEL_LOG - model_name: my_model, batchSize: 1
2025-05-05T00:44:23,503 [INFO ] W-9000-my_model_1.0-stdout MODEL_LOG - OpenVINO is not enabled
2025-05-05T00:44:23,503 [INFO ] W-9000-my_model_1.0-stdout MODEL_LOG - proceeding without onnxruntime
2025-05-05T00:44:23,503 [INFO ] W-9000-my_model_1.0-stdout MODEL_LOG - Torch TensorRT not enabled
2025-05-05T00:44:23,504 [INFO ] W-9000-my_model_1.0-stdout MODEL_LOG - Failed to load model my_model, exception No model weights could be loaded
2025-05-05T00:44:23,505 [INFO ] W-9000-my_model_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 1371
2025-05-05T00:44:23,505 [INFO ] W-9000-my_model_1.0-stdout MODEL_LOG - Traceback (most recent call last):
2025-05-05T00:44:23,505 [INFO ] W-9000-my_model_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 1371
2025-05-05T00:44:23,505 [DEBUG] W-9000-my_model_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-my_model_1.0 State change WORKER_STARTED -> WORKER_ERROR
2025-05-05T00:44:23,505 [INFO ] W-9000-my_model_1.0-stdout MODEL_LOG -   File "/home/pupu/Desktop/project/.venv/lib/python3.12/site-packages/ts/model_service_worker.py", line 133, in load_model
2025-05-05T00:44:23,505 [DEBUG] W-9000-my_model_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-my_model_1.0 State change WORKER_STARTED -> WORKER_ERROR
2025-05-05T00:44:23,505 [INFO ] epollEventLoopGroup-5-4 org.pytorch.serve.wlm.WorkerThread - 9000 Worker disconnected. WORKER_STARTED
2025-05-05T00:44:23,505 [INFO ] W-9000-my_model_1.0-stdout MODEL_LOG -     service = model_loader.load(
2025-05-05T00:44:23,505 [INFO ] W-9000-my_model_1.0 TS_METRICS - WorkerThreadTime.Milliseconds:0.0|#Level:Host|#hostname:pupu-Stvullinc,timestamp:1746387863
2025-05-05T00:44:23,505 [INFO ] epollEventLoopGroup-5-4 org.pytorch.serve.wlm.WorkerThread - 9000 Worker disconnected. WORKER_STARTED
2025-05-05T00:44:23,506 [INFO ] W-9000-my_model_1.0-stdout MODEL_LOG -               ^^^^^^^^^^^^^^^^^^
2025-05-05T00:44:23,506 [DEBUG] W-9000-my_model_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_ERROR
2025-05-05T00:44:23,506 [INFO ] W-9000-my_model_1.0-stdout MODEL_LOG -   File "/home/pupu/Desktop/project/.venv/lib/python3.12/site-packages/ts/model_loader.py", line 143, in load
2025-05-05T00:44:23,506 [DEBUG] W-9000-my_model_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_ERROR
2025-05-05T00:44:23,506 [INFO ] W-9000-my_model_1.0-stdout MODEL_LOG -     initialize_fn(service.context)
2025-05-05T00:44:23,506 [INFO ] W-9000-my_model_1.0-stdout MODEL_LOG -   File "/tmp/models/1595b2e1bad84d53b7dc47d2998fd305/handler.py", line 19, in initialize
2025-05-05T00:44:23,506 [DEBUG] W-9000-my_model_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died., responseTimeout:120sec
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1770) ~[?:?]
	at java.util.concurrent.LinkedBlockingDeque.pollFirst(LinkedBlockingDeque.java:515) ~[?:?]
	at java.util.concurrent.LinkedBlockingDeque.poll(LinkedBlockingDeque.java:677) ~[?:?]
	at org.pytorch.serve.wlm.Model.pollBatch(Model.java:482) ~[model-server.jar:?]
	at org.pytorch.serve.wlm.BatchAggregator.pollBatch(BatchAggregator.java:189) ~[model-server.jar:?]
	at org.pytorch.serve.wlm.BatchAggregator.getRequest(BatchAggregator.java:36) ~[model-server.jar:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:195) ~[model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144) ~[?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642) ~[?:?]
	at java.lang.Thread.run(Thread.java:1583) [?:?]
2025-05-05T00:44:23,506 [INFO ] W-9000-my_model_1.0-stdout MODEL_LOG -     super().initialize(context)
2025-05-05T00:44:23,506 [DEBUG] W-9000-my_model_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died., responseTimeout:120sec
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1770) ~[?:?]
	at java.util.concurrent.LinkedBlockingDeque.pollFirst(LinkedBlockingDeque.java:515) ~[?:?]
	at java.util.concurrent.LinkedBlockingDeque.poll(LinkedBlockingDeque.java:677) ~[?:?]
	at org.pytorch.serve.wlm.Model.pollBatch(Model.java:482) ~[model-server.jar:?]
	at org.pytorch.serve.wlm.BatchAggregator.pollBatch(BatchAggregator.java:189) ~[model-server.jar:?]
	at org.pytorch.serve.wlm.BatchAggregator.getRequest(BatchAggregator.java:36) ~[model-server.jar:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:195) ~[model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144) ~[?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642) ~[?:?]
	at java.lang.Thread.run(Thread.java:1583) [?:?]
2025-05-05T00:44:23,507 [DEBUG] W-9000-my_model_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-my_model_1.0 State change WORKER_ERROR -> WORKER_STOPPED
2025-05-05T00:44:23,507 [INFO ] W-9000-my_model_1.0-stdout MODEL_LOG -   File "/home/pupu/Desktop/project/.venv/lib/python3.12/site-packages/ts/torch_handler/base_handler.py", line 223, in initialize
2025-05-05T00:44:23,507 [DEBUG] W-9000-my_model_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-my_model_1.0 State change WORKER_ERROR -> WORKER_STOPPED
2025-05-05T00:44:23,507 [WARN ] W-9000-my_model_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery failed again
2025-05-05T00:44:23,507 [INFO ] W-9000-my_model_1.0-stdout MODEL_LOG -     raise RuntimeError("No model weights could be loaded")
2025-05-05T00:44:23,507 [WARN ] W-9000-my_model_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery failed again
2025-05-05T00:44:23,507 [INFO ] W-9000-my_model_1.0-stdout MODEL_LOG - RuntimeError: No model weights could be loaded
2025-05-05T00:44:23,507 [INFO ] W-9000-my_model_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9000 in 3 seconds.
2025-05-05T00:44:23,507 [INFO ] W-9000-my_model_1.0-stdout MODEL_LOG - Backend worker process died.
2025-05-05T00:44:23,507 [INFO ] W-9000-my_model_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9000 in 3 seconds.
2025-05-05T00:44:23,507 [INFO ] W-9000-my_model_1.0-stdout MODEL_LOG - Traceback (most recent call last):
2025-05-05T00:44:23,507 [INFO ] W-9000-my_model_1.0-stdout MODEL_LOG -   File "/home/pupu/Desktop/project/.venv/lib/python3.12/site-packages/ts/model_service_worker.py", line 301, in <module>
2025-05-05T00:44:23,508 [INFO ] W-9000-my_model_1.0-stdout MODEL_LOG -     worker.run_server()
2025-05-05T00:44:23,508 [INFO ] W-9000-my_model_1.0-stdout MODEL_LOG -   File "/home/pupu/Desktop/project/.venv/lib/python3.12/site-packages/ts/model_service_worker.py", line 268, in run_server
2025-05-05T00:44:23,508 [INFO ] W-9000-my_model_1.0-stdout MODEL_LOG -     self.handle_connection(cl_socket)
2025-05-05T00:44:23,508 [INFO ] W-9000-my_model_1.0-stdout MODEL_LOG -   File "/home/pupu/Desktop/project/.venv/lib/python3.12/site-packages/ts/model_service_worker.py", line 201, in handle_connection
2025-05-05T00:44:23,508 [INFO ] W-9000-my_model_1.0-stdout MODEL_LOG -     raise RuntimeError("{} - {}".format(code, result))
2025-05-05T00:44:23,508 [INFO ] W-9000-my_model_1.0-stdout MODEL_LOG - RuntimeError: 500 - Unknown exception
2025-05-05T00:44:23,509 [INFO ] W-9000-my_model_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-my_model_1.0-stdout
2025-05-05T00:44:23,509 [INFO ] W-9000-my_model_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-my_model_1.0-stdout
2025-05-05T00:44:23,590 [INFO ] W-9000-my_model_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-my_model_1.0-stderr
2025-05-05T00:44:23,590 [INFO ] W-9000-my_model_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-my_model_1.0-stderr
2025-05-05T00:44:26,508 [DEBUG] W-9000-my_model_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/home/pupu/Desktop/project/.venv/bin/python, /home/pupu/Desktop/project/.venv/lib/python3.12/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9000, --metrics-config, /home/pupu/Desktop/project/.venv/lib/python3.12/site-packages/ts/configs/metrics.yaml]
2025-05-05T00:44:26,508 [DEBUG] W-9000-my_model_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/home/pupu/Desktop/project/.venv/bin/python, /home/pupu/Desktop/project/.venv/lib/python3.12/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9000, --metrics-config, /home/pupu/Desktop/project/.venv/lib/python3.12/site-packages/ts/configs/metrics.yaml]
2025-05-05T00:44:28,268 [INFO ] W-9000-my_model_1.0-stdout MODEL_LOG - s_name_part0=/tmp/.ts.sock, s_name_part1=9000, pid=8811
2025-05-05T00:44:28,269 [INFO ] W-9000-my_model_1.0-stdout MODEL_LOG - Listening on port: /tmp/.ts.sock.9000
2025-05-05T00:44:28,277 [INFO ] W-9000-my_model_1.0-stdout MODEL_LOG - Successfully loaded /home/pupu/Desktop/project/.venv/lib/python3.12/site-packages/ts/configs/metrics.yaml.
2025-05-05T00:44:28,277 [INFO ] W-9000-my_model_1.0-stdout MODEL_LOG - [PID]8811
2025-05-05T00:44:28,277 [INFO ] W-9000-my_model_1.0-stdout MODEL_LOG - Torch worker started.
2025-05-05T00:44:28,277 [DEBUG] W-9000-my_model_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-my_model_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2025-05-05T00:44:28,277 [DEBUG] W-9000-my_model_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-my_model_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2025-05-05T00:44:28,277 [INFO ] W-9000-my_model_1.0-stdout MODEL_LOG - Python runtime: 3.12.3
2025-05-05T00:44:28,277 [INFO ] W-9000-my_model_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9000
2025-05-05T00:44:28,277 [INFO ] W-9000-my_model_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9000
2025-05-05T00:44:28,279 [DEBUG] W-9000-my_model_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req.cmd LOAD repeats 1 to backend at: 1746387868279
2025-05-05T00:44:28,279 [INFO ] W-9000-my_model_1.0-stdout MODEL_LOG - Connection accepted: /tmp/.ts.sock.9000.
2025-05-05T00:44:28,279 [DEBUG] W-9000-my_model_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req.cmd LOAD repeats 1 to backend at: 1746387868279
2025-05-05T00:44:28,279 [INFO ] W-9000-my_model_1.0 org.pytorch.serve.wlm.WorkerThread - Looping backend response at: 1746387868279
2025-05-05T00:44:28,279 [INFO ] W-9000-my_model_1.0 org.pytorch.serve.wlm.WorkerThread - Looping backend response at: 1746387868279
2025-05-05T00:44:28,297 [INFO ] W-9000-my_model_1.0-stdout MODEL_LOG - model_name: my_model, batchSize: 1
2025-05-05T00:44:29,597 [INFO ] W-9000-my_model_1.0-stdout MODEL_LOG - OpenVINO is not enabled
2025-05-05T00:44:29,597 [INFO ] W-9000-my_model_1.0-stdout MODEL_LOG - proceeding without onnxruntime
2025-05-05T00:44:29,597 [INFO ] W-9000-my_model_1.0-stdout MODEL_LOG - Torch TensorRT not enabled
2025-05-05T00:44:29,598 [INFO ] W-9000-my_model_1.0-stdout MODEL_LOG - Failed to load model my_model, exception No model weights could be loaded
2025-05-05T00:44:29,598 [INFO ] W-9000-my_model_1.0-stdout MODEL_LOG - Traceback (most recent call last):
2025-05-05T00:44:29,598 [INFO ] W-9000-my_model_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 1318
2025-05-05T00:44:29,598 [INFO ] W-9000-my_model_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 1318
2025-05-05T00:44:29,599 [INFO ] W-9000-my_model_1.0-stdout MODEL_LOG -   File "/home/pupu/Desktop/project/.venv/lib/python3.12/site-packages/ts/model_service_worker.py", line 133, in load_model
2025-05-05T00:44:29,599 [DEBUG] W-9000-my_model_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-my_model_1.0 State change WORKER_STARTED -> WORKER_ERROR
2025-05-05T00:44:29,599 [DEBUG] W-9000-my_model_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-my_model_1.0 State change WORKER_STARTED -> WORKER_ERROR
2025-05-05T00:44:29,599 [INFO ] W-9000-my_model_1.0-stdout MODEL_LOG -     service = model_loader.load(
2025-05-05T00:44:29,599 [INFO ] W-9000-my_model_1.0-stdout MODEL_LOG -               ^^^^^^^^^^^^^^^^^^
2025-05-05T00:44:29,599 [INFO ] W-9000-my_model_1.0 TS_METRICS - WorkerThreadTime.Milliseconds:2.0|#Level:Host|#hostname:pupu-Stvullinc,timestamp:1746387869
2025-05-05T00:44:29,599 [INFO ] W-9000-my_model_1.0-stdout MODEL_LOG -   File "/home/pupu/Desktop/project/.venv/lib/python3.12/site-packages/ts/model_loader.py", line 143, in load
2025-05-05T00:44:29,599 [INFO ] epollEventLoopGroup-5-5 org.pytorch.serve.wlm.WorkerThread - 9000 Worker disconnected. WORKER_ERROR
2025-05-05T00:44:29,599 [INFO ] W-9000-my_model_1.0-stdout MODEL_LOG -     initialize_fn(service.context)
2025-05-05T00:44:29,599 [INFO ] W-9000-my_model_1.0-stdout MODEL_LOG -   File "/tmp/models/1595b2e1bad84d53b7dc47d2998fd305/handler.py", line 19, in initialize
2025-05-05T00:44:29,599 [INFO ] epollEventLoopGroup-5-5 org.pytorch.serve.wlm.WorkerThread - 9000 Worker disconnected. WORKER_ERROR
2025-05-05T00:44:29,599 [INFO ] W-9000-my_model_1.0-stdout MODEL_LOG -     super().initialize(context)
2025-05-05T00:44:29,599 [DEBUG] W-9000-my_model_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_ERROR
2025-05-05T00:44:29,599 [DEBUG] W-9000-my_model_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_ERROR
2025-05-05T00:44:29,600 [INFO ] W-9000-my_model_1.0-stdout MODEL_LOG -   File "/home/pupu/Desktop/project/.venv/lib/python3.12/site-packages/ts/torch_handler/base_handler.py", line 223, in initialize
2025-05-05T00:44:29,600 [DEBUG] W-9000-my_model_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died., responseTimeout:120sec
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1770) ~[?:?]
	at java.util.concurrent.LinkedBlockingDeque.pollFirst(LinkedBlockingDeque.java:515) ~[?:?]
	at java.util.concurrent.LinkedBlockingDeque.poll(LinkedBlockingDeque.java:677) ~[?:?]
	at org.pytorch.serve.wlm.Model.pollBatch(Model.java:482) ~[model-server.jar:?]
	at org.pytorch.serve.wlm.BatchAggregator.pollBatch(BatchAggregator.java:189) ~[model-server.jar:?]
	at org.pytorch.serve.wlm.BatchAggregator.getRequest(BatchAggregator.java:36) ~[model-server.jar:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:195) ~[model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144) ~[?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642) ~[?:?]
	at java.lang.Thread.run(Thread.java:1583) [?:?]
2025-05-05T00:44:29,600 [INFO ] W-9000-my_model_1.0-stdout MODEL_LOG -     raise RuntimeError("No model weights could be loaded")
2025-05-05T00:44:29,600 [DEBUG] W-9000-my_model_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died., responseTimeout:120sec
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1770) ~[?:?]
	at java.util.concurrent.LinkedBlockingDeque.pollFirst(LinkedBlockingDeque.java:515) ~[?:?]
	at java.util.concurrent.LinkedBlockingDeque.poll(LinkedBlockingDeque.java:677) ~[?:?]
	at org.pytorch.serve.wlm.Model.pollBatch(Model.java:482) ~[model-server.jar:?]
	at org.pytorch.serve.wlm.BatchAggregator.pollBatch(BatchAggregator.java:189) ~[model-server.jar:?]
	at org.pytorch.serve.wlm.BatchAggregator.getRequest(BatchAggregator.java:36) ~[model-server.jar:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:195) ~[model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144) ~[?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642) ~[?:?]
	at java.lang.Thread.run(Thread.java:1583) [?:?]
2025-05-05T00:44:29,600 [INFO ] W-9000-my_model_1.0-stdout MODEL_LOG - RuntimeError: No model weights could be loaded
2025-05-05T00:44:29,600 [DEBUG] W-9000-my_model_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-my_model_1.0 State change WORKER_ERROR -> WORKER_STOPPED
2025-05-05T00:44:29,600 [INFO ] W-9000-my_model_1.0-stdout MODEL_LOG - Backend worker process died.
2025-05-05T00:44:29,600 [DEBUG] W-9000-my_model_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-my_model_1.0 State change WORKER_ERROR -> WORKER_STOPPED
2025-05-05T00:44:29,600 [WARN ] W-9000-my_model_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery failed again
2025-05-05T00:44:29,600 [INFO ] W-9000-my_model_1.0-stdout MODEL_LOG - Traceback (most recent call last):
2025-05-05T00:44:29,600 [WARN ] W-9000-my_model_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery failed again
2025-05-05T00:44:29,601 [INFO ] W-9000-my_model_1.0-stdout MODEL_LOG -   File "/home/pupu/Desktop/project/.venv/lib/python3.12/site-packages/ts/model_service_worker.py", line 301, in <module>
2025-05-05T00:44:29,601 [INFO ] W-9000-my_model_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9000 in 5 seconds.
2025-05-05T00:44:29,601 [INFO ] W-9000-my_model_1.0-stdout MODEL_LOG -     worker.run_server()
2025-05-05T00:44:29,601 [INFO ] W-9000-my_model_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9000 in 5 seconds.
2025-05-05T00:44:29,601 [INFO ] W-9000-my_model_1.0-stdout MODEL_LOG -   File "/home/pupu/Desktop/project/.venv/lib/python3.12/site-packages/ts/model_service_worker.py", line 268, in run_server
2025-05-05T00:44:29,601 [INFO ] W-9000-my_model_1.0-stdout MODEL_LOG -     self.handle_connection(cl_socket)
2025-05-05T00:44:29,601 [INFO ] W-9000-my_model_1.0-stdout MODEL_LOG -   File "/home/pupu/Desktop/project/.venv/lib/python3.12/site-packages/ts/model_service_worker.py", line 201, in handle_connection
2025-05-05T00:44:29,601 [INFO ] W-9000-my_model_1.0-stdout MODEL_LOG -     raise RuntimeError("{} - {}".format(code, result))
2025-05-05T00:44:29,602 [INFO ] W-9000-my_model_1.0-stdout MODEL_LOG - RuntimeError: 500 - Unknown exception
2025-05-05T00:44:29,602 [INFO ] W-9000-my_model_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-my_model_1.0-stdout
2025-05-05T00:44:29,602 [INFO ] W-9000-my_model_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-my_model_1.0-stdout
2025-05-05T00:44:29,679 [INFO ] W-9000-my_model_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-my_model_1.0-stderr
2025-05-05T00:44:29,679 [INFO ] W-9000-my_model_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-my_model_1.0-stderr
2025-05-05T00:44:34,602 [DEBUG] W-9000-my_model_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/home/pupu/Desktop/project/.venv/bin/python, /home/pupu/Desktop/project/.venv/lib/python3.12/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9000, --metrics-config, /home/pupu/Desktop/project/.venv/lib/python3.12/site-packages/ts/configs/metrics.yaml]
2025-05-05T00:44:34,602 [DEBUG] W-9000-my_model_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/home/pupu/Desktop/project/.venv/bin/python, /home/pupu/Desktop/project/.venv/lib/python3.12/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9000, --metrics-config, /home/pupu/Desktop/project/.venv/lib/python3.12/site-packages/ts/configs/metrics.yaml]
2025-05-05T00:44:36,365 [INFO ] W-9000-my_model_1.0-stdout MODEL_LOG - s_name_part0=/tmp/.ts.sock, s_name_part1=9000, pid=8827
2025-05-05T00:44:36,366 [INFO ] W-9000-my_model_1.0-stdout MODEL_LOG - Listening on port: /tmp/.ts.sock.9000
2025-05-05T00:44:36,375 [INFO ] W-9000-my_model_1.0-stdout MODEL_LOG - Successfully loaded /home/pupu/Desktop/project/.venv/lib/python3.12/site-packages/ts/configs/metrics.yaml.
2025-05-05T00:44:36,375 [INFO ] W-9000-my_model_1.0-stdout MODEL_LOG - [PID]8827
2025-05-05T00:44:36,375 [INFO ] W-9000-my_model_1.0-stdout MODEL_LOG - Torch worker started.
2025-05-05T00:44:36,375 [DEBUG] W-9000-my_model_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-my_model_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2025-05-05T00:44:36,375 [DEBUG] W-9000-my_model_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-my_model_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2025-05-05T00:44:36,375 [INFO ] W-9000-my_model_1.0-stdout MODEL_LOG - Python runtime: 3.12.3
2025-05-05T00:44:36,375 [INFO ] W-9000-my_model_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9000
2025-05-05T00:44:36,375 [INFO ] W-9000-my_model_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9000
2025-05-05T00:44:36,377 [INFO ] W-9000-my_model_1.0-stdout MODEL_LOG - Connection accepted: /tmp/.ts.sock.9000.
2025-05-05T00:44:36,377 [DEBUG] W-9000-my_model_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req.cmd LOAD repeats 1 to backend at: 1746387876377
2025-05-05T00:44:36,377 [DEBUG] W-9000-my_model_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req.cmd LOAD repeats 1 to backend at: 1746387876377
2025-05-05T00:44:36,378 [INFO ] W-9000-my_model_1.0 org.pytorch.serve.wlm.WorkerThread - Looping backend response at: 1746387876378
2025-05-05T00:44:36,378 [INFO ] W-9000-my_model_1.0 org.pytorch.serve.wlm.WorkerThread - Looping backend response at: 1746387876378
2025-05-05T00:44:36,392 [INFO ] W-9000-my_model_1.0-stdout MODEL_LOG - model_name: my_model, batchSize: 1
2025-05-05T00:44:37,683 [INFO ] W-9000-my_model_1.0-stdout MODEL_LOG - OpenVINO is not enabled
2025-05-05T00:44:37,683 [INFO ] W-9000-my_model_1.0-stdout MODEL_LOG - proceeding without onnxruntime
2025-05-05T00:44:37,683 [INFO ] W-9000-my_model_1.0-stdout MODEL_LOG - Torch TensorRT not enabled
2025-05-05T00:44:37,685 [INFO ] W-9000-my_model_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 1307
2025-05-05T00:44:37,684 [INFO ] W-9000-my_model_1.0-stdout MODEL_LOG - Failed to load model my_model, exception No model weights could be loaded
2025-05-05T00:44:37,685 [INFO ] W-9000-my_model_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 1307
2025-05-05T00:44:37,685 [DEBUG] W-9000-my_model_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-my_model_1.0 State change WORKER_STARTED -> WORKER_ERROR
2025-05-05T00:44:37,685 [INFO ] W-9000-my_model_1.0-stdout MODEL_LOG - Traceback (most recent call last):
2025-05-05T00:44:37,685 [DEBUG] W-9000-my_model_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-my_model_1.0 State change WORKER_STARTED -> WORKER_ERROR
2025-05-05T00:44:37,685 [INFO ] W-9000-my_model_1.0-stdout MODEL_LOG -   File "/home/pupu/Desktop/project/.venv/lib/python3.12/site-packages/ts/model_service_worker.py", line 133, in load_model
2025-05-05T00:44:37,685 [INFO ] W-9000-my_model_1.0 TS_METRICS - WorkerThreadTime.Milliseconds:1.0|#Level:Host|#hostname:pupu-Stvullinc,timestamp:1746387877
2025-05-05T00:44:37,685 [INFO ] W-9000-my_model_1.0-stdout MODEL_LOG -     service = model_loader.load(
2025-05-05T00:44:37,685 [INFO ] W-9000-my_model_1.0-stdout MODEL_LOG -               ^^^^^^^^^^^^^^^^^^
2025-05-05T00:44:37,685 [INFO ] W-9000-my_model_1.0-stdout MODEL_LOG -   File "/home/pupu/Desktop/project/.venv/lib/python3.12/site-packages/ts/model_loader.py", line 143, in load
2025-05-05T00:44:37,686 [INFO ] W-9000-my_model_1.0-stdout MODEL_LOG -     initialize_fn(service.context)
2025-05-05T00:44:37,685 [INFO ] epollEventLoopGroup-5-6 org.pytorch.serve.wlm.WorkerThread - 9000 Worker disconnected. WORKER_ERROR
2025-05-05T00:44:37,686 [INFO ] W-9000-my_model_1.0-stdout MODEL_LOG -   File "/tmp/models/1595b2e1bad84d53b7dc47d2998fd305/handler.py", line 19, in initialize
2025-05-05T00:44:37,685 [INFO ] epollEventLoopGroup-5-6 org.pytorch.serve.wlm.WorkerThread - 9000 Worker disconnected. WORKER_ERROR
2025-05-05T00:44:37,686 [INFO ] W-9000-my_model_1.0-stdout MODEL_LOG -     super().initialize(context)
2025-05-05T00:44:37,686 [DEBUG] W-9000-my_model_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_ERROR
2025-05-05T00:44:37,686 [DEBUG] W-9000-my_model_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_ERROR
2025-05-05T00:44:37,686 [INFO ] W-9000-my_model_1.0-stdout MODEL_LOG -   File "/home/pupu/Desktop/project/.venv/lib/python3.12/site-packages/ts/torch_handler/base_handler.py", line 223, in initialize
2025-05-05T00:44:37,686 [DEBUG] W-9000-my_model_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died., responseTimeout:120sec
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1770) ~[?:?]
	at java.util.concurrent.LinkedBlockingDeque.pollFirst(LinkedBlockingDeque.java:515) ~[?:?]
	at java.util.concurrent.LinkedBlockingDeque.poll(LinkedBlockingDeque.java:677) ~[?:?]
	at org.pytorch.serve.wlm.Model.pollBatch(Model.java:482) ~[model-server.jar:?]
	at org.pytorch.serve.wlm.BatchAggregator.pollBatch(BatchAggregator.java:189) ~[model-server.jar:?]
	at org.pytorch.serve.wlm.BatchAggregator.getRequest(BatchAggregator.java:36) ~[model-server.jar:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:195) ~[model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144) ~[?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642) ~[?:?]
	at java.lang.Thread.run(Thread.java:1583) [?:?]
2025-05-05T00:44:37,686 [INFO ] W-9000-my_model_1.0-stdout MODEL_LOG -     raise RuntimeError("No model weights could be loaded")
2025-05-05T00:44:37,686 [DEBUG] W-9000-my_model_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died., responseTimeout:120sec
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1770) ~[?:?]
	at java.util.concurrent.LinkedBlockingDeque.pollFirst(LinkedBlockingDeque.java:515) ~[?:?]
	at java.util.concurrent.LinkedBlockingDeque.poll(LinkedBlockingDeque.java:677) ~[?:?]
	at org.pytorch.serve.wlm.Model.pollBatch(Model.java:482) ~[model-server.jar:?]
	at org.pytorch.serve.wlm.BatchAggregator.pollBatch(BatchAggregator.java:189) ~[model-server.jar:?]
	at org.pytorch.serve.wlm.BatchAggregator.getRequest(BatchAggregator.java:36) ~[model-server.jar:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:195) ~[model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144) ~[?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642) ~[?:?]
	at java.lang.Thread.run(Thread.java:1583) [?:?]
2025-05-05T00:44:37,687 [DEBUG] W-9000-my_model_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-my_model_1.0 State change WORKER_ERROR -> WORKER_STOPPED
2025-05-05T00:44:37,687 [INFO ] W-9000-my_model_1.0-stdout MODEL_LOG - RuntimeError: No model weights could be loaded
2025-05-05T00:44:37,687 [DEBUG] W-9000-my_model_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-my_model_1.0 State change WORKER_ERROR -> WORKER_STOPPED
2025-05-05T00:44:37,687 [WARN ] W-9000-my_model_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery failed again
2025-05-05T00:44:37,687 [WARN ] W-9000-my_model_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery failed again
2025-05-05T00:44:37,687 [INFO ] W-9000-my_model_1.0-stdout MODEL_LOG - Backend worker process died.
2025-05-05T00:44:37,687 [INFO ] W-9000-my_model_1.0-stdout MODEL_LOG - Traceback (most recent call last):
2025-05-05T00:44:37,687 [INFO ] W-9000-my_model_1.0-stdout MODEL_LOG -   File "/home/pupu/Desktop/project/.venv/lib/python3.12/site-packages/ts/model_service_worker.py", line 301, in <module>
2025-05-05T00:44:37,687 [INFO ] W-9000-my_model_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9000 in 8 seconds.
2025-05-05T00:44:37,687 [INFO ] W-9000-my_model_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9000 in 8 seconds.
2025-05-05T00:44:37,687 [INFO ] W-9000-my_model_1.0-stdout MODEL_LOG -     worker.run_server()
2025-05-05T00:44:37,688 [INFO ] W-9000-my_model_1.0-stdout MODEL_LOG -   File "/home/pupu/Desktop/project/.venv/lib/python3.12/site-packages/ts/model_service_worker.py", line 268, in run_server
2025-05-05T00:44:37,688 [INFO ] W-9000-my_model_1.0-stdout MODEL_LOG -     self.handle_connection(cl_socket)
2025-05-05T00:44:37,688 [INFO ] W-9000-my_model_1.0-stdout MODEL_LOG -   File "/home/pupu/Desktop/project/.venv/lib/python3.12/site-packages/ts/model_service_worker.py", line 201, in handle_connection
2025-05-05T00:44:37,688 [INFO ] W-9000-my_model_1.0-stdout MODEL_LOG -     raise RuntimeError("{} - {}".format(code, result))
2025-05-05T00:44:37,688 [INFO ] W-9000-my_model_1.0-stdout MODEL_LOG - RuntimeError: 500 - Unknown exception
2025-05-05T00:44:37,688 [INFO ] W-9000-my_model_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-my_model_1.0-stdout
2025-05-05T00:44:37,688 [INFO ] W-9000-my_model_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-my_model_1.0-stdout
2025-05-05T00:44:37,747 [INFO ] W-9000-my_model_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-my_model_1.0-stderr
2025-05-05T00:44:37,747 [INFO ] W-9000-my_model_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-my_model_1.0-stderr
2025-05-05T00:44:45,688 [DEBUG] W-9000-my_model_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/home/pupu/Desktop/project/.venv/bin/python, /home/pupu/Desktop/project/.venv/lib/python3.12/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9000, --metrics-config, /home/pupu/Desktop/project/.venv/lib/python3.12/site-packages/ts/configs/metrics.yaml]
2025-05-05T00:44:45,688 [DEBUG] W-9000-my_model_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/home/pupu/Desktop/project/.venv/bin/python, /home/pupu/Desktop/project/.venv/lib/python3.12/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9000, --metrics-config, /home/pupu/Desktop/project/.venv/lib/python3.12/site-packages/ts/configs/metrics.yaml]
2025-05-05T00:44:47,551 [INFO ] W-9000-my_model_1.0-stdout MODEL_LOG - s_name_part0=/tmp/.ts.sock, s_name_part1=9000, pid=8848
2025-05-05T00:44:47,552 [INFO ] W-9000-my_model_1.0-stdout MODEL_LOG - Listening on port: /tmp/.ts.sock.9000
2025-05-05T00:44:47,560 [INFO ] W-9000-my_model_1.0-stdout MODEL_LOG - Successfully loaded /home/pupu/Desktop/project/.venv/lib/python3.12/site-packages/ts/configs/metrics.yaml.
2025-05-05T00:44:47,560 [INFO ] W-9000-my_model_1.0-stdout MODEL_LOG - [PID]8848
2025-05-05T00:44:47,560 [INFO ] W-9000-my_model_1.0-stdout MODEL_LOG - Torch worker started.
2025-05-05T00:44:47,560 [DEBUG] W-9000-my_model_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-my_model_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2025-05-05T00:44:47,560 [DEBUG] W-9000-my_model_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-my_model_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2025-05-05T00:44:47,560 [INFO ] W-9000-my_model_1.0-stdout MODEL_LOG - Python runtime: 3.12.3
2025-05-05T00:44:47,561 [INFO ] W-9000-my_model_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9000
2025-05-05T00:44:47,561 [INFO ] W-9000-my_model_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9000
2025-05-05T00:44:47,565 [DEBUG] W-9000-my_model_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req.cmd LOAD repeats 1 to backend at: 1746387887565
2025-05-05T00:44:47,565 [INFO ] W-9000-my_model_1.0-stdout MODEL_LOG - Connection accepted: /tmp/.ts.sock.9000.
2025-05-05T00:44:47,565 [DEBUG] W-9000-my_model_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req.cmd LOAD repeats 1 to backend at: 1746387887565
2025-05-05T00:44:47,566 [INFO ] W-9000-my_model_1.0 org.pytorch.serve.wlm.WorkerThread - Looping backend response at: 1746387887566
2025-05-05T00:44:47,566 [INFO ] W-9000-my_model_1.0 org.pytorch.serve.wlm.WorkerThread - Looping backend response at: 1746387887566
2025-05-05T00:44:47,583 [INFO ] W-9000-my_model_1.0-stdout MODEL_LOG - model_name: my_model, batchSize: 1
2025-05-05T00:44:48,903 [INFO ] W-9000-my_model_1.0-stdout MODEL_LOG - OpenVINO is not enabled
2025-05-05T00:44:48,903 [INFO ] W-9000-my_model_1.0-stdout MODEL_LOG - proceeding without onnxruntime
2025-05-05T00:44:48,903 [INFO ] W-9000-my_model_1.0-stdout MODEL_LOG - Torch TensorRT not enabled
2025-05-05T00:44:48,905 [INFO ] W-9000-my_model_1.0-stdout MODEL_LOG - Failed to load model my_model, exception No model weights could be loaded
2025-05-05T00:44:48,905 [INFO ] W-9000-my_model_1.0-stdout MODEL_LOG - Traceback (most recent call last):
2025-05-05T00:44:48,905 [INFO ] W-9000-my_model_1.0-stdout MODEL_LOG -   File "/home/pupu/Desktop/project/.venv/lib/python3.12/site-packages/ts/model_service_worker.py", line 133, in load_model
2025-05-05T00:44:48,905 [INFO ] W-9000-my_model_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 1339
2025-05-05T00:44:48,905 [INFO ] W-9000-my_model_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 1339
2025-05-05T00:44:48,905 [INFO ] W-9000-my_model_1.0-stdout MODEL_LOG -     service = model_loader.load(
2025-05-05T00:44:48,905 [DEBUG] W-9000-my_model_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-my_model_1.0 State change WORKER_STARTED -> WORKER_ERROR
2025-05-05T00:44:48,905 [DEBUG] W-9000-my_model_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-my_model_1.0 State change WORKER_STARTED -> WORKER_ERROR
2025-05-05T00:44:48,905 [INFO ] W-9000-my_model_1.0 TS_METRICS - WorkerThreadTime.Milliseconds:1.0|#Level:Host|#hostname:pupu-Stvullinc,timestamp:1746387888
2025-05-05T00:44:48,905 [INFO ] W-9000-my_model_1.0-stdout MODEL_LOG -               ^^^^^^^^^^^^^^^^^^
2025-05-05T00:44:48,906 [INFO ] W-9000-my_model_1.0-stdout MODEL_LOG -   File "/home/pupu/Desktop/project/.venv/lib/python3.12/site-packages/ts/model_loader.py", line 143, in load
2025-05-05T00:44:48,906 [INFO ] W-9000-my_model_1.0-stdout MODEL_LOG -     initialize_fn(service.context)
2025-05-05T00:44:48,906 [INFO ] epollEventLoopGroup-5-7 org.pytorch.serve.wlm.WorkerThread - 9000 Worker disconnected. WORKER_ERROR
2025-05-05T00:44:48,906 [INFO ] W-9000-my_model_1.0-stdout MODEL_LOG -   File "/tmp/models/1595b2e1bad84d53b7dc47d2998fd305/handler.py", line 19, in initialize
2025-05-05T00:44:48,906 [INFO ] epollEventLoopGroup-5-7 org.pytorch.serve.wlm.WorkerThread - 9000 Worker disconnected. WORKER_ERROR
2025-05-05T00:44:48,906 [INFO ] W-9000-my_model_1.0-stdout MODEL_LOG -     super().initialize(context)
2025-05-05T00:44:48,906 [DEBUG] W-9000-my_model_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_ERROR
2025-05-05T00:44:48,906 [DEBUG] W-9000-my_model_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_ERROR
2025-05-05T00:44:48,906 [INFO ] W-9000-my_model_1.0-stdout MODEL_LOG -   File "/home/pupu/Desktop/project/.venv/lib/python3.12/site-packages/ts/torch_handler/base_handler.py", line 223, in initialize
2025-05-05T00:44:48,906 [INFO ] W-9000-my_model_1.0-stdout MODEL_LOG -     raise RuntimeError("No model weights could be loaded")
2025-05-05T00:44:48,906 [INFO ] W-9000-my_model_1.0-stdout MODEL_LOG - RuntimeError: No model weights could be loaded
2025-05-05T00:44:48,906 [DEBUG] W-9000-my_model_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died., responseTimeout:120sec
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1770) ~[?:?]
	at java.util.concurrent.LinkedBlockingDeque.pollFirst(LinkedBlockingDeque.java:515) ~[?:?]
	at java.util.concurrent.LinkedBlockingDeque.poll(LinkedBlockingDeque.java:677) ~[?:?]
	at org.pytorch.serve.wlm.Model.pollBatch(Model.java:482) ~[model-server.jar:?]
	at org.pytorch.serve.wlm.BatchAggregator.pollBatch(BatchAggregator.java:189) ~[model-server.jar:?]
	at org.pytorch.serve.wlm.BatchAggregator.getRequest(BatchAggregator.java:36) ~[model-server.jar:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:195) ~[model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144) ~[?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642) ~[?:?]
	at java.lang.Thread.run(Thread.java:1583) [?:?]
2025-05-05T00:44:48,906 [INFO ] W-9000-my_model_1.0-stdout MODEL_LOG - Backend worker process died.
2025-05-05T00:44:48,906 [DEBUG] W-9000-my_model_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died., responseTimeout:120sec
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1770) ~[?:?]
	at java.util.concurrent.LinkedBlockingDeque.pollFirst(LinkedBlockingDeque.java:515) ~[?:?]
	at java.util.concurrent.LinkedBlockingDeque.poll(LinkedBlockingDeque.java:677) ~[?:?]
	at org.pytorch.serve.wlm.Model.pollBatch(Model.java:482) ~[model-server.jar:?]
	at org.pytorch.serve.wlm.BatchAggregator.pollBatch(BatchAggregator.java:189) ~[model-server.jar:?]
	at org.pytorch.serve.wlm.BatchAggregator.getRequest(BatchAggregator.java:36) ~[model-server.jar:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:195) ~[model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144) ~[?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642) ~[?:?]
	at java.lang.Thread.run(Thread.java:1583) [?:?]
2025-05-05T00:44:48,907 [INFO ] W-9000-my_model_1.0-stdout MODEL_LOG - Traceback (most recent call last):
2025-05-05T00:44:48,907 [DEBUG] W-9000-my_model_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-my_model_1.0 State change WORKER_ERROR -> WORKER_STOPPED
2025-05-05T00:44:48,907 [INFO ] W-9000-my_model_1.0-stdout MODEL_LOG -   File "/home/pupu/Desktop/project/.venv/lib/python3.12/site-packages/ts/model_service_worker.py", line 301, in <module>
2025-05-05T00:44:48,907 [DEBUG] W-9000-my_model_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-my_model_1.0 State change WORKER_ERROR -> WORKER_STOPPED
2025-05-05T00:44:48,907 [WARN ] W-9000-my_model_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery failed again
2025-05-05T00:44:48,907 [INFO ] W-9000-my_model_1.0-stdout MODEL_LOG -     worker.run_server()
2025-05-05T00:44:48,907 [WARN ] W-9000-my_model_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery failed again
2025-05-05T00:44:48,907 [INFO ] W-9000-my_model_1.0-stdout MODEL_LOG -   File "/home/pupu/Desktop/project/.venv/lib/python3.12/site-packages/ts/model_service_worker.py", line 268, in run_server
2025-05-05T00:44:48,907 [INFO ] W-9000-my_model_1.0-stdout MODEL_LOG -     self.handle_connection(cl_socket)
2025-05-05T00:44:48,907 [INFO ] W-9000-my_model_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9000 in 13 seconds.
2025-05-05T00:44:48,907 [INFO ] W-9000-my_model_1.0-stdout MODEL_LOG -   File "/home/pupu/Desktop/project/.venv/lib/python3.12/site-packages/ts/model_service_worker.py", line 201, in handle_connection
2025-05-05T00:44:48,907 [INFO ] W-9000-my_model_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9000 in 13 seconds.
2025-05-05T00:44:48,907 [INFO ] W-9000-my_model_1.0-stdout MODEL_LOG -     raise RuntimeError("{} - {}".format(code, result))
2025-05-05T00:44:48,907 [INFO ] W-9000-my_model_1.0-stdout MODEL_LOG - RuntimeError: 500 - Unknown exception
2025-05-05T00:44:48,907 [INFO ] W-9000-my_model_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-my_model_1.0-stdout
2025-05-05T00:44:48,907 [INFO ] W-9000-my_model_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-my_model_1.0-stdout
2025-05-05T00:44:48,978 [INFO ] W-9000-my_model_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-my_model_1.0-stderr
2025-05-05T00:44:48,978 [INFO ] W-9000-my_model_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-my_model_1.0-stderr
2025-05-05T00:44:53,184 [INFO ] pool-3-thread-2 TS_METRICS - CPUUtilization.Percent:25.0|#Level:Host|#hostname:pupu-Stvullinc,timestamp:1746387893
2025-05-05T00:44:53,185 [INFO ] pool-3-thread-2 TS_METRICS - DiskAvailable.Gigabytes:75.47029113769531|#Level:Host|#hostname:pupu-Stvullinc,timestamp:1746387893
2025-05-05T00:44:53,185 [INFO ] pool-3-thread-2 TS_METRICS - DiskUsage.Gigabytes:62.71155548095703|#Level:Host|#hostname:pupu-Stvullinc,timestamp:1746387893
2025-05-05T00:44:53,186 [INFO ] pool-3-thread-2 TS_METRICS - DiskUtilization.Percent:45.4|#Level:Host|#hostname:pupu-Stvullinc,timestamp:1746387893
2025-05-05T00:44:53,186 [INFO ] pool-3-thread-2 TS_METRICS - GPUMemoryUtilization.Percent:3.8818359375|#Level:Host,DeviceId:0|#hostname:pupu-Stvullinc,timestamp:1746387893
2025-05-05T00:44:53,187 [INFO ] pool-3-thread-2 TS_METRICS - GPUMemoryUsed.Megabytes:318.0|#Level:Host,DeviceId:0|#hostname:pupu-Stvullinc,timestamp:1746387893
2025-05-05T00:44:53,187 [INFO ] pool-3-thread-2 TS_METRICS - GPUUtilization.Percent:12.0|#Level:Host,DeviceId:0|#hostname:pupu-Stvullinc,timestamp:1746387893
2025-05-05T00:44:53,187 [INFO ] pool-3-thread-2 TS_METRICS - MemoryAvailable.Megabytes:9707.09765625|#Level:Host|#hostname:pupu-Stvullinc,timestamp:1746387893
2025-05-05T00:44:53,187 [INFO ] pool-3-thread-2 TS_METRICS - MemoryUsed.Megabytes:5803.2890625|#Level:Host|#hostname:pupu-Stvullinc,timestamp:1746387893
2025-05-05T00:44:53,188 [INFO ] pool-3-thread-2 TS_METRICS - MemoryUtilization.Percent:39.2|#Level:Host|#hostname:pupu-Stvullinc,timestamp:1746387893
2025-05-05T00:45:01,908 [DEBUG] W-9000-my_model_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/home/pupu/Desktop/project/.venv/bin/python, /home/pupu/Desktop/project/.venv/lib/python3.12/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9000, --metrics-config, /home/pupu/Desktop/project/.venv/lib/python3.12/site-packages/ts/configs/metrics.yaml]
2025-05-05T00:45:01,908 [DEBUG] W-9000-my_model_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/home/pupu/Desktop/project/.venv/bin/python, /home/pupu/Desktop/project/.venv/lib/python3.12/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9000, --metrics-config, /home/pupu/Desktop/project/.venv/lib/python3.12/site-packages/ts/configs/metrics.yaml]
2025-05-05T00:45:03,760 [INFO ] W-9000-my_model_1.0-stdout MODEL_LOG - s_name_part0=/tmp/.ts.sock, s_name_part1=9000, pid=8895
2025-05-05T00:45:03,761 [INFO ] W-9000-my_model_1.0-stdout MODEL_LOG - Listening on port: /tmp/.ts.sock.9000
2025-05-05T00:45:03,769 [INFO ] W-9000-my_model_1.0-stdout MODEL_LOG - Successfully loaded /home/pupu/Desktop/project/.venv/lib/python3.12/site-packages/ts/configs/metrics.yaml.
2025-05-05T00:45:03,769 [INFO ] W-9000-my_model_1.0-stdout MODEL_LOG - [PID]8895
2025-05-05T00:45:03,769 [INFO ] W-9000-my_model_1.0-stdout MODEL_LOG - Torch worker started.
2025-05-05T00:45:03,769 [DEBUG] W-9000-my_model_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-my_model_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2025-05-05T00:45:03,769 [DEBUG] W-9000-my_model_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-my_model_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2025-05-05T00:45:03,770 [INFO ] W-9000-my_model_1.0-stdout MODEL_LOG - Python runtime: 3.12.3
2025-05-05T00:45:03,770 [INFO ] W-9000-my_model_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9000
2025-05-05T00:45:03,770 [INFO ] W-9000-my_model_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9000
2025-05-05T00:45:03,771 [DEBUG] W-9000-my_model_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req.cmd LOAD repeats 1 to backend at: 1746387903771
2025-05-05T00:45:03,771 [DEBUG] W-9000-my_model_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req.cmd LOAD repeats 1 to backend at: 1746387903771
2025-05-05T00:45:03,771 [INFO ] W-9000-my_model_1.0-stdout MODEL_LOG - Connection accepted: /tmp/.ts.sock.9000.
2025-05-05T00:45:03,771 [INFO ] W-9000-my_model_1.0 org.pytorch.serve.wlm.WorkerThread - Looping backend response at: 1746387903771
2025-05-05T00:45:03,771 [INFO ] W-9000-my_model_1.0 org.pytorch.serve.wlm.WorkerThread - Looping backend response at: 1746387903771
2025-05-05T00:45:03,788 [INFO ] W-9000-my_model_1.0-stdout MODEL_LOG - model_name: my_model, batchSize: 1
2025-05-05T00:45:05,318 [INFO ] W-9000-my_model_1.0-stdout MODEL_LOG - OpenVINO is not enabled
2025-05-05T00:45:05,319 [INFO ] W-9000-my_model_1.0-stdout MODEL_LOG - proceeding without onnxruntime
2025-05-05T00:45:05,319 [INFO ] W-9000-my_model_1.0-stdout MODEL_LOG - Torch TensorRT not enabled
2025-05-05T00:45:05,320 [INFO ] W-9000-my_model_1.0-stdout MODEL_LOG - Failed to load model my_model, exception No model weights could be loaded
2025-05-05T00:45:05,321 [INFO ] W-9000-my_model_1.0-stdout MODEL_LOG - Traceback (most recent call last):
2025-05-05T00:45:05,321 [INFO ] W-9000-my_model_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 1550
2025-05-05T00:45:05,321 [INFO ] W-9000-my_model_1.0-stdout MODEL_LOG -   File "/home/pupu/Desktop/project/.venv/lib/python3.12/site-packages/ts/model_service_worker.py", line 133, in load_model
2025-05-05T00:45:05,321 [INFO ] W-9000-my_model_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 1550
2025-05-05T00:45:05,321 [INFO ] W-9000-my_model_1.0-stdout MODEL_LOG -     service = model_loader.load(
2025-05-05T00:45:05,321 [DEBUG] W-9000-my_model_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-my_model_1.0 State change WORKER_STARTED -> WORKER_ERROR
2025-05-05T00:45:05,321 [DEBUG] W-9000-my_model_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-my_model_1.0 State change WORKER_STARTED -> WORKER_ERROR
2025-05-05T00:45:05,321 [INFO ] W-9000-my_model_1.0-stdout MODEL_LOG -               ^^^^^^^^^^^^^^^^^^
2025-05-05T00:45:05,321 [INFO ] W-9000-my_model_1.0-stdout MODEL_LOG -   File "/home/pupu/Desktop/project/.venv/lib/python3.12/site-packages/ts/model_loader.py", line 143, in load
2025-05-05T00:45:05,321 [INFO ] W-9000-my_model_1.0 TS_METRICS - WorkerThreadTime.Milliseconds:0.0|#Level:Host|#hostname:pupu-Stvullinc,timestamp:1746387905
2025-05-05T00:45:05,321 [INFO ] W-9000-my_model_1.0-stdout MODEL_LOG -     initialize_fn(service.context)
2025-05-05T00:45:05,321 [INFO ] W-9000-my_model_1.0-stdout MODEL_LOG -   File "/tmp/models/1595b2e1bad84d53b7dc47d2998fd305/handler.py", line 19, in initialize
2025-05-05T00:45:05,321 [INFO ] W-9000-my_model_1.0-stdout MODEL_LOG -     super().initialize(context)
2025-05-05T00:45:05,321 [INFO ] epollEventLoopGroup-5-8 org.pytorch.serve.wlm.WorkerThread - 9000 Worker disconnected. WORKER_ERROR
2025-05-05T00:45:05,321 [INFO ] epollEventLoopGroup-5-8 org.pytorch.serve.wlm.WorkerThread - 9000 Worker disconnected. WORKER_ERROR
2025-05-05T00:45:05,322 [INFO ] W-9000-my_model_1.0-stdout MODEL_LOG -   File "/home/pupu/Desktop/project/.venv/lib/python3.12/site-packages/ts/torch_handler/base_handler.py", line 223, in initialize
2025-05-05T00:45:05,322 [INFO ] W-9000-my_model_1.0-stdout MODEL_LOG -     raise RuntimeError("No model weights could be loaded")
2025-05-05T00:45:05,322 [DEBUG] W-9000-my_model_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_ERROR
2025-05-05T00:45:05,322 [INFO ] W-9000-my_model_1.0-stdout MODEL_LOG - RuntimeError: No model weights could be loaded
2025-05-05T00:45:05,322 [DEBUG] W-9000-my_model_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_ERROR
2025-05-05T00:45:05,322 [INFO ] W-9000-my_model_1.0-stdout MODEL_LOG - Backend worker process died.
2025-05-05T00:45:05,322 [INFO ] W-9000-my_model_1.0-stdout MODEL_LOG - Traceback (most recent call last):
2025-05-05T00:45:05,322 [INFO ] W-9000-my_model_1.0-stdout MODEL_LOG -   File "/home/pupu/Desktop/project/.venv/lib/python3.12/site-packages/ts/model_service_worker.py", line 301, in <module>
2025-05-05T00:45:05,322 [DEBUG] W-9000-my_model_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died., responseTimeout:120sec
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1770) ~[?:?]
	at java.util.concurrent.LinkedBlockingDeque.pollFirst(LinkedBlockingDeque.java:515) ~[?:?]
	at java.util.concurrent.LinkedBlockingDeque.poll(LinkedBlockingDeque.java:677) ~[?:?]
	at org.pytorch.serve.wlm.Model.pollBatch(Model.java:482) ~[model-server.jar:?]
	at org.pytorch.serve.wlm.BatchAggregator.pollBatch(BatchAggregator.java:189) ~[model-server.jar:?]
	at org.pytorch.serve.wlm.BatchAggregator.getRequest(BatchAggregator.java:36) ~[model-server.jar:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:195) ~[model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144) ~[?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642) ~[?:?]
	at java.lang.Thread.run(Thread.java:1583) [?:?]
2025-05-05T00:45:05,323 [INFO ] W-9000-my_model_1.0-stdout MODEL_LOG -     worker.run_server()
2025-05-05T00:45:05,322 [DEBUG] W-9000-my_model_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died., responseTimeout:120sec
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1770) ~[?:?]
	at java.util.concurrent.LinkedBlockingDeque.pollFirst(LinkedBlockingDeque.java:515) ~[?:?]
	at java.util.concurrent.LinkedBlockingDeque.poll(LinkedBlockingDeque.java:677) ~[?:?]
	at org.pytorch.serve.wlm.Model.pollBatch(Model.java:482) ~[model-server.jar:?]
	at org.pytorch.serve.wlm.BatchAggregator.pollBatch(BatchAggregator.java:189) ~[model-server.jar:?]
	at org.pytorch.serve.wlm.BatchAggregator.getRequest(BatchAggregator.java:36) ~[model-server.jar:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:195) ~[model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144) ~[?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642) ~[?:?]
	at java.lang.Thread.run(Thread.java:1583) [?:?]
2025-05-05T00:45:05,323 [INFO ] W-9000-my_model_1.0-stdout MODEL_LOG -   File "/home/pupu/Desktop/project/.venv/lib/python3.12/site-packages/ts/model_service_worker.py", line 268, in run_server
2025-05-05T00:45:05,323 [DEBUG] W-9000-my_model_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-my_model_1.0 State change WORKER_ERROR -> WORKER_STOPPED
2025-05-05T00:45:05,323 [DEBUG] W-9000-my_model_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-my_model_1.0 State change WORKER_ERROR -> WORKER_STOPPED
2025-05-05T00:45:05,323 [INFO ] W-9000-my_model_1.0-stdout MODEL_LOG -     self.handle_connection(cl_socket)
2025-05-05T00:45:05,323 [WARN ] W-9000-my_model_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery failed again
2025-05-05T00:45:05,323 [WARN ] W-9000-my_model_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery failed again
2025-05-05T00:45:05,323 [INFO ] W-9000-my_model_1.0-stdout MODEL_LOG -   File "/home/pupu/Desktop/project/.venv/lib/python3.12/site-packages/ts/model_service_worker.py", line 201, in handle_connection
2025-05-05T00:45:05,323 [INFO ] W-9000-my_model_1.0-stdout MODEL_LOG -     raise RuntimeError("{} - {}".format(code, result))
2025-05-05T00:45:05,323 [INFO ] W-9000-my_model_1.0-stdout MODEL_LOG - RuntimeError: 500 - Unknown exception
2025-05-05T00:45:05,323 [INFO ] W-9000-my_model_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9000 in 21 seconds.
2025-05-05T00:45:05,323 [INFO ] W-9000-my_model_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-my_model_1.0-stdout
2025-05-05T00:45:05,323 [INFO ] W-9000-my_model_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9000 in 21 seconds.
2025-05-05T00:45:05,323 [INFO ] W-9000-my_model_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-my_model_1.0-stdout
2025-05-05T00:45:05,395 [INFO ] W-9000-my_model_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-my_model_1.0-stderr
2025-05-05T00:45:05,395 [INFO ] W-9000-my_model_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-my_model_1.0-stderr
2025-05-05T00:45:26,324 [DEBUG] W-9000-my_model_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/home/pupu/Desktop/project/.venv/bin/python, /home/pupu/Desktop/project/.venv/lib/python3.12/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9000, --metrics-config, /home/pupu/Desktop/project/.venv/lib/python3.12/site-packages/ts/configs/metrics.yaml]
2025-05-05T00:45:26,324 [DEBUG] W-9000-my_model_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/home/pupu/Desktop/project/.venv/bin/python, /home/pupu/Desktop/project/.venv/lib/python3.12/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9000, --metrics-config, /home/pupu/Desktop/project/.venv/lib/python3.12/site-packages/ts/configs/metrics.yaml]
2025-05-05T00:45:28,096 [INFO ] W-9000-my_model_1.0-stdout MODEL_LOG - s_name_part0=/tmp/.ts.sock, s_name_part1=9000, pid=8962
2025-05-05T00:45:28,097 [INFO ] W-9000-my_model_1.0-stdout MODEL_LOG - Listening on port: /tmp/.ts.sock.9000
2025-05-05T00:45:28,107 [INFO ] W-9000-my_model_1.0-stdout MODEL_LOG - Successfully loaded /home/pupu/Desktop/project/.venv/lib/python3.12/site-packages/ts/configs/metrics.yaml.
2025-05-05T00:45:28,107 [INFO ] W-9000-my_model_1.0-stdout MODEL_LOG - [PID]8962
2025-05-05T00:45:28,107 [INFO ] W-9000-my_model_1.0-stdout MODEL_LOG - Torch worker started.
2025-05-05T00:45:28,107 [DEBUG] W-9000-my_model_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-my_model_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2025-05-05T00:45:28,107 [DEBUG] W-9000-my_model_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-my_model_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2025-05-05T00:45:28,107 [INFO ] W-9000-my_model_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9000
2025-05-05T00:45:28,107 [INFO ] W-9000-my_model_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9000
2025-05-05T00:45:28,108 [INFO ] W-9000-my_model_1.0-stdout MODEL_LOG - Python runtime: 3.12.3
2025-05-05T00:45:28,108 [INFO ] W-9000-my_model_1.0-stdout MODEL_LOG - Connection accepted: /tmp/.ts.sock.9000.
2025-05-05T00:45:28,108 [DEBUG] W-9000-my_model_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req.cmd LOAD repeats 1 to backend at: 1746387928108
2025-05-05T00:45:28,108 [DEBUG] W-9000-my_model_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req.cmd LOAD repeats 1 to backend at: 1746387928108
2025-05-05T00:45:28,109 [INFO ] W-9000-my_model_1.0 org.pytorch.serve.wlm.WorkerThread - Looping backend response at: 1746387928109
2025-05-05T00:45:28,109 [INFO ] W-9000-my_model_1.0 org.pytorch.serve.wlm.WorkerThread - Looping backend response at: 1746387928109
2025-05-05T00:45:28,122 [INFO ] W-9000-my_model_1.0-stdout MODEL_LOG - model_name: my_model, batchSize: 1
2025-05-05T00:45:29,661 [INFO ] W-9000-my_model_1.0-stdout MODEL_LOG - OpenVINO is not enabled
2025-05-05T00:45:29,662 [INFO ] W-9000-my_model_1.0-stdout MODEL_LOG - proceeding without onnxruntime
2025-05-05T00:45:29,662 [INFO ] W-9000-my_model_1.0-stdout MODEL_LOG - Torch TensorRT not enabled
2025-05-05T00:45:29,663 [INFO ] W-9000-my_model_1.0-stdout MODEL_LOG - Failed to load model my_model, exception No model weights could be loaded
2025-05-05T00:45:29,663 [INFO ] W-9000-my_model_1.0-stdout MODEL_LOG - Traceback (most recent call last):
2025-05-05T00:45:29,663 [INFO ] W-9000-my_model_1.0-stdout MODEL_LOG -   File "/home/pupu/Desktop/project/.venv/lib/python3.12/site-packages/ts/model_service_worker.py", line 133, in load_model
2025-05-05T00:45:29,663 [INFO ] W-9000-my_model_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 1554
2025-05-05T00:45:29,663 [INFO ] W-9000-my_model_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 1554
2025-05-05T00:45:29,663 [INFO ] W-9000-my_model_1.0-stdout MODEL_LOG -     service = model_loader.load(
2025-05-05T00:45:29,664 [DEBUG] W-9000-my_model_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-my_model_1.0 State change WORKER_STARTED -> WORKER_ERROR
2025-05-05T00:45:29,664 [DEBUG] W-9000-my_model_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-my_model_1.0 State change WORKER_STARTED -> WORKER_ERROR
2025-05-05T00:45:29,664 [INFO ] W-9000-my_model_1.0-stdout MODEL_LOG -               ^^^^^^^^^^^^^^^^^^
2025-05-05T00:45:29,664 [INFO ] W-9000-my_model_1.0 TS_METRICS - WorkerThreadTime.Milliseconds:2.0|#Level:Host|#hostname:pupu-Stvullinc,timestamp:1746387929
2025-05-05T00:45:29,664 [INFO ] W-9000-my_model_1.0-stdout MODEL_LOG -   File "/home/pupu/Desktop/project/.venv/lib/python3.12/site-packages/ts/model_loader.py", line 143, in load
2025-05-05T00:45:29,664 [INFO ] W-9000-my_model_1.0-stdout MODEL_LOG -     initialize_fn(service.context)
2025-05-05T00:45:29,664 [INFO ] W-9000-my_model_1.0-stdout MODEL_LOG -   File "/tmp/models/1595b2e1bad84d53b7dc47d2998fd305/handler.py", line 19, in initialize
2025-05-05T00:45:29,664 [INFO ] W-9000-my_model_1.0-stdout MODEL_LOG -     super().initialize(context)
2025-05-05T00:45:29,664 [INFO ] epollEventLoopGroup-5-9 org.pytorch.serve.wlm.WorkerThread - 9000 Worker disconnected. WORKER_ERROR
2025-05-05T00:45:29,664 [INFO ] W-9000-my_model_1.0-stdout MODEL_LOG -   File "/home/pupu/Desktop/project/.venv/lib/python3.12/site-packages/ts/torch_handler/base_handler.py", line 223, in initialize
2025-05-05T00:45:29,664 [INFO ] W-9000-my_model_1.0-stdout MODEL_LOG -     raise RuntimeError("No model weights could be loaded")
2025-05-05T00:45:29,665 [INFO ] W-9000-my_model_1.0-stdout MODEL_LOG - RuntimeError: No model weights could be loaded
2025-05-05T00:45:29,665 [INFO ] W-9000-my_model_1.0-stdout MODEL_LOG - Backend worker process died.
2025-05-05T00:45:29,665 [INFO ] W-9000-my_model_1.0-stdout MODEL_LOG - Traceback (most recent call last):
2025-05-05T00:45:29,665 [INFO ] W-9000-my_model_1.0-stdout MODEL_LOG -   File "/home/pupu/Desktop/project/.venv/lib/python3.12/site-packages/ts/model_service_worker.py", line 301, in <module>
2025-05-05T00:45:29,665 [INFO ] W-9000-my_model_1.0-stdout MODEL_LOG -     worker.run_server()
2025-05-05T00:45:29,665 [INFO ] W-9000-my_model_1.0-stdout MODEL_LOG -   File "/home/pupu/Desktop/project/.venv/lib/python3.12/site-packages/ts/model_service_worker.py", line 268, in run_server
2025-05-05T00:45:29,665 [INFO ] W-9000-my_model_1.0-stdout MODEL_LOG -     self.handle_connection(cl_socket)
2025-05-05T00:45:29,665 [INFO ] W-9000-my_model_1.0-stdout MODEL_LOG -   File "/home/pupu/Desktop/project/.venv/lib/python3.12/site-packages/ts/model_service_worker.py", line 201, in handle_connection
2025-05-05T00:45:29,665 [INFO ] W-9000-my_model_1.0-stdout MODEL_LOG -     raise RuntimeError("{} - {}".format(code, result))
2025-05-05T00:45:29,666 [INFO ] W-9000-my_model_1.0-stdout MODEL_LOG - RuntimeError: 500 - Unknown exception
2025-05-05T00:45:29,664 [INFO ] epollEventLoopGroup-5-9 org.pytorch.serve.wlm.WorkerThread - 9000 Worker disconnected. WORKER_ERROR
2025-05-05T00:45:29,666 [DEBUG] W-9000-my_model_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_ERROR
2025-05-05T00:45:29,666 [DEBUG] W-9000-my_model_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_ERROR
2025-05-05T00:45:29,666 [DEBUG] W-9000-my_model_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died., responseTimeout:120sec
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1770) ~[?:?]
	at java.util.concurrent.LinkedBlockingDeque.pollFirst(LinkedBlockingDeque.java:515) ~[?:?]
	at java.util.concurrent.LinkedBlockingDeque.poll(LinkedBlockingDeque.java:677) ~[?:?]
	at org.pytorch.serve.wlm.Model.pollBatch(Model.java:482) ~[model-server.jar:?]
	at org.pytorch.serve.wlm.BatchAggregator.pollBatch(BatchAggregator.java:189) ~[model-server.jar:?]
	at org.pytorch.serve.wlm.BatchAggregator.getRequest(BatchAggregator.java:36) ~[model-server.jar:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:195) ~[model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144) ~[?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642) ~[?:?]
	at java.lang.Thread.run(Thread.java:1583) [?:?]
2025-05-05T00:45:29,666 [DEBUG] W-9000-my_model_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died., responseTimeout:120sec
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1770) ~[?:?]
	at java.util.concurrent.LinkedBlockingDeque.pollFirst(LinkedBlockingDeque.java:515) ~[?:?]
	at java.util.concurrent.LinkedBlockingDeque.poll(LinkedBlockingDeque.java:677) ~[?:?]
	at org.pytorch.serve.wlm.Model.pollBatch(Model.java:482) ~[model-server.jar:?]
	at org.pytorch.serve.wlm.BatchAggregator.pollBatch(BatchAggregator.java:189) ~[model-server.jar:?]
	at org.pytorch.serve.wlm.BatchAggregator.getRequest(BatchAggregator.java:36) ~[model-server.jar:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:195) ~[model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144) ~[?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642) ~[?:?]
	at java.lang.Thread.run(Thread.java:1583) [?:?]
2025-05-05T00:45:29,667 [DEBUG] W-9000-my_model_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-my_model_1.0 State change WORKER_ERROR -> WORKER_STOPPED
2025-05-05T00:45:29,667 [DEBUG] W-9000-my_model_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-my_model_1.0 State change WORKER_ERROR -> WORKER_STOPPED
2025-05-05T00:45:29,667 [WARN ] W-9000-my_model_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery failed again
2025-05-05T00:45:29,667 [WARN ] W-9000-my_model_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery failed again
2025-05-05T00:45:29,667 [INFO ] W-9000-my_model_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9000 in 34 seconds.
2025-05-05T00:45:29,667 [INFO ] W-9000-my_model_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9000 in 34 seconds.
2025-05-05T00:45:29,746 [INFO ] W-9000-my_model_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-my_model_1.0-stdout
2025-05-05T00:45:29,746 [INFO ] W-9000-my_model_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-my_model_1.0-stdout
2025-05-05T00:45:29,746 [INFO ] W-9000-my_model_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-my_model_1.0-stderr
2025-05-05T00:45:29,746 [INFO ] W-9000-my_model_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-my_model_1.0-stderr
2025-05-05T00:46:03,668 [DEBUG] W-9000-my_model_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/home/pupu/Desktop/project/.venv/bin/python, /home/pupu/Desktop/project/.venv/lib/python3.12/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9000, --metrics-config, /home/pupu/Desktop/project/.venv/lib/python3.12/site-packages/ts/configs/metrics.yaml]
2025-05-05T00:46:03,668 [DEBUG] W-9000-my_model_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/home/pupu/Desktop/project/.venv/bin/python, /home/pupu/Desktop/project/.venv/lib/python3.12/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9000, --metrics-config, /home/pupu/Desktop/project/.venv/lib/python3.12/site-packages/ts/configs/metrics.yaml]
2025-05-05T00:46:05,471 [INFO ] W-9000-my_model_1.0-stdout MODEL_LOG - s_name_part0=/tmp/.ts.sock, s_name_part1=9000, pid=9059
2025-05-05T00:46:05,472 [INFO ] W-9000-my_model_1.0-stdout MODEL_LOG - Listening on port: /tmp/.ts.sock.9000
2025-05-05T00:46:05,480 [INFO ] W-9000-my_model_1.0-stdout MODEL_LOG - Successfully loaded /home/pupu/Desktop/project/.venv/lib/python3.12/site-packages/ts/configs/metrics.yaml.
2025-05-05T00:46:05,481 [INFO ] W-9000-my_model_1.0-stdout MODEL_LOG - [PID]9059
2025-05-05T00:46:05,481 [INFO ] W-9000-my_model_1.0-stdout MODEL_LOG - Torch worker started.
2025-05-05T00:46:05,481 [DEBUG] W-9000-my_model_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-my_model_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2025-05-05T00:46:05,481 [DEBUG] W-9000-my_model_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-my_model_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2025-05-05T00:46:05,481 [INFO ] W-9000-my_model_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9000
2025-05-05T00:46:05,481 [INFO ] W-9000-my_model_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9000
2025-05-05T00:46:05,481 [INFO ] W-9000-my_model_1.0-stdout MODEL_LOG - Python runtime: 3.12.3
2025-05-05T00:46:05,482 [DEBUG] W-9000-my_model_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req.cmd LOAD repeats 1 to backend at: 1746387965482
2025-05-05T00:46:05,482 [DEBUG] W-9000-my_model_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req.cmd LOAD repeats 1 to backend at: 1746387965482
2025-05-05T00:46:05,482 [INFO ] W-9000-my_model_1.0-stdout MODEL_LOG - Connection accepted: /tmp/.ts.sock.9000.
2025-05-05T00:46:05,483 [INFO ] W-9000-my_model_1.0 org.pytorch.serve.wlm.WorkerThread - Looping backend response at: 1746387965483
2025-05-05T00:46:05,483 [INFO ] W-9000-my_model_1.0 org.pytorch.serve.wlm.WorkerThread - Looping backend response at: 1746387965483
2025-05-05T00:46:05,499 [INFO ] W-9000-my_model_1.0-stdout MODEL_LOG - model_name: my_model, batchSize: 1
2025-05-05T00:46:06,824 [INFO ] W-9000-my_model_1.0-stdout MODEL_LOG - OpenVINO is not enabled
2025-05-05T00:46:06,825 [INFO ] W-9000-my_model_1.0-stdout MODEL_LOG - proceeding without onnxruntime
2025-05-05T00:46:06,825 [INFO ] W-9000-my_model_1.0-stdout MODEL_LOG - Torch TensorRT not enabled
2025-05-05T00:46:06,826 [INFO ] W-9000-my_model_1.0-stdout MODEL_LOG - Failed to load model my_model, exception No model weights could be loaded
2025-05-05T00:46:06,826 [INFO ] W-9000-my_model_1.0-stdout MODEL_LOG - Traceback (most recent call last):
2025-05-05T00:46:06,826 [INFO ] W-9000-my_model_1.0-stdout MODEL_LOG -   File "/home/pupu/Desktop/project/.venv/lib/python3.12/site-packages/ts/model_service_worker.py", line 133, in load_model
2025-05-05T00:46:06,826 [INFO ] W-9000-my_model_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 1343
2025-05-05T00:46:06,826 [INFO ] W-9000-my_model_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 1343
2025-05-05T00:46:06,827 [DEBUG] W-9000-my_model_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-my_model_1.0 State change WORKER_STARTED -> WORKER_ERROR
2025-05-05T00:46:06,827 [DEBUG] W-9000-my_model_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-my_model_1.0 State change WORKER_STARTED -> WORKER_ERROR
2025-05-05T00:46:06,827 [INFO ] W-9000-my_model_1.0 TS_METRICS - WorkerThreadTime.Milliseconds:2.0|#Level:Host|#hostname:pupu-Stvullinc,timestamp:1746387966
2025-05-05T00:46:06,826 [INFO ] W-9000-my_model_1.0-stdout MODEL_LOG -     service = model_loader.load(
2025-05-05T00:46:06,827 [INFO ] W-9000-my_model_1.0-stdout MODEL_LOG -               ^^^^^^^^^^^^^^^^^^
2025-05-05T00:46:06,827 [INFO ] W-9000-my_model_1.0-stdout MODEL_LOG -   File "/home/pupu/Desktop/project/.venv/lib/python3.12/site-packages/ts/model_loader.py", line 143, in load
2025-05-05T00:46:06,827 [INFO ] W-9000-my_model_1.0-stdout MODEL_LOG -     initialize_fn(service.context)
2025-05-05T00:46:06,827 [INFO ] W-9000-my_model_1.0-stdout MODEL_LOG -   File "/tmp/models/1595b2e1bad84d53b7dc47d2998fd305/handler.py", line 19, in initialize
2025-05-05T00:46:06,827 [INFO ] W-9000-my_model_1.0-stdout MODEL_LOG -     super().initialize(context)
2025-05-05T00:46:06,827 [INFO ] epollEventLoopGroup-5-10 org.pytorch.serve.wlm.WorkerThread - 9000 Worker disconnected. WORKER_ERROR
2025-05-05T00:46:06,828 [INFO ] W-9000-my_model_1.0-stdout MODEL_LOG -   File "/home/pupu/Desktop/project/.venv/lib/python3.12/site-packages/ts/torch_handler/base_handler.py", line 223, in initialize
2025-05-05T00:46:06,827 [INFO ] epollEventLoopGroup-5-10 org.pytorch.serve.wlm.WorkerThread - 9000 Worker disconnected. WORKER_ERROR
2025-05-05T00:46:06,828 [INFO ] W-9000-my_model_1.0-stdout MODEL_LOG -     raise RuntimeError("No model weights could be loaded")
2025-05-05T00:46:06,828 [INFO ] W-9000-my_model_1.0-stdout MODEL_LOG - RuntimeError: No model weights could be loaded
2025-05-05T00:46:06,828 [DEBUG] W-9000-my_model_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_ERROR
2025-05-05T00:46:06,828 [INFO ] W-9000-my_model_1.0-stdout MODEL_LOG - Backend worker process died.
2025-05-05T00:46:06,828 [DEBUG] W-9000-my_model_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_ERROR
2025-05-05T00:46:06,828 [INFO ] W-9000-my_model_1.0-stdout MODEL_LOG - Traceback (most recent call last):
2025-05-05T00:46:06,828 [INFO ] W-9000-my_model_1.0-stdout MODEL_LOG -   File "/home/pupu/Desktop/project/.venv/lib/python3.12/site-packages/ts/model_service_worker.py", line 301, in <module>
2025-05-05T00:46:06,828 [INFO ] W-9000-my_model_1.0-stdout MODEL_LOG -     worker.run_server()
2025-05-05T00:46:06,828 [DEBUG] W-9000-my_model_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died., responseTimeout:120sec
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1770) ~[?:?]
	at java.util.concurrent.LinkedBlockingDeque.pollFirst(LinkedBlockingDeque.java:515) ~[?:?]
	at java.util.concurrent.LinkedBlockingDeque.poll(LinkedBlockingDeque.java:677) ~[?:?]
	at org.pytorch.serve.wlm.Model.pollBatch(Model.java:482) ~[model-server.jar:?]
	at org.pytorch.serve.wlm.BatchAggregator.pollBatch(BatchAggregator.java:189) ~[model-server.jar:?]
	at org.pytorch.serve.wlm.BatchAggregator.getRequest(BatchAggregator.java:36) ~[model-server.jar:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:195) ~[model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144) ~[?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642) ~[?:?]
	at java.lang.Thread.run(Thread.java:1583) [?:?]
2025-05-05T00:46:06,828 [INFO ] W-9000-my_model_1.0-stdout MODEL_LOG -   File "/home/pupu/Desktop/project/.venv/lib/python3.12/site-packages/ts/model_service_worker.py", line 268, in run_server
2025-05-05T00:46:06,828 [DEBUG] W-9000-my_model_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died., responseTimeout:120sec
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1770) ~[?:?]
	at java.util.concurrent.LinkedBlockingDeque.pollFirst(LinkedBlockingDeque.java:515) ~[?:?]
	at java.util.concurrent.LinkedBlockingDeque.poll(LinkedBlockingDeque.java:677) ~[?:?]
	at org.pytorch.serve.wlm.Model.pollBatch(Model.java:482) ~[model-server.jar:?]
	at org.pytorch.serve.wlm.BatchAggregator.pollBatch(BatchAggregator.java:189) ~[model-server.jar:?]
	at org.pytorch.serve.wlm.BatchAggregator.getRequest(BatchAggregator.java:36) ~[model-server.jar:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:195) ~[model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144) ~[?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642) ~[?:?]
	at java.lang.Thread.run(Thread.java:1583) [?:?]
2025-05-05T00:46:06,829 [INFO ] W-9000-my_model_1.0-stdout MODEL_LOG -     self.handle_connection(cl_socket)
2025-05-05T00:46:06,829 [DEBUG] W-9000-my_model_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-my_model_1.0 State change WORKER_ERROR -> WORKER_STOPPED
2025-05-05T00:46:06,829 [DEBUG] W-9000-my_model_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-my_model_1.0 State change WORKER_ERROR -> WORKER_STOPPED
2025-05-05T00:46:06,829 [WARN ] W-9000-my_model_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery failed again
2025-05-05T00:46:06,829 [WARN ] W-9000-my_model_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery failed again
2025-05-05T00:46:06,829 [INFO ] W-9000-my_model_1.0-stdout MODEL_LOG -   File "/home/pupu/Desktop/project/.venv/lib/python3.12/site-packages/ts/model_service_worker.py", line 201, in handle_connection
2025-05-05T00:46:06,829 [INFO ] W-9000-my_model_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9000 in 55 seconds.
2025-05-05T00:46:06,829 [INFO ] W-9000-my_model_1.0-stdout MODEL_LOG -     raise RuntimeError("{} - {}".format(code, result))
2025-05-05T00:46:06,829 [INFO ] W-9000-my_model_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9000 in 55 seconds.
2025-05-05T00:46:06,830 [INFO ] W-9000-my_model_1.0-stdout MODEL_LOG - RuntimeError: 500 - Unknown exception
2025-05-05T00:46:06,830 [INFO ] W-9000-my_model_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-my_model_1.0-stdout
2025-05-05T00:46:06,830 [INFO ] W-9000-my_model_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-my_model_1.0-stdout
2025-05-05T00:46:06,923 [INFO ] W-9000-my_model_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-my_model_1.0-stderr
2025-05-05T00:46:06,923 [INFO ] W-9000-my_model_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-my_model_1.0-stderr
2025-05-05T01:00:46,182 [DEBUG] main org.pytorch.serve.util.ConfigManager - xpu-smi not available or failed: Cannot run program "xpu-smi": error=2, No such file or directory
2025-05-05T01:00:46,182 [DEBUG] main org.pytorch.serve.util.ConfigManager - xpu-smi not available or failed: Cannot run program "xpu-smi": error=2, No such file or directory
2025-05-05T01:00:46,187 [WARN ] main org.pytorch.serve.util.ConfigManager - Your torchserve instance can access any URL to load models. When deploying to production, make sure to limit the set of allowed_urls in config.properties
2025-05-05T01:00:46,187 [WARN ] main org.pytorch.serve.util.ConfigManager - Your torchserve instance can access any URL to load models. When deploying to production, make sure to limit the set of allowed_urls in config.properties
2025-05-05T01:00:46,210 [INFO ] main org.pytorch.serve.servingsdk.impl.PluginsManager - Initializing plugins manager...
2025-05-05T01:00:46,210 [INFO ] main org.pytorch.serve.servingsdk.impl.PluginsManager - Initializing plugins manager...
2025-05-05T01:00:46,279 [INFO ] main org.pytorch.serve.metrics.configuration.MetricConfiguration - Successfully loaded metrics configuration from /home/pupu/Desktop/project/.venv/lib/python3.12/site-packages/ts/configs/metrics.yaml
2025-05-05T01:00:46,279 [INFO ] main org.pytorch.serve.metrics.configuration.MetricConfiguration - Successfully loaded metrics configuration from /home/pupu/Desktop/project/.venv/lib/python3.12/site-packages/ts/configs/metrics.yaml
2025-05-05T01:00:46,362 [INFO ] main org.pytorch.serve.ModelServer - 
Torchserve version: 0.12.0
TS Home: /home/pupu/Desktop/project/.venv/lib/python3.12/site-packages
Current directory: /home/pupu/Desktop/project
Temp directory: /tmp
Metrics config path: /home/pupu/Desktop/project/.venv/lib/python3.12/site-packages/ts/configs/metrics.yaml
Number of GPUs: 1
Number of CPUs: 8
Max heap size: 3990 M
Python executable: /home/pupu/Desktop/project/.venv/bin/python
Config file: N/A
Inference address: http://127.0.0.1:8080
Management address: http://127.0.0.1:8081
Metrics address: http://127.0.0.1:8082
Model Store: /home/pupu/Desktop/project/there_is_our_server
Initial Models: my_model=my_model.mar
Log dir: /home/pupu/Desktop/project/logs
Metrics dir: /home/pupu/Desktop/project/logs
Netty threads: 0
Netty client threads: 0
Default workers per model: 1
Blacklist Regex: N/A
Maximum Response Size: 6553500
Maximum Request Size: 6553500
Limit Maximum Image Pixels: true
Prefer direct buffer: false
Allowed Urls: [file://.*|http(s)?://.*]
Custom python dependency for model allowed: false
Enable metrics API: true
Metrics mode: LOG
Disable system metrics: false
Workflow Store: /home/pupu/Desktop/project/there_is_our_server
CPP log config: N/A
Model config: N/A
System metrics command: default
Model API enabled: false
2025-05-05T01:00:46,362 [INFO ] main org.pytorch.serve.ModelServer - 
Torchserve version: 0.12.0
TS Home: /home/pupu/Desktop/project/.venv/lib/python3.12/site-packages
Current directory: /home/pupu/Desktop/project
Temp directory: /tmp
Metrics config path: /home/pupu/Desktop/project/.venv/lib/python3.12/site-packages/ts/configs/metrics.yaml
Number of GPUs: 1
Number of CPUs: 8
Max heap size: 3990 M
Python executable: /home/pupu/Desktop/project/.venv/bin/python
Config file: N/A
Inference address: http://127.0.0.1:8080
Management address: http://127.0.0.1:8081
Metrics address: http://127.0.0.1:8082
Model Store: /home/pupu/Desktop/project/there_is_our_server
Initial Models: my_model=my_model.mar
Log dir: /home/pupu/Desktop/project/logs
Metrics dir: /home/pupu/Desktop/project/logs
Netty threads: 0
Netty client threads: 0
Default workers per model: 1
Blacklist Regex: N/A
Maximum Response Size: 6553500
Maximum Request Size: 6553500
Limit Maximum Image Pixels: true
Prefer direct buffer: false
Allowed Urls: [file://.*|http(s)?://.*]
Custom python dependency for model allowed: false
Enable metrics API: true
Metrics mode: LOG
Disable system metrics: false
Workflow Store: /home/pupu/Desktop/project/there_is_our_server
CPP log config: N/A
Model config: N/A
System metrics command: default
Model API enabled: false
2025-05-05T01:00:46,372 [INFO ] main org.pytorch.serve.servingsdk.impl.PluginsManager -  Loading snapshot serializer plugin...
2025-05-05T01:00:46,372 [INFO ] main org.pytorch.serve.servingsdk.impl.PluginsManager -  Loading snapshot serializer plugin...
2025-05-05T01:00:46,394 [INFO ] main org.pytorch.serve.ModelServer - Loading initial models: my_model.mar
2025-05-05T01:00:46,394 [INFO ] main org.pytorch.serve.ModelServer - Loading initial models: my_model.mar
2025-05-05T01:00:47,568 [DEBUG] main org.pytorch.serve.wlm.ModelVersionedRefs - Adding new version 1.0 for model my_model
2025-05-05T01:00:47,568 [DEBUG] main org.pytorch.serve.wlm.ModelVersionedRefs - Adding new version 1.0 for model my_model
2025-05-05T01:00:47,568 [DEBUG] main org.pytorch.serve.wlm.ModelVersionedRefs - Setting default version to 1.0 for model my_model
2025-05-05T01:00:47,568 [DEBUG] main org.pytorch.serve.wlm.ModelVersionedRefs - Setting default version to 1.0 for model my_model
2025-05-05T01:00:47,568 [INFO ] main org.pytorch.serve.wlm.ModelManager - Model my_model loaded.
2025-05-05T01:00:47,568 [INFO ] main org.pytorch.serve.wlm.ModelManager - Model my_model loaded.
2025-05-05T01:00:47,569 [DEBUG] main org.pytorch.serve.wlm.ModelManager - updateModel: my_model, count: 1
2025-05-05T01:00:47,569 [DEBUG] main org.pytorch.serve.wlm.ModelManager - updateModel: my_model, count: 1
2025-05-05T01:00:47,576 [DEBUG] W-9000-my_model_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/home/pupu/Desktop/project/.venv/bin/python, /home/pupu/Desktop/project/.venv/lib/python3.12/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9000, --metrics-config, /home/pupu/Desktop/project/.venv/lib/python3.12/site-packages/ts/configs/metrics.yaml]
2025-05-05T01:00:47,576 [DEBUG] W-9000-my_model_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/home/pupu/Desktop/project/.venv/bin/python, /home/pupu/Desktop/project/.venv/lib/python3.12/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9000, --metrics-config, /home/pupu/Desktop/project/.venv/lib/python3.12/site-packages/ts/configs/metrics.yaml]
2025-05-05T01:00:47,576 [INFO ] main org.pytorch.serve.ModelServer - Initialize Inference server with: EpollServerSocketChannel.
2025-05-05T01:00:47,576 [INFO ] main org.pytorch.serve.ModelServer - Initialize Inference server with: EpollServerSocketChannel.
2025-05-05T01:00:47,639 [INFO ] main org.pytorch.serve.ModelServer - Inference API bind to: http://127.0.0.1:8080
2025-05-05T01:00:47,639 [INFO ] main org.pytorch.serve.ModelServer - Inference API bind to: http://127.0.0.1:8080
2025-05-05T01:00:47,639 [INFO ] main org.pytorch.serve.ModelServer - Initialize Management server with: EpollServerSocketChannel.
2025-05-05T01:00:47,639 [INFO ] main org.pytorch.serve.ModelServer - Initialize Management server with: EpollServerSocketChannel.
2025-05-05T01:00:47,640 [INFO ] main org.pytorch.serve.ModelServer - Management API bind to: http://127.0.0.1:8081
2025-05-05T01:00:47,640 [INFO ] main org.pytorch.serve.ModelServer - Management API bind to: http://127.0.0.1:8081
2025-05-05T01:00:47,641 [INFO ] main org.pytorch.serve.ModelServer - Initialize Metrics server with: EpollServerSocketChannel.
2025-05-05T01:00:47,641 [INFO ] main org.pytorch.serve.ModelServer - Initialize Metrics server with: EpollServerSocketChannel.
2025-05-05T01:00:47,641 [INFO ] main org.pytorch.serve.ModelServer - Metrics API bind to: http://127.0.0.1:8082
2025-05-05T01:00:47,641 [INFO ] main org.pytorch.serve.ModelServer - Metrics API bind to: http://127.0.0.1:8082
2025-05-05T01:00:47,847 [WARN ] pool-3-thread-1 org.pytorch.serve.metrics.MetricCollector - worker pid is not available yet.
2025-05-05T01:00:47,847 [WARN ] pool-3-thread-1 org.pytorch.serve.metrics.MetricCollector - worker pid is not available yet.
2025-05-05T01:00:48,383 [INFO ] pool-3-thread-1 TS_METRICS - CPUUtilization.Percent:50.0|#Level:Host|#hostname:pupu-Stvullinc,timestamp:1746388848
2025-05-05T01:00:48,385 [INFO ] pool-3-thread-1 TS_METRICS - DiskAvailable.Gigabytes:75.4417839050293|#Level:Host|#hostname:pupu-Stvullinc,timestamp:1746388848
2025-05-05T01:00:48,385 [INFO ] pool-3-thread-1 TS_METRICS - DiskUsage.Gigabytes:62.74006271362305|#Level:Host|#hostname:pupu-Stvullinc,timestamp:1746388848
2025-05-05T01:00:48,385 [INFO ] pool-3-thread-1 TS_METRICS - DiskUtilization.Percent:45.4|#Level:Host|#hostname:pupu-Stvullinc,timestamp:1746388848
2025-05-05T01:00:48,386 [INFO ] pool-3-thread-1 TS_METRICS - GPUMemoryUtilization.Percent:3.5888671875|#Level:Host,DeviceId:0|#hostname:pupu-Stvullinc,timestamp:1746388848
2025-05-05T01:00:48,386 [INFO ] pool-3-thread-1 TS_METRICS - GPUMemoryUsed.Megabytes:294.0|#Level:Host,DeviceId:0|#hostname:pupu-Stvullinc,timestamp:1746388848
2025-05-05T01:00:48,387 [INFO ] pool-3-thread-1 TS_METRICS - GPUUtilization.Percent:6.0|#Level:Host,DeviceId:0|#hostname:pupu-Stvullinc,timestamp:1746388848
2025-05-05T01:00:48,387 [INFO ] pool-3-thread-1 TS_METRICS - MemoryAvailable.Megabytes:9302.29296875|#Level:Host|#hostname:pupu-Stvullinc,timestamp:1746388848
2025-05-05T01:00:48,388 [INFO ] pool-3-thread-1 TS_METRICS - MemoryUsed.Megabytes:6210.6015625|#Level:Host|#hostname:pupu-Stvullinc,timestamp:1746388848
2025-05-05T01:00:48,388 [INFO ] pool-3-thread-1 TS_METRICS - MemoryUtilization.Percent:41.7|#Level:Host|#hostname:pupu-Stvullinc,timestamp:1746388848
2025-05-05T01:00:49,467 [INFO ] W-9000-my_model_1.0-stdout MODEL_LOG - s_name_part0=/tmp/.ts.sock, s_name_part1=9000, pid=9593
2025-05-05T01:00:49,469 [INFO ] W-9000-my_model_1.0-stdout MODEL_LOG - Listening on port: /tmp/.ts.sock.9000
2025-05-05T01:00:49,476 [INFO ] W-9000-my_model_1.0-stdout MODEL_LOG - Successfully loaded /home/pupu/Desktop/project/.venv/lib/python3.12/site-packages/ts/configs/metrics.yaml.
2025-05-05T01:00:49,476 [INFO ] W-9000-my_model_1.0-stdout MODEL_LOG - [PID]9593
2025-05-05T01:00:49,476 [INFO ] W-9000-my_model_1.0-stdout MODEL_LOG - Torch worker started.
2025-05-05T01:00:49,477 [INFO ] W-9000-my_model_1.0-stdout MODEL_LOG - Python runtime: 3.12.3
2025-05-05T01:00:49,477 [DEBUG] W-9000-my_model_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-my_model_1.0 State change null -> WORKER_STARTED
2025-05-05T01:00:49,477 [DEBUG] W-9000-my_model_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-my_model_1.0 State change null -> WORKER_STARTED
2025-05-05T01:00:49,483 [INFO ] W-9000-my_model_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9000
2025-05-05T01:00:49,483 [INFO ] W-9000-my_model_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9000
2025-05-05T01:00:49,492 [INFO ] W-9000-my_model_1.0-stdout MODEL_LOG - Connection accepted: /tmp/.ts.sock.9000.
2025-05-05T01:00:49,496 [DEBUG] W-9000-my_model_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req.cmd LOAD repeats 1 to backend at: 1746388849496
2025-05-05T01:00:49,496 [DEBUG] W-9000-my_model_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req.cmd LOAD repeats 1 to backend at: 1746388849496
2025-05-05T01:00:49,499 [INFO ] W-9000-my_model_1.0 org.pytorch.serve.wlm.WorkerThread - Looping backend response at: 1746388849499
2025-05-05T01:00:49,499 [INFO ] W-9000-my_model_1.0 org.pytorch.serve.wlm.WorkerThread - Looping backend response at: 1746388849499
2025-05-05T01:00:49,532 [INFO ] W-9000-my_model_1.0-stdout MODEL_LOG - model_name: my_model, batchSize: 1
2025-05-05T01:00:50,895 [INFO ] W-9000-my_model_1.0-stdout MODEL_LOG - OpenVINO is not enabled
2025-05-05T01:00:50,896 [INFO ] W-9000-my_model_1.0-stdout MODEL_LOG - proceeding without onnxruntime
2025-05-05T01:00:50,896 [INFO ] W-9000-my_model_1.0-stdout MODEL_LOG - Torch TensorRT not enabled
2025-05-05T01:00:50,897 [INFO ] W-9000-my_model_1.0-stdout MODEL_LOG - Failed to load model my_model, exception No model weights could be loaded
2025-05-05T01:00:50,898 [INFO ] W-9000-my_model_1.0-stdout MODEL_LOG - Traceback (most recent call last):
2025-05-05T01:00:50,898 [INFO ] W-9000-my_model_1.0-stdout MODEL_LOG -   File "/home/pupu/Desktop/project/.venv/lib/python3.12/site-packages/ts/model_service_worker.py", line 133, in load_model
2025-05-05T01:00:50,898 [INFO ] W-9000-my_model_1.0-stdout MODEL_LOG -     service = model_loader.load(
2025-05-05T01:00:50,898 [INFO ] W-9000-my_model_1.0-stdout MODEL_LOG -               ^^^^^^^^^^^^^^^^^^
2025-05-05T01:00:50,899 [INFO ] W-9000-my_model_1.0-stdout MODEL_LOG -   File "/home/pupu/Desktop/project/.venv/lib/python3.12/site-packages/ts/model_loader.py", line 143, in load
2025-05-05T01:00:50,899 [INFO ] W-9000-my_model_1.0-stdout MODEL_LOG -     initialize_fn(service.context)
2025-05-05T01:00:50,899 [INFO ] W-9000-my_model_1.0-stdout MODEL_LOG -   File "/tmp/models/be6f4d4e933d47cd83e5258e9ca51515/handler.py", line 28, in initialize
2025-05-05T01:00:50,899 [INFO ] W-9000-my_model_1.0-stdout MODEL_LOG -     super().initialize(context)
2025-05-05T01:00:50,900 [INFO ] W-9000-my_model_1.0-stdout MODEL_LOG -   File "/home/pupu/Desktop/project/.venv/lib/python3.12/site-packages/ts/torch_handler/base_handler.py", line 223, in initialize
2025-05-05T01:00:50,900 [INFO ] W-9000-my_model_1.0-stdout MODEL_LOG -     raise RuntimeError("No model weights could be loaded")
2025-05-05T01:00:50,900 [INFO ] W-9000-my_model_1.0-stdout MODEL_LOG - RuntimeError: No model weights could be loaded
2025-05-05T01:00:50,900 [INFO ] W-9000-my_model_1.0-stdout MODEL_LOG - Backend worker process died.
2025-05-05T01:00:50,900 [INFO ] W-9000-my_model_1.0-stdout MODEL_LOG - Traceback (most recent call last):
2025-05-05T01:00:50,901 [INFO ] W-9000-my_model_1.0-stdout MODEL_LOG -   File "/home/pupu/Desktop/project/.venv/lib/python3.12/site-packages/ts/model_service_worker.py", line 301, in <module>
2025-05-05T01:00:50,901 [INFO ] W-9000-my_model_1.0-stdout MODEL_LOG -     worker.run_server()
2025-05-05T01:00:50,901 [INFO ] W-9000-my_model_1.0-stdout MODEL_LOG -   File "/home/pupu/Desktop/project/.venv/lib/python3.12/site-packages/ts/model_service_worker.py", line 268, in run_server
2025-05-05T01:00:50,901 [INFO ] W-9000-my_model_1.0-stdout MODEL_LOG -     self.handle_connection(cl_socket)
2025-05-05T01:00:50,901 [INFO ] W-9000-my_model_1.0-stdout MODEL_LOG -   File "/home/pupu/Desktop/project/.venv/lib/python3.12/site-packages/ts/model_service_worker.py", line 201, in handle_connection
2025-05-05T01:00:50,902 [INFO ] W-9000-my_model_1.0-stdout MODEL_LOG -     raise RuntimeError("{} - {}".format(code, result))
2025-05-05T01:00:50,902 [INFO ] W-9000-my_model_1.0-stdout MODEL_LOG - RuntimeError: 500 - Unknown exception
2025-05-05T01:00:50,905 [INFO ] W-9000-my_model_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 1406
2025-05-05T01:00:50,905 [INFO ] W-9000-my_model_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 1406
2025-05-05T01:00:50,906 [DEBUG] W-9000-my_model_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-my_model_1.0 State change WORKER_STARTED -> WORKER_ERROR
2025-05-05T01:00:50,906 [DEBUG] W-9000-my_model_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-my_model_1.0 State change WORKER_STARTED -> WORKER_ERROR
2025-05-05T01:00:50,906 [INFO ] epollEventLoopGroup-5-1 org.pytorch.serve.wlm.WorkerThread - 9000 Worker disconnected. WORKER_STARTED
2025-05-05T01:00:50,906 [INFO ] W-9000-my_model_1.0 TS_METRICS - WorkerThreadTime.Milliseconds:4.0|#Level:Host|#hostname:pupu-Stvullinc,timestamp:1746388850
2025-05-05T01:00:50,906 [INFO ] epollEventLoopGroup-5-1 org.pytorch.serve.wlm.WorkerThread - 9000 Worker disconnected. WORKER_STARTED
2025-05-05T01:00:50,906 [DEBUG] W-9000-my_model_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_ERROR
2025-05-05T01:00:50,906 [DEBUG] W-9000-my_model_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_ERROR
2025-05-05T01:00:50,907 [DEBUG] W-9000-my_model_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died., responseTimeout:120sec
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1770) ~[?:?]
	at java.util.concurrent.LinkedBlockingDeque.pollFirst(LinkedBlockingDeque.java:515) ~[?:?]
	at java.util.concurrent.LinkedBlockingDeque.poll(LinkedBlockingDeque.java:677) ~[?:?]
	at org.pytorch.serve.wlm.Model.pollBatch(Model.java:482) ~[model-server.jar:?]
	at org.pytorch.serve.wlm.BatchAggregator.pollBatch(BatchAggregator.java:189) ~[model-server.jar:?]
	at org.pytorch.serve.wlm.BatchAggregator.getRequest(BatchAggregator.java:36) ~[model-server.jar:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:195) ~[model-server.jar:?]
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:572) ~[?:?]
	at java.util.concurrent.FutureTask.run(FutureTask.java:317) ~[?:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144) ~[?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642) ~[?:?]
	at java.lang.Thread.run(Thread.java:1583) [?:?]
2025-05-05T01:00:50,907 [DEBUG] W-9000-my_model_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died., responseTimeout:120sec
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1770) ~[?:?]
	at java.util.concurrent.LinkedBlockingDeque.pollFirst(LinkedBlockingDeque.java:515) ~[?:?]
	at java.util.concurrent.LinkedBlockingDeque.poll(LinkedBlockingDeque.java:677) ~[?:?]
	at org.pytorch.serve.wlm.Model.pollBatch(Model.java:482) ~[model-server.jar:?]
	at org.pytorch.serve.wlm.BatchAggregator.pollBatch(BatchAggregator.java:189) ~[model-server.jar:?]
	at org.pytorch.serve.wlm.BatchAggregator.getRequest(BatchAggregator.java:36) ~[model-server.jar:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:195) ~[model-server.jar:?]
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:572) ~[?:?]
	at java.util.concurrent.FutureTask.run(FutureTask.java:317) ~[?:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144) ~[?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642) ~[?:?]
	at java.lang.Thread.run(Thread.java:1583) [?:?]
2025-05-05T01:00:50,918 [DEBUG] W-9000-my_model_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-my_model_1.0 State change WORKER_ERROR -> WORKER_STOPPED
2025-05-05T01:00:50,918 [DEBUG] W-9000-my_model_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-my_model_1.0 State change WORKER_ERROR -> WORKER_STOPPED
2025-05-05T01:00:50,918 [INFO ] W-9000-my_model_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery start timestamp: 1746388850918
2025-05-05T01:00:50,918 [INFO ] W-9000-my_model_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery start timestamp: 1746388850918
2025-05-05T01:00:50,919 [INFO ] W-9000-my_model_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9000 in 1 seconds.
2025-05-05T01:00:50,919 [INFO ] W-9000-my_model_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9000 in 1 seconds.
2025-05-05T01:00:50,954 [INFO ] W-9000-my_model_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-my_model_1.0-stdout
2025-05-05T01:00:50,954 [INFO ] W-9000-my_model_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-my_model_1.0-stdout
2025-05-05T01:00:50,954 [INFO ] W-9000-my_model_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-my_model_1.0-stderr
2025-05-05T01:00:50,954 [INFO ] W-9000-my_model_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-my_model_1.0-stderr
2025-05-05T01:00:51,920 [DEBUG] W-9000-my_model_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/home/pupu/Desktop/project/.venv/bin/python, /home/pupu/Desktop/project/.venv/lib/python3.12/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9000, --metrics-config, /home/pupu/Desktop/project/.venv/lib/python3.12/site-packages/ts/configs/metrics.yaml]
2025-05-05T01:00:51,920 [DEBUG] W-9000-my_model_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/home/pupu/Desktop/project/.venv/bin/python, /home/pupu/Desktop/project/.venv/lib/python3.12/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9000, --metrics-config, /home/pupu/Desktop/project/.venv/lib/python3.12/site-packages/ts/configs/metrics.yaml]
2025-05-05T01:00:53,772 [INFO ] W-9000-my_model_1.0-stdout MODEL_LOG - s_name_part0=/tmp/.ts.sock, s_name_part1=9000, pid=9634
2025-05-05T01:00:53,773 [INFO ] W-9000-my_model_1.0-stdout MODEL_LOG - Listening on port: /tmp/.ts.sock.9000
2025-05-05T01:00:53,780 [INFO ] W-9000-my_model_1.0-stdout MODEL_LOG - Successfully loaded /home/pupu/Desktop/project/.venv/lib/python3.12/site-packages/ts/configs/metrics.yaml.
2025-05-05T01:00:53,780 [INFO ] W-9000-my_model_1.0-stdout MODEL_LOG - [PID]9634
2025-05-05T01:00:53,781 [DEBUG] W-9000-my_model_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-my_model_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2025-05-05T01:00:53,780 [INFO ] W-9000-my_model_1.0-stdout MODEL_LOG - Torch worker started.
2025-05-05T01:00:53,781 [DEBUG] W-9000-my_model_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-my_model_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2025-05-05T01:00:53,781 [INFO ] W-9000-my_model_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9000
2025-05-05T01:00:53,781 [INFO ] W-9000-my_model_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9000
2025-05-05T01:00:53,781 [INFO ] W-9000-my_model_1.0-stdout MODEL_LOG - Python runtime: 3.12.3
2025-05-05T01:00:53,782 [DEBUG] W-9000-my_model_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req.cmd LOAD repeats 1 to backend at: 1746388853782
2025-05-05T01:00:53,782 [INFO ] W-9000-my_model_1.0-stdout MODEL_LOG - Connection accepted: /tmp/.ts.sock.9000.
2025-05-05T01:00:53,782 [DEBUG] W-9000-my_model_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req.cmd LOAD repeats 1 to backend at: 1746388853782
2025-05-05T01:00:53,782 [INFO ] W-9000-my_model_1.0 org.pytorch.serve.wlm.WorkerThread - Looping backend response at: 1746388853782
2025-05-05T01:00:53,782 [INFO ] W-9000-my_model_1.0 org.pytorch.serve.wlm.WorkerThread - Looping backend response at: 1746388853782
2025-05-05T01:00:53,808 [INFO ] W-9000-my_model_1.0-stdout MODEL_LOG - model_name: my_model, batchSize: 1
2025-05-05T01:00:55,162 [INFO ] W-9000-my_model_1.0-stdout MODEL_LOG - OpenVINO is not enabled
2025-05-05T01:00:55,163 [INFO ] W-9000-my_model_1.0-stdout MODEL_LOG - proceeding without onnxruntime
2025-05-05T01:00:55,164 [INFO ] W-9000-my_model_1.0-stdout MODEL_LOG - Torch TensorRT not enabled
2025-05-05T01:00:55,165 [INFO ] W-9000-my_model_1.0-stdout MODEL_LOG - Failed to load model my_model, exception No model weights could be loaded
2025-05-05T01:00:55,165 [INFO ] W-9000-my_model_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 1382
2025-05-05T01:00:55,165 [INFO ] W-9000-my_model_1.0-stdout MODEL_LOG - Traceback (most recent call last):
2025-05-05T01:00:55,165 [INFO ] W-9000-my_model_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 1382
2025-05-05T01:00:55,165 [DEBUG] W-9000-my_model_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-my_model_1.0 State change WORKER_STARTED -> WORKER_ERROR
2025-05-05T01:00:55,165 [DEBUG] W-9000-my_model_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-my_model_1.0 State change WORKER_STARTED -> WORKER_ERROR
2025-05-05T01:00:55,165 [INFO ] W-9000-my_model_1.0-stdout MODEL_LOG -   File "/home/pupu/Desktop/project/.venv/lib/python3.12/site-packages/ts/model_service_worker.py", line 133, in load_model
2025-05-05T01:00:55,165 [INFO ] W-9000-my_model_1.0-stdout MODEL_LOG -     service = model_loader.load(
2025-05-05T01:00:55,165 [INFO ] W-9000-my_model_1.0 TS_METRICS - WorkerThreadTime.Milliseconds:1.0|#Level:Host|#hostname:pupu-Stvullinc,timestamp:1746388855
2025-05-05T01:00:55,166 [INFO ] W-9000-my_model_1.0-stdout MODEL_LOG -               ^^^^^^^^^^^^^^^^^^
2025-05-05T01:00:55,166 [INFO ] epollEventLoopGroup-5-2 org.pytorch.serve.wlm.WorkerThread - 9000 Worker disconnected. WORKER_ERROR
2025-05-05T01:00:55,166 [INFO ] W-9000-my_model_1.0-stdout MODEL_LOG -   File "/home/pupu/Desktop/project/.venv/lib/python3.12/site-packages/ts/model_loader.py", line 143, in load
2025-05-05T01:00:55,166 [INFO ] epollEventLoopGroup-5-2 org.pytorch.serve.wlm.WorkerThread - 9000 Worker disconnected. WORKER_ERROR
2025-05-05T01:00:55,166 [INFO ] W-9000-my_model_1.0-stdout MODEL_LOG -     initialize_fn(service.context)
2025-05-05T01:00:55,166 [DEBUG] W-9000-my_model_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_ERROR
2025-05-05T01:00:55,166 [DEBUG] W-9000-my_model_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_ERROR
2025-05-05T01:00:55,166 [INFO ] W-9000-my_model_1.0-stdout MODEL_LOG -   File "/tmp/models/be6f4d4e933d47cd83e5258e9ca51515/handler.py", line 28, in initialize
2025-05-05T01:00:55,167 [INFO ] W-9000-my_model_1.0-stdout MODEL_LOG -     super().initialize(context)
2025-05-05T01:00:55,166 [DEBUG] W-9000-my_model_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died., responseTimeout:120sec
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1770) ~[?:?]
	at java.util.concurrent.LinkedBlockingDeque.pollFirst(LinkedBlockingDeque.java:515) ~[?:?]
	at java.util.concurrent.LinkedBlockingDeque.poll(LinkedBlockingDeque.java:677) ~[?:?]
	at org.pytorch.serve.wlm.Model.pollBatch(Model.java:482) ~[model-server.jar:?]
	at org.pytorch.serve.wlm.BatchAggregator.pollBatch(BatchAggregator.java:189) ~[model-server.jar:?]
	at org.pytorch.serve.wlm.BatchAggregator.getRequest(BatchAggregator.java:36) ~[model-server.jar:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:195) ~[model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144) ~[?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642) ~[?:?]
	at java.lang.Thread.run(Thread.java:1583) [?:?]
2025-05-05T01:00:55,167 [INFO ] W-9000-my_model_1.0-stdout MODEL_LOG -   File "/home/pupu/Desktop/project/.venv/lib/python3.12/site-packages/ts/torch_handler/base_handler.py", line 223, in initialize
2025-05-05T01:00:55,166 [DEBUG] W-9000-my_model_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died., responseTimeout:120sec
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1770) ~[?:?]
	at java.util.concurrent.LinkedBlockingDeque.pollFirst(LinkedBlockingDeque.java:515) ~[?:?]
	at java.util.concurrent.LinkedBlockingDeque.poll(LinkedBlockingDeque.java:677) ~[?:?]
	at org.pytorch.serve.wlm.Model.pollBatch(Model.java:482) ~[model-server.jar:?]
	at org.pytorch.serve.wlm.BatchAggregator.pollBatch(BatchAggregator.java:189) ~[model-server.jar:?]
	at org.pytorch.serve.wlm.BatchAggregator.getRequest(BatchAggregator.java:36) ~[model-server.jar:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:195) ~[model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144) ~[?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642) ~[?:?]
	at java.lang.Thread.run(Thread.java:1583) [?:?]
2025-05-05T01:00:55,167 [INFO ] W-9000-my_model_1.0-stdout MODEL_LOG -     raise RuntimeError("No model weights could be loaded")
2025-05-05T01:00:55,167 [DEBUG] W-9000-my_model_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-my_model_1.0 State change WORKER_ERROR -> WORKER_STOPPED
2025-05-05T01:00:55,167 [DEBUG] W-9000-my_model_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-my_model_1.0 State change WORKER_ERROR -> WORKER_STOPPED
2025-05-05T01:00:55,167 [INFO ] W-9000-my_model_1.0-stdout MODEL_LOG - RuntimeError: No model weights could be loaded
2025-05-05T01:00:55,167 [WARN ] W-9000-my_model_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery failed again
2025-05-05T01:00:55,167 [WARN ] W-9000-my_model_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery failed again
2025-05-05T01:00:55,167 [INFO ] W-9000-my_model_1.0-stdout MODEL_LOG - Backend worker process died.
2025-05-05T01:00:55,168 [INFO ] W-9000-my_model_1.0-stdout MODEL_LOG - Traceback (most recent call last):
2025-05-05T01:00:55,168 [INFO ] W-9000-my_model_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9000 in 1 seconds.
2025-05-05T01:00:55,168 [INFO ] W-9000-my_model_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9000 in 1 seconds.
2025-05-05T01:00:55,168 [INFO ] W-9000-my_model_1.0-stdout MODEL_LOG -   File "/home/pupu/Desktop/project/.venv/lib/python3.12/site-packages/ts/model_service_worker.py", line 301, in <module>
2025-05-05T01:00:55,168 [INFO ] W-9000-my_model_1.0-stdout MODEL_LOG -     worker.run_server()
2025-05-05T01:00:55,168 [INFO ] W-9000-my_model_1.0-stdout MODEL_LOG -   File "/home/pupu/Desktop/project/.venv/lib/python3.12/site-packages/ts/model_service_worker.py", line 268, in run_server
2025-05-05T01:00:55,168 [INFO ] W-9000-my_model_1.0-stdout MODEL_LOG -     self.handle_connection(cl_socket)
2025-05-05T01:00:55,169 [INFO ] W-9000-my_model_1.0-stdout MODEL_LOG -   File "/home/pupu/Desktop/project/.venv/lib/python3.12/site-packages/ts/model_service_worker.py", line 201, in handle_connection
2025-05-05T01:00:55,169 [INFO ] W-9000-my_model_1.0-stdout MODEL_LOG -     raise RuntimeError("{} - {}".format(code, result))
2025-05-05T01:00:55,169 [INFO ] W-9000-my_model_1.0-stdout MODEL_LOG - RuntimeError: 500 - Unknown exception
2025-05-05T01:00:55,169 [INFO ] W-9000-my_model_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-my_model_1.0-stdout
2025-05-05T01:00:55,169 [INFO ] W-9000-my_model_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-my_model_1.0-stdout
2025-05-05T01:00:55,259 [INFO ] W-9000-my_model_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-my_model_1.0-stderr
2025-05-05T01:00:55,259 [INFO ] W-9000-my_model_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-my_model_1.0-stderr
2025-05-05T01:00:56,169 [DEBUG] W-9000-my_model_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/home/pupu/Desktop/project/.venv/bin/python, /home/pupu/Desktop/project/.venv/lib/python3.12/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9000, --metrics-config, /home/pupu/Desktop/project/.venv/lib/python3.12/site-packages/ts/configs/metrics.yaml]
2025-05-05T01:00:56,169 [DEBUG] W-9000-my_model_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/home/pupu/Desktop/project/.venv/bin/python, /home/pupu/Desktop/project/.venv/lib/python3.12/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9000, --metrics-config, /home/pupu/Desktop/project/.venv/lib/python3.12/site-packages/ts/configs/metrics.yaml]
2025-05-05T01:00:58,022 [INFO ] W-9000-my_model_1.0-stdout MODEL_LOG - s_name_part0=/tmp/.ts.sock, s_name_part1=9000, pid=9656
2025-05-05T01:00:58,023 [INFO ] W-9000-my_model_1.0-stdout MODEL_LOG - Listening on port: /tmp/.ts.sock.9000
2025-05-05T01:00:58,032 [INFO ] W-9000-my_model_1.0-stdout MODEL_LOG - Successfully loaded /home/pupu/Desktop/project/.venv/lib/python3.12/site-packages/ts/configs/metrics.yaml.
2025-05-05T01:00:58,033 [INFO ] W-9000-my_model_1.0-stdout MODEL_LOG - [PID]9656
2025-05-05T01:00:58,034 [DEBUG] W-9000-my_model_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-my_model_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2025-05-05T01:00:58,034 [INFO ] W-9000-my_model_1.0-stdout MODEL_LOG - Torch worker started.
2025-05-05T01:00:58,034 [DEBUG] W-9000-my_model_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-my_model_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2025-05-05T01:00:58,034 [INFO ] W-9000-my_model_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9000
2025-05-05T01:00:58,034 [INFO ] W-9000-my_model_1.0-stdout MODEL_LOG - Python runtime: 3.12.3
2025-05-05T01:00:58,034 [INFO ] W-9000-my_model_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9000
2025-05-05T01:00:58,036 [DEBUG] W-9000-my_model_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req.cmd LOAD repeats 1 to backend at: 1746388858036
2025-05-05T01:00:58,036 [DEBUG] W-9000-my_model_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req.cmd LOAD repeats 1 to backend at: 1746388858036
2025-05-05T01:00:58,036 [INFO ] W-9000-my_model_1.0-stdout MODEL_LOG - Connection accepted: /tmp/.ts.sock.9000.
2025-05-05T01:00:58,036 [INFO ] W-9000-my_model_1.0 org.pytorch.serve.wlm.WorkerThread - Looping backend response at: 1746388858036
2025-05-05T01:00:58,036 [INFO ] W-9000-my_model_1.0 org.pytorch.serve.wlm.WorkerThread - Looping backend response at: 1746388858036
2025-05-05T01:00:58,051 [INFO ] W-9000-my_model_1.0-stdout MODEL_LOG - model_name: my_model, batchSize: 1
2025-05-05T01:00:59,460 [INFO ] W-9000-my_model_1.0-stdout MODEL_LOG - OpenVINO is not enabled
2025-05-05T01:00:59,460 [INFO ] W-9000-my_model_1.0-stdout MODEL_LOG - proceeding without onnxruntime
2025-05-05T01:00:59,461 [INFO ] W-9000-my_model_1.0-stdout MODEL_LOG - Torch TensorRT not enabled
2025-05-05T01:00:59,462 [INFO ] W-9000-my_model_1.0-stdout MODEL_LOG - Failed to load model my_model, exception No model weights could be loaded
2025-05-05T01:00:59,463 [INFO ] W-9000-my_model_1.0-stdout MODEL_LOG - Traceback (most recent call last):
2025-05-05T01:00:59,463 [INFO ] W-9000-my_model_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 1427
2025-05-05T01:00:59,463 [INFO ] W-9000-my_model_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 1427
2025-05-05T01:00:59,463 [INFO ] W-9000-my_model_1.0-stdout MODEL_LOG -   File "/home/pupu/Desktop/project/.venv/lib/python3.12/site-packages/ts/model_service_worker.py", line 133, in load_model
2025-05-05T01:00:59,463 [DEBUG] W-9000-my_model_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-my_model_1.0 State change WORKER_STARTED -> WORKER_ERROR
2025-05-05T01:00:59,463 [DEBUG] W-9000-my_model_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-my_model_1.0 State change WORKER_STARTED -> WORKER_ERROR
2025-05-05T01:00:59,463 [INFO ] W-9000-my_model_1.0-stdout MODEL_LOG -     service = model_loader.load(
2025-05-05T01:00:59,463 [INFO ] W-9000-my_model_1.0 TS_METRICS - WorkerThreadTime.Milliseconds:0.0|#Level:Host|#hostname:pupu-Stvullinc,timestamp:1746388859
2025-05-05T01:00:59,463 [INFO ] W-9000-my_model_1.0-stdout MODEL_LOG -               ^^^^^^^^^^^^^^^^^^
2025-05-05T01:00:59,463 [INFO ] epollEventLoopGroup-5-3 org.pytorch.serve.wlm.WorkerThread - 9000 Worker disconnected. WORKER_ERROR
2025-05-05T01:00:59,463 [INFO ] W-9000-my_model_1.0-stdout MODEL_LOG -   File "/home/pupu/Desktop/project/.venv/lib/python3.12/site-packages/ts/model_loader.py", line 143, in load
2025-05-05T01:00:59,464 [INFO ] W-9000-my_model_1.0-stdout MODEL_LOG -     initialize_fn(service.context)
2025-05-05T01:00:59,463 [INFO ] epollEventLoopGroup-5-3 org.pytorch.serve.wlm.WorkerThread - 9000 Worker disconnected. WORKER_ERROR
2025-05-05T01:00:59,464 [DEBUG] W-9000-my_model_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_ERROR
2025-05-05T01:00:59,464 [INFO ] W-9000-my_model_1.0-stdout MODEL_LOG -   File "/tmp/models/be6f4d4e933d47cd83e5258e9ca51515/handler.py", line 28, in initialize
2025-05-05T01:00:59,464 [DEBUG] W-9000-my_model_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_ERROR
2025-05-05T01:00:59,464 [INFO ] W-9000-my_model_1.0-stdout MODEL_LOG -     super().initialize(context)
2025-05-05T01:00:59,464 [DEBUG] W-9000-my_model_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died., responseTimeout:120sec
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1770) ~[?:?]
	at java.util.concurrent.LinkedBlockingDeque.pollFirst(LinkedBlockingDeque.java:515) ~[?:?]
	at java.util.concurrent.LinkedBlockingDeque.poll(LinkedBlockingDeque.java:677) ~[?:?]
	at org.pytorch.serve.wlm.Model.pollBatch(Model.java:482) ~[model-server.jar:?]
	at org.pytorch.serve.wlm.BatchAggregator.pollBatch(BatchAggregator.java:189) ~[model-server.jar:?]
	at org.pytorch.serve.wlm.BatchAggregator.getRequest(BatchAggregator.java:36) ~[model-server.jar:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:195) ~[model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144) ~[?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642) ~[?:?]
	at java.lang.Thread.run(Thread.java:1583) [?:?]
2025-05-05T01:00:59,464 [INFO ] W-9000-my_model_1.0-stdout MODEL_LOG -   File "/home/pupu/Desktop/project/.venv/lib/python3.12/site-packages/ts/torch_handler/base_handler.py", line 223, in initialize
2025-05-05T01:00:59,465 [INFO ] W-9000-my_model_1.0-stdout MODEL_LOG -     raise RuntimeError("No model weights could be loaded")
2025-05-05T01:00:59,465 [INFO ] W-9000-my_model_1.0-stdout MODEL_LOG - RuntimeError: No model weights could be loaded
2025-05-05T01:00:59,464 [DEBUG] W-9000-my_model_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died., responseTimeout:120sec
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1770) ~[?:?]
	at java.util.concurrent.LinkedBlockingDeque.pollFirst(LinkedBlockingDeque.java:515) ~[?:?]
	at java.util.concurrent.LinkedBlockingDeque.poll(LinkedBlockingDeque.java:677) ~[?:?]
	at org.pytorch.serve.wlm.Model.pollBatch(Model.java:482) ~[model-server.jar:?]
	at org.pytorch.serve.wlm.BatchAggregator.pollBatch(BatchAggregator.java:189) ~[model-server.jar:?]
	at org.pytorch.serve.wlm.BatchAggregator.getRequest(BatchAggregator.java:36) ~[model-server.jar:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:195) ~[model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144) ~[?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642) ~[?:?]
	at java.lang.Thread.run(Thread.java:1583) [?:?]
2025-05-05T01:00:59,465 [INFO ] W-9000-my_model_1.0-stdout MODEL_LOG - Backend worker process died.
2025-05-05T01:00:59,465 [DEBUG] W-9000-my_model_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-my_model_1.0 State change WORKER_ERROR -> WORKER_STOPPED
2025-05-05T01:00:59,465 [DEBUG] W-9000-my_model_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-my_model_1.0 State change WORKER_ERROR -> WORKER_STOPPED
2025-05-05T01:00:59,465 [WARN ] W-9000-my_model_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery failed again
2025-05-05T01:00:59,465 [INFO ] W-9000-my_model_1.0-stdout MODEL_LOG - Traceback (most recent call last):
2025-05-05T01:00:59,465 [WARN ] W-9000-my_model_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery failed again
2025-05-05T01:00:59,465 [INFO ] W-9000-my_model_1.0-stdout MODEL_LOG -   File "/home/pupu/Desktop/project/.venv/lib/python3.12/site-packages/ts/model_service_worker.py", line 301, in <module>
2025-05-05T01:00:59,465 [INFO ] W-9000-my_model_1.0-stdout MODEL_LOG -     worker.run_server()
2025-05-05T01:00:59,466 [INFO ] W-9000-my_model_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9000 in 2 seconds.
2025-05-05T01:00:59,466 [INFO ] W-9000-my_model_1.0-stdout MODEL_LOG -   File "/home/pupu/Desktop/project/.venv/lib/python3.12/site-packages/ts/model_service_worker.py", line 268, in run_server
2025-05-05T01:00:59,466 [INFO ] W-9000-my_model_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9000 in 2 seconds.
2025-05-05T01:00:59,466 [INFO ] W-9000-my_model_1.0-stdout MODEL_LOG -     self.handle_connection(cl_socket)
2025-05-05T01:00:59,466 [INFO ] W-9000-my_model_1.0-stdout MODEL_LOG -   File "/home/pupu/Desktop/project/.venv/lib/python3.12/site-packages/ts/model_service_worker.py", line 201, in handle_connection
2025-05-05T01:00:59,466 [INFO ] W-9000-my_model_1.0-stdout MODEL_LOG -     raise RuntimeError("{} - {}".format(code, result))
2025-05-05T01:00:59,466 [INFO ] W-9000-my_model_1.0-stdout MODEL_LOG - RuntimeError: 500 - Unknown exception
2025-05-05T01:00:59,467 [INFO ] W-9000-my_model_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-my_model_1.0-stdout
2025-05-05T01:00:59,467 [INFO ] W-9000-my_model_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-my_model_1.0-stdout
2025-05-05T01:00:59,540 [INFO ] W-9000-my_model_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-my_model_1.0-stderr
2025-05-05T01:00:59,540 [INFO ] W-9000-my_model_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-my_model_1.0-stderr
2025-05-05T01:01:01,467 [DEBUG] W-9000-my_model_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/home/pupu/Desktop/project/.venv/bin/python, /home/pupu/Desktop/project/.venv/lib/python3.12/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9000, --metrics-config, /home/pupu/Desktop/project/.venv/lib/python3.12/site-packages/ts/configs/metrics.yaml]
2025-05-05T01:01:01,467 [DEBUG] W-9000-my_model_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/home/pupu/Desktop/project/.venv/bin/python, /home/pupu/Desktop/project/.venv/lib/python3.12/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9000, --metrics-config, /home/pupu/Desktop/project/.venv/lib/python3.12/site-packages/ts/configs/metrics.yaml]
2025-05-05T01:22:18,696 [DEBUG] main org.pytorch.serve.util.ConfigManager - xpu-smi not available or failed: Cannot run program "xpu-smi": error=2, No such file or directory
2025-05-05T01:22:18,696 [DEBUG] main org.pytorch.serve.util.ConfigManager - xpu-smi not available or failed: Cannot run program "xpu-smi": error=2, No such file or directory
2025-05-05T01:22:18,701 [WARN ] main org.pytorch.serve.util.ConfigManager - Your torchserve instance can access any URL to load models. When deploying to production, make sure to limit the set of allowed_urls in config.properties
2025-05-05T01:22:18,701 [WARN ] main org.pytorch.serve.util.ConfigManager - Your torchserve instance can access any URL to load models. When deploying to production, make sure to limit the set of allowed_urls in config.properties
2025-05-05T01:22:18,722 [INFO ] main org.pytorch.serve.servingsdk.impl.PluginsManager - Initializing plugins manager...
2025-05-05T01:22:18,722 [INFO ] main org.pytorch.serve.servingsdk.impl.PluginsManager - Initializing plugins manager...
2025-05-05T01:22:18,779 [INFO ] main org.pytorch.serve.metrics.configuration.MetricConfiguration - Successfully loaded metrics configuration from /home/pupu/Desktop/project/.venv/lib/python3.12/site-packages/ts/configs/metrics.yaml
2025-05-05T01:22:18,779 [INFO ] main org.pytorch.serve.metrics.configuration.MetricConfiguration - Successfully loaded metrics configuration from /home/pupu/Desktop/project/.venv/lib/python3.12/site-packages/ts/configs/metrics.yaml
2025-05-05T01:22:18,860 [INFO ] main org.pytorch.serve.ModelServer - 
Torchserve version: 0.12.0
TS Home: /home/pupu/Desktop/project/.venv/lib/python3.12/site-packages
Current directory: /home/pupu/Desktop/project
Temp directory: /tmp
Metrics config path: /home/pupu/Desktop/project/.venv/lib/python3.12/site-packages/ts/configs/metrics.yaml
Number of GPUs: 1
Number of CPUs: 8
Max heap size: 3990 M
Python executable: /home/pupu/Desktop/project/.venv/bin/python
Config file: N/A
Inference address: http://127.0.0.1:8080
Management address: http://127.0.0.1:8081
Metrics address: http://127.0.0.1:8082
Model Store: /home/pupu/Desktop/project/there_is_our_server
Initial Models: my_model=my_model.mar
Log dir: /home/pupu/Desktop/project/logs
Metrics dir: /home/pupu/Desktop/project/logs
Netty threads: 0
Netty client threads: 0
Default workers per model: 1
Blacklist Regex: N/A
Maximum Response Size: 6553500
Maximum Request Size: 6553500
Limit Maximum Image Pixels: true
Prefer direct buffer: false
Allowed Urls: [file://.*|http(s)?://.*]
Custom python dependency for model allowed: false
Enable metrics API: true
Metrics mode: LOG
Disable system metrics: false
Workflow Store: /home/pupu/Desktop/project/there_is_our_server
CPP log config: N/A
Model config: N/A
System metrics command: default
Model API enabled: false
2025-05-05T01:22:18,860 [INFO ] main org.pytorch.serve.ModelServer - 
Torchserve version: 0.12.0
TS Home: /home/pupu/Desktop/project/.venv/lib/python3.12/site-packages
Current directory: /home/pupu/Desktop/project
Temp directory: /tmp
Metrics config path: /home/pupu/Desktop/project/.venv/lib/python3.12/site-packages/ts/configs/metrics.yaml
Number of GPUs: 1
Number of CPUs: 8
Max heap size: 3990 M
Python executable: /home/pupu/Desktop/project/.venv/bin/python
Config file: N/A
Inference address: http://127.0.0.1:8080
Management address: http://127.0.0.1:8081
Metrics address: http://127.0.0.1:8082
Model Store: /home/pupu/Desktop/project/there_is_our_server
Initial Models: my_model=my_model.mar
Log dir: /home/pupu/Desktop/project/logs
Metrics dir: /home/pupu/Desktop/project/logs
Netty threads: 0
Netty client threads: 0
Default workers per model: 1
Blacklist Regex: N/A
Maximum Response Size: 6553500
Maximum Request Size: 6553500
Limit Maximum Image Pixels: true
Prefer direct buffer: false
Allowed Urls: [file://.*|http(s)?://.*]
Custom python dependency for model allowed: false
Enable metrics API: true
Metrics mode: LOG
Disable system metrics: false
Workflow Store: /home/pupu/Desktop/project/there_is_our_server
CPP log config: N/A
Model config: N/A
System metrics command: default
Model API enabled: false
2025-05-05T01:22:18,868 [INFO ] main org.pytorch.serve.servingsdk.impl.PluginsManager -  Loading snapshot serializer plugin...
2025-05-05T01:22:18,868 [INFO ] main org.pytorch.serve.servingsdk.impl.PluginsManager -  Loading snapshot serializer plugin...
2025-05-05T01:22:18,887 [INFO ] main org.pytorch.serve.ModelServer - Loading initial models: my_model.mar
2025-05-05T01:22:18,887 [INFO ] main org.pytorch.serve.ModelServer - Loading initial models: my_model.mar
2025-05-05T01:22:20,053 [DEBUG] main org.pytorch.serve.wlm.ModelVersionedRefs - Adding new version 1.0 for model my_model
2025-05-05T01:22:20,053 [DEBUG] main org.pytorch.serve.wlm.ModelVersionedRefs - Adding new version 1.0 for model my_model
2025-05-05T01:22:20,053 [DEBUG] main org.pytorch.serve.wlm.ModelVersionedRefs - Setting default version to 1.0 for model my_model
2025-05-05T01:22:20,053 [DEBUG] main org.pytorch.serve.wlm.ModelVersionedRefs - Setting default version to 1.0 for model my_model
2025-05-05T01:22:20,053 [INFO ] main org.pytorch.serve.wlm.ModelManager - Model my_model loaded.
2025-05-05T01:22:20,053 [INFO ] main org.pytorch.serve.wlm.ModelManager - Model my_model loaded.
2025-05-05T01:22:20,054 [DEBUG] main org.pytorch.serve.wlm.ModelManager - updateModel: my_model, count: 1
2025-05-05T01:22:20,054 [DEBUG] main org.pytorch.serve.wlm.ModelManager - updateModel: my_model, count: 1
2025-05-05T01:22:20,061 [DEBUG] W-9000-my_model_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/home/pupu/Desktop/project/.venv/bin/python, /home/pupu/Desktop/project/.venv/lib/python3.12/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9000, --metrics-config, /home/pupu/Desktop/project/.venv/lib/python3.12/site-packages/ts/configs/metrics.yaml]
2025-05-05T01:22:20,062 [INFO ] main org.pytorch.serve.ModelServer - Initialize Inference server with: EpollServerSocketChannel.
2025-05-05T01:22:20,062 [INFO ] main org.pytorch.serve.ModelServer - Initialize Inference server with: EpollServerSocketChannel.
2025-05-05T01:22:20,061 [DEBUG] W-9000-my_model_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/home/pupu/Desktop/project/.venv/bin/python, /home/pupu/Desktop/project/.venv/lib/python3.12/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9000, --metrics-config, /home/pupu/Desktop/project/.venv/lib/python3.12/site-packages/ts/configs/metrics.yaml]
2025-05-05T01:22:20,122 [INFO ] main org.pytorch.serve.ModelServer - Inference API bind to: http://127.0.0.1:8080
2025-05-05T01:22:20,122 [INFO ] main org.pytorch.serve.ModelServer - Inference API bind to: http://127.0.0.1:8080
2025-05-05T01:22:20,122 [INFO ] main org.pytorch.serve.ModelServer - Initialize Management server with: EpollServerSocketChannel.
2025-05-05T01:22:20,122 [INFO ] main org.pytorch.serve.ModelServer - Initialize Management server with: EpollServerSocketChannel.
2025-05-05T01:22:20,123 [INFO ] main org.pytorch.serve.ModelServer - Management API bind to: http://127.0.0.1:8081
2025-05-05T01:22:20,123 [INFO ] main org.pytorch.serve.ModelServer - Management API bind to: http://127.0.0.1:8081
2025-05-05T01:22:20,123 [INFO ] main org.pytorch.serve.ModelServer - Initialize Metrics server with: EpollServerSocketChannel.
2025-05-05T01:22:20,123 [INFO ] main org.pytorch.serve.ModelServer - Initialize Metrics server with: EpollServerSocketChannel.
2025-05-05T01:22:20,124 [INFO ] main org.pytorch.serve.ModelServer - Metrics API bind to: http://127.0.0.1:8082
2025-05-05T01:22:20,124 [INFO ] main org.pytorch.serve.ModelServer - Metrics API bind to: http://127.0.0.1:8082
2025-05-05T01:22:20,366 [WARN ] pool-3-thread-1 org.pytorch.serve.metrics.MetricCollector - worker pid is not available yet.
2025-05-05T01:22:20,366 [WARN ] pool-3-thread-1 org.pytorch.serve.metrics.MetricCollector - worker pid is not available yet.
2025-05-05T01:22:20,888 [INFO ] pool-3-thread-1 TS_METRICS - CPUUtilization.Percent:0.0|#Level:Host|#hostname:pupu-Stvullinc,timestamp:1746390140
2025-05-05T01:22:20,890 [INFO ] pool-3-thread-1 TS_METRICS - DiskAvailable.Gigabytes:75.43640518188477|#Level:Host|#hostname:pupu-Stvullinc,timestamp:1746390140
2025-05-05T01:22:20,890 [INFO ] pool-3-thread-1 TS_METRICS - DiskUsage.Gigabytes:62.74544143676758|#Level:Host|#hostname:pupu-Stvullinc,timestamp:1746390140
2025-05-05T01:22:20,891 [INFO ] pool-3-thread-1 TS_METRICS - DiskUtilization.Percent:45.4|#Level:Host|#hostname:pupu-Stvullinc,timestamp:1746390140
2025-05-05T01:22:20,891 [INFO ] pool-3-thread-1 TS_METRICS - GPUMemoryUtilization.Percent:4.38232421875|#Level:Host,DeviceId:0|#hostname:pupu-Stvullinc,timestamp:1746390140
2025-05-05T01:22:20,892 [INFO ] pool-3-thread-1 TS_METRICS - GPUMemoryUsed.Megabytes:359.0|#Level:Host,DeviceId:0|#hostname:pupu-Stvullinc,timestamp:1746390140
2025-05-05T01:22:20,892 [INFO ] pool-3-thread-1 TS_METRICS - GPUUtilization.Percent:5.0|#Level:Host,DeviceId:0|#hostname:pupu-Stvullinc,timestamp:1746390140
2025-05-05T01:22:20,892 [INFO ] pool-3-thread-1 TS_METRICS - MemoryAvailable.Megabytes:9179.015625|#Level:Host|#hostname:pupu-Stvullinc,timestamp:1746390140
2025-05-05T01:22:20,892 [INFO ] pool-3-thread-1 TS_METRICS - MemoryUsed.Megabytes:6347.12109375|#Level:Host|#hostname:pupu-Stvullinc,timestamp:1746390140
2025-05-05T01:22:20,893 [INFO ] pool-3-thread-1 TS_METRICS - MemoryUtilization.Percent:42.5|#Level:Host|#hostname:pupu-Stvullinc,timestamp:1746390140
2025-05-05T01:22:21,885 [INFO ] W-9000-my_model_1.0-stdout MODEL_LOG - s_name_part0=/tmp/.ts.sock, s_name_part1=9000, pid=10687
2025-05-05T01:22:21,886 [INFO ] W-9000-my_model_1.0-stdout MODEL_LOG - Listening on port: /tmp/.ts.sock.9000
2025-05-05T01:22:21,893 [INFO ] W-9000-my_model_1.0-stdout MODEL_LOG - Successfully loaded /home/pupu/Desktop/project/.venv/lib/python3.12/site-packages/ts/configs/metrics.yaml.
2025-05-05T01:22:21,893 [INFO ] W-9000-my_model_1.0-stdout MODEL_LOG - [PID]10687
2025-05-05T01:22:21,893 [INFO ] W-9000-my_model_1.0-stdout MODEL_LOG - Torch worker started.
2025-05-05T01:22:21,894 [INFO ] W-9000-my_model_1.0-stdout MODEL_LOG - Python runtime: 3.12.3
2025-05-05T01:22:21,894 [DEBUG] W-9000-my_model_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-my_model_1.0 State change null -> WORKER_STARTED
2025-05-05T01:22:21,894 [DEBUG] W-9000-my_model_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-my_model_1.0 State change null -> WORKER_STARTED
2025-05-05T01:22:21,898 [INFO ] W-9000-my_model_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9000
2025-05-05T01:22:21,898 [INFO ] W-9000-my_model_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9000
2025-05-05T01:22:21,907 [INFO ] W-9000-my_model_1.0-stdout MODEL_LOG - Connection accepted: /tmp/.ts.sock.9000.
2025-05-05T01:22:21,911 [DEBUG] W-9000-my_model_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req.cmd LOAD repeats 1 to backend at: 1746390141910
2025-05-05T01:22:21,911 [DEBUG] W-9000-my_model_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req.cmd LOAD repeats 1 to backend at: 1746390141910
2025-05-05T01:22:21,914 [INFO ] W-9000-my_model_1.0 org.pytorch.serve.wlm.WorkerThread - Looping backend response at: 1746390141913
2025-05-05T01:22:21,914 [INFO ] W-9000-my_model_1.0 org.pytorch.serve.wlm.WorkerThread - Looping backend response at: 1746390141913
2025-05-05T01:22:21,938 [INFO ] W-9000-my_model_1.0-stdout MODEL_LOG - model_name: my_model, batchSize: 1
2025-05-05T01:22:23,184 [INFO ] W-9000-my_model_1.0-stdout MODEL_LOG - OpenVINO is not enabled
2025-05-05T01:22:23,184 [INFO ] W-9000-my_model_1.0-stdout MODEL_LOG - proceeding without onnxruntime
2025-05-05T01:22:23,184 [INFO ] W-9000-my_model_1.0-stdout MODEL_LOG - Torch TensorRT not enabled
2025-05-05T01:22:23,186 [INFO ] W-9000-my_model_1.0-stdout MODEL_LOG - Failed to load model my_model, exception No model weights could be loaded
2025-05-05T01:22:23,186 [INFO ] W-9000-my_model_1.0-stdout MODEL_LOG - Traceback (most recent call last):
2025-05-05T01:22:23,186 [INFO ] W-9000-my_model_1.0-stdout MODEL_LOG -   File "/home/pupu/Desktop/project/.venv/lib/python3.12/site-packages/ts/model_service_worker.py", line 133, in load_model
2025-05-05T01:22:23,186 [INFO ] W-9000-my_model_1.0-stdout MODEL_LOG -     service = model_loader.load(
2025-05-05T01:22:23,187 [INFO ] W-9000-my_model_1.0-stdout MODEL_LOG -               ^^^^^^^^^^^^^^^^^^
2025-05-05T01:22:23,187 [INFO ] W-9000-my_model_1.0-stdout MODEL_LOG -   File "/home/pupu/Desktop/project/.venv/lib/python3.12/site-packages/ts/model_loader.py", line 143, in load
2025-05-05T01:22:23,187 [INFO ] W-9000-my_model_1.0-stdout MODEL_LOG -     initialize_fn(service.context)
2025-05-05T01:22:23,187 [INFO ] W-9000-my_model_1.0-stdout MODEL_LOG -   File "/tmp/models/e52cc43b92744f23bb0d0daa46d638a6/handler.py", line 31, in initialize
2025-05-05T01:22:23,187 [INFO ] W-9000-my_model_1.0-stdout MODEL_LOG -     super().initialize(context)
2025-05-05T01:22:23,188 [INFO ] W-9000-my_model_1.0-stdout MODEL_LOG -   File "/home/pupu/Desktop/project/.venv/lib/python3.12/site-packages/ts/torch_handler/base_handler.py", line 223, in initialize
2025-05-05T01:22:23,188 [INFO ] W-9000-my_model_1.0-stdout MODEL_LOG -     raise RuntimeError("No model weights could be loaded")
2025-05-05T01:22:23,188 [INFO ] W-9000-my_model_1.0-stdout MODEL_LOG - RuntimeError: No model weights could be loaded
2025-05-05T01:22:23,188 [INFO ] W-9000-my_model_1.0-stdout MODEL_LOG - Backend worker process died.
2025-05-05T01:22:23,188 [INFO ] W-9000-my_model_1.0-stdout MODEL_LOG - Traceback (most recent call last):
2025-05-05T01:22:23,188 [INFO ] W-9000-my_model_1.0-stdout MODEL_LOG -   File "/home/pupu/Desktop/project/.venv/lib/python3.12/site-packages/ts/model_service_worker.py", line 301, in <module>
2025-05-05T01:22:23,188 [INFO ] W-9000-my_model_1.0-stdout MODEL_LOG -     worker.run_server()
2025-05-05T01:22:23,189 [INFO ] W-9000-my_model_1.0-stdout MODEL_LOG -   File "/home/pupu/Desktop/project/.venv/lib/python3.12/site-packages/ts/model_service_worker.py", line 268, in run_server
2025-05-05T01:22:23,189 [INFO ] W-9000-my_model_1.0-stdout MODEL_LOG -     self.handle_connection(cl_socket)
2025-05-05T01:22:23,189 [INFO ] W-9000-my_model_1.0-stdout MODEL_LOG -   File "/home/pupu/Desktop/project/.venv/lib/python3.12/site-packages/ts/model_service_worker.py", line 201, in handle_connection
2025-05-05T01:22:23,189 [INFO ] W-9000-my_model_1.0-stdout MODEL_LOG -     raise RuntimeError("{} - {}".format(code, result))
2025-05-05T01:22:23,189 [INFO ] W-9000-my_model_1.0-stdout MODEL_LOG - RuntimeError: 500 - Unknown exception
2025-05-05T01:22:23,193 [INFO ] W-9000-my_model_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 1279
2025-05-05T01:22:23,193 [INFO ] W-9000-my_model_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 1279
2025-05-05T01:22:23,194 [DEBUG] W-9000-my_model_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-my_model_1.0 State change WORKER_STARTED -> WORKER_ERROR
2025-05-05T01:22:23,194 [INFO ] epollEventLoopGroup-5-1 org.pytorch.serve.wlm.WorkerThread - 9000 Worker disconnected. WORKER_STARTED
2025-05-05T01:22:23,194 [DEBUG] W-9000-my_model_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-my_model_1.0 State change WORKER_STARTED -> WORKER_ERROR
2025-05-05T01:22:23,194 [INFO ] epollEventLoopGroup-5-1 org.pytorch.serve.wlm.WorkerThread - 9000 Worker disconnected. WORKER_STARTED
2025-05-05T01:22:23,194 [INFO ] W-9000-my_model_1.0 TS_METRICS - WorkerThreadTime.Milliseconds:5.0|#Level:Host|#hostname:pupu-Stvullinc,timestamp:1746390143
2025-05-05T01:22:23,194 [DEBUG] W-9000-my_model_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_ERROR
2025-05-05T01:22:23,194 [DEBUG] W-9000-my_model_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_ERROR
2025-05-05T01:22:23,195 [DEBUG] W-9000-my_model_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died., responseTimeout:120sec
java.lang.InterruptedException: null
	at java.util.concurrent.locks.ReentrantLock$Sync.lockInterruptibly(ReentrantLock.java:159) ~[?:?]
	at java.util.concurrent.locks.ReentrantLock.lockInterruptibly(ReentrantLock.java:372) ~[?:?]
	at org.pytorch.serve.wlm.Model.pollBatch(Model.java:478) ~[model-server.jar:?]
	at org.pytorch.serve.wlm.BatchAggregator.pollBatch(BatchAggregator.java:189) ~[model-server.jar:?]
	at org.pytorch.serve.wlm.BatchAggregator.getRequest(BatchAggregator.java:36) ~[model-server.jar:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:195) ~[model-server.jar:?]
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:572) ~[?:?]
	at java.util.concurrent.FutureTask.run(FutureTask.java:317) ~[?:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144) ~[?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642) ~[?:?]
	at java.lang.Thread.run(Thread.java:1583) [?:?]
2025-05-05T01:22:23,195 [DEBUG] W-9000-my_model_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died., responseTimeout:120sec
java.lang.InterruptedException: null
	at java.util.concurrent.locks.ReentrantLock$Sync.lockInterruptibly(ReentrantLock.java:159) ~[?:?]
	at java.util.concurrent.locks.ReentrantLock.lockInterruptibly(ReentrantLock.java:372) ~[?:?]
	at org.pytorch.serve.wlm.Model.pollBatch(Model.java:478) ~[model-server.jar:?]
	at org.pytorch.serve.wlm.BatchAggregator.pollBatch(BatchAggregator.java:189) ~[model-server.jar:?]
	at org.pytorch.serve.wlm.BatchAggregator.getRequest(BatchAggregator.java:36) ~[model-server.jar:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:195) ~[model-server.jar:?]
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:572) ~[?:?]
	at java.util.concurrent.FutureTask.run(FutureTask.java:317) ~[?:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144) ~[?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642) ~[?:?]
	at java.lang.Thread.run(Thread.java:1583) [?:?]
2025-05-05T01:22:23,208 [DEBUG] W-9000-my_model_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-my_model_1.0 State change WORKER_ERROR -> WORKER_STOPPED
2025-05-05T01:22:23,208 [DEBUG] W-9000-my_model_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-my_model_1.0 State change WORKER_ERROR -> WORKER_STOPPED
2025-05-05T01:22:23,208 [INFO ] W-9000-my_model_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery start timestamp: 1746390143208
2025-05-05T01:22:23,208 [INFO ] W-9000-my_model_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery start timestamp: 1746390143208
2025-05-05T01:22:23,209 [INFO ] W-9000-my_model_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9000 in 1 seconds.
2025-05-05T01:22:23,209 [INFO ] W-9000-my_model_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9000 in 1 seconds.
2025-05-05T01:22:23,259 [INFO ] W-9000-my_model_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-my_model_1.0-stdout
2025-05-05T01:22:23,259 [INFO ] W-9000-my_model_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-my_model_1.0-stdout
2025-05-05T01:22:23,259 [INFO ] W-9000-my_model_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-my_model_1.0-stderr
2025-05-05T01:22:23,259 [INFO ] W-9000-my_model_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-my_model_1.0-stderr
2025-05-05T01:22:24,210 [DEBUG] W-9000-my_model_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/home/pupu/Desktop/project/.venv/bin/python, /home/pupu/Desktop/project/.venv/lib/python3.12/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9000, --metrics-config, /home/pupu/Desktop/project/.venv/lib/python3.12/site-packages/ts/configs/metrics.yaml]
2025-05-05T01:22:24,210 [DEBUG] W-9000-my_model_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/home/pupu/Desktop/project/.venv/bin/python, /home/pupu/Desktop/project/.venv/lib/python3.12/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9000, --metrics-config, /home/pupu/Desktop/project/.venv/lib/python3.12/site-packages/ts/configs/metrics.yaml]
2025-05-05T01:22:25,868 [INFO ] W-9000-my_model_1.0-stdout MODEL_LOG - s_name_part0=/tmp/.ts.sock, s_name_part1=9000, pid=10727
2025-05-05T01:22:25,869 [INFO ] W-9000-my_model_1.0-stdout MODEL_LOG - Listening on port: /tmp/.ts.sock.9000
2025-05-05T01:22:25,876 [INFO ] W-9000-my_model_1.0-stdout MODEL_LOG - Successfully loaded /home/pupu/Desktop/project/.venv/lib/python3.12/site-packages/ts/configs/metrics.yaml.
2025-05-05T01:22:25,876 [INFO ] W-9000-my_model_1.0-stdout MODEL_LOG - [PID]10727
2025-05-05T01:22:25,877 [INFO ] W-9000-my_model_1.0-stdout MODEL_LOG - Torch worker started.
2025-05-05T01:22:25,877 [DEBUG] W-9000-my_model_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-my_model_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2025-05-05T01:22:25,877 [DEBUG] W-9000-my_model_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-my_model_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2025-05-05T01:22:25,877 [INFO ] W-9000-my_model_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9000
2025-05-05T01:22:25,877 [INFO ] W-9000-my_model_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9000
2025-05-05T01:22:25,877 [INFO ] W-9000-my_model_1.0-stdout MODEL_LOG - Python runtime: 3.12.3
2025-05-05T01:22:25,878 [DEBUG] W-9000-my_model_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req.cmd LOAD repeats 1 to backend at: 1746390145878
2025-05-05T01:22:25,878 [INFO ] W-9000-my_model_1.0-stdout MODEL_LOG - Connection accepted: /tmp/.ts.sock.9000.
2025-05-05T01:22:25,878 [DEBUG] W-9000-my_model_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req.cmd LOAD repeats 1 to backend at: 1746390145878
2025-05-05T01:22:25,879 [INFO ] W-9000-my_model_1.0 org.pytorch.serve.wlm.WorkerThread - Looping backend response at: 1746390145879
2025-05-05T01:22:25,879 [INFO ] W-9000-my_model_1.0 org.pytorch.serve.wlm.WorkerThread - Looping backend response at: 1746390145879
2025-05-05T01:22:25,897 [INFO ] W-9000-my_model_1.0-stdout MODEL_LOG - model_name: my_model, batchSize: 1
2025-05-05T01:22:27,152 [INFO ] W-9000-my_model_1.0-stdout MODEL_LOG - OpenVINO is not enabled
2025-05-05T01:22:27,152 [INFO ] W-9000-my_model_1.0-stdout MODEL_LOG - proceeding without onnxruntime
2025-05-05T01:22:27,152 [INFO ] W-9000-my_model_1.0-stdout MODEL_LOG - Torch TensorRT not enabled
2025-05-05T01:22:27,153 [INFO ] W-9000-my_model_1.0-stdout MODEL_LOG - Failed to load model my_model, exception No model weights could be loaded
2025-05-05T01:22:27,154 [INFO ] W-9000-my_model_1.0-stdout MODEL_LOG - Traceback (most recent call last):
2025-05-05T01:22:27,154 [INFO ] W-9000-my_model_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 1275
2025-05-05T01:22:27,154 [INFO ] W-9000-my_model_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 1275
2025-05-05T01:22:27,154 [INFO ] W-9000-my_model_1.0-stdout MODEL_LOG -   File "/home/pupu/Desktop/project/.venv/lib/python3.12/site-packages/ts/model_service_worker.py", line 133, in load_model
2025-05-05T01:22:27,154 [DEBUG] W-9000-my_model_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-my_model_1.0 State change WORKER_STARTED -> WORKER_ERROR
2025-05-05T01:22:27,154 [DEBUG] W-9000-my_model_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-my_model_1.0 State change WORKER_STARTED -> WORKER_ERROR
2025-05-05T01:22:27,154 [INFO ] W-9000-my_model_1.0-stdout MODEL_LOG -     service = model_loader.load(
2025-05-05T01:22:27,154 [INFO ] W-9000-my_model_1.0-stdout MODEL_LOG -               ^^^^^^^^^^^^^^^^^^
2025-05-05T01:22:27,154 [INFO ] W-9000-my_model_1.0 TS_METRICS - WorkerThreadTime.Milliseconds:1.0|#Level:Host|#hostname:pupu-Stvullinc,timestamp:1746390147
2025-05-05T01:22:27,154 [INFO ] epollEventLoopGroup-5-2 org.pytorch.serve.wlm.WorkerThread - 9000 Worker disconnected. WORKER_STARTED
2025-05-05T01:22:27,154 [INFO ] W-9000-my_model_1.0-stdout MODEL_LOG -   File "/home/pupu/Desktop/project/.venv/lib/python3.12/site-packages/ts/model_loader.py", line 143, in load
2025-05-05T01:22:27,154 [INFO ] W-9000-my_model_1.0-stdout MODEL_LOG -     initialize_fn(service.context)
2025-05-05T01:22:27,154 [INFO ] epollEventLoopGroup-5-2 org.pytorch.serve.wlm.WorkerThread - 9000 Worker disconnected. WORKER_STARTED
2025-05-05T01:22:27,155 [INFO ] W-9000-my_model_1.0-stdout MODEL_LOG -   File "/tmp/models/e52cc43b92744f23bb0d0daa46d638a6/handler.py", line 31, in initialize
2025-05-05T01:22:27,155 [DEBUG] W-9000-my_model_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_ERROR
2025-05-05T01:22:27,155 [DEBUG] W-9000-my_model_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_ERROR
2025-05-05T01:22:27,155 [INFO ] W-9000-my_model_1.0-stdout MODEL_LOG -     super().initialize(context)
2025-05-05T01:22:27,155 [DEBUG] W-9000-my_model_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died., responseTimeout:120sec
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1770) ~[?:?]
	at java.util.concurrent.LinkedBlockingDeque.pollFirst(LinkedBlockingDeque.java:515) ~[?:?]
	at java.util.concurrent.LinkedBlockingDeque.poll(LinkedBlockingDeque.java:677) ~[?:?]
	at org.pytorch.serve.wlm.Model.pollBatch(Model.java:482) ~[model-server.jar:?]
	at org.pytorch.serve.wlm.BatchAggregator.pollBatch(BatchAggregator.java:189) ~[model-server.jar:?]
	at org.pytorch.serve.wlm.BatchAggregator.getRequest(BatchAggregator.java:36) ~[model-server.jar:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:195) ~[model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144) ~[?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642) ~[?:?]
	at java.lang.Thread.run(Thread.java:1583) [?:?]
2025-05-05T01:22:27,155 [INFO ] W-9000-my_model_1.0-stdout MODEL_LOG -   File "/home/pupu/Desktop/project/.venv/lib/python3.12/site-packages/ts/torch_handler/base_handler.py", line 223, in initialize
2025-05-05T01:22:27,155 [DEBUG] W-9000-my_model_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died., responseTimeout:120sec
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1770) ~[?:?]
	at java.util.concurrent.LinkedBlockingDeque.pollFirst(LinkedBlockingDeque.java:515) ~[?:?]
	at java.util.concurrent.LinkedBlockingDeque.poll(LinkedBlockingDeque.java:677) ~[?:?]
	at org.pytorch.serve.wlm.Model.pollBatch(Model.java:482) ~[model-server.jar:?]
	at org.pytorch.serve.wlm.BatchAggregator.pollBatch(BatchAggregator.java:189) ~[model-server.jar:?]
	at org.pytorch.serve.wlm.BatchAggregator.getRequest(BatchAggregator.java:36) ~[model-server.jar:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:195) ~[model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144) ~[?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642) ~[?:?]
	at java.lang.Thread.run(Thread.java:1583) [?:?]
2025-05-05T01:22:27,155 [INFO ] W-9000-my_model_1.0-stdout MODEL_LOG -     raise RuntimeError("No model weights could be loaded")
2025-05-05T01:22:27,156 [DEBUG] W-9000-my_model_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-my_model_1.0 State change WORKER_ERROR -> WORKER_STOPPED
2025-05-05T01:22:27,156 [INFO ] W-9000-my_model_1.0-stdout MODEL_LOG - RuntimeError: No model weights could be loaded
2025-05-05T01:22:27,156 [DEBUG] W-9000-my_model_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-my_model_1.0 State change WORKER_ERROR -> WORKER_STOPPED
2025-05-05T01:22:27,156 [WARN ] W-9000-my_model_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery failed again
2025-05-05T01:22:27,156 [WARN ] W-9000-my_model_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery failed again
2025-05-05T01:22:27,156 [INFO ] W-9000-my_model_1.0-stdout MODEL_LOG - Backend worker process died.
2025-05-05T01:22:27,156 [INFO ] W-9000-my_model_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9000 in 1 seconds.
2025-05-05T01:22:27,156 [INFO ] W-9000-my_model_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9000 in 1 seconds.
2025-05-05T01:22:27,156 [INFO ] W-9000-my_model_1.0-stdout MODEL_LOG - Traceback (most recent call last):
2025-05-05T01:22:27,157 [INFO ] W-9000-my_model_1.0-stdout MODEL_LOG -   File "/home/pupu/Desktop/project/.venv/lib/python3.12/site-packages/ts/model_service_worker.py", line 301, in <module>
2025-05-05T01:22:27,157 [INFO ] W-9000-my_model_1.0-stdout MODEL_LOG -     worker.run_server()
2025-05-05T01:22:27,157 [INFO ] W-9000-my_model_1.0-stdout MODEL_LOG -   File "/home/pupu/Desktop/project/.venv/lib/python3.12/site-packages/ts/model_service_worker.py", line 268, in run_server
2025-05-05T01:22:27,157 [INFO ] W-9000-my_model_1.0-stdout MODEL_LOG -     self.handle_connection(cl_socket)
2025-05-05T01:22:27,157 [INFO ] W-9000-my_model_1.0-stdout MODEL_LOG -   File "/home/pupu/Desktop/project/.venv/lib/python3.12/site-packages/ts/model_service_worker.py", line 201, in handle_connection
2025-05-05T01:22:27,157 [INFO ] W-9000-my_model_1.0-stdout MODEL_LOG -     raise RuntimeError("{} - {}".format(code, result))
2025-05-05T01:22:27,158 [INFO ] W-9000-my_model_1.0-stdout MODEL_LOG - RuntimeError: 500 - Unknown exception
2025-05-05T01:22:27,158 [INFO ] W-9000-my_model_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-my_model_1.0-stdout
2025-05-05T01:22:27,158 [INFO ] W-9000-my_model_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-my_model_1.0-stdout
2025-05-05T01:22:27,218 [INFO ] W-9000-my_model_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-my_model_1.0-stderr
2025-05-05T01:22:27,218 [INFO ] W-9000-my_model_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-my_model_1.0-stderr
2025-05-05T01:22:28,157 [DEBUG] W-9000-my_model_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/home/pupu/Desktop/project/.venv/bin/python, /home/pupu/Desktop/project/.venv/lib/python3.12/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9000, --metrics-config, /home/pupu/Desktop/project/.venv/lib/python3.12/site-packages/ts/configs/metrics.yaml]
2025-05-05T01:22:28,157 [DEBUG] W-9000-my_model_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/home/pupu/Desktop/project/.venv/bin/python, /home/pupu/Desktop/project/.venv/lib/python3.12/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9000, --metrics-config, /home/pupu/Desktop/project/.venv/lib/python3.12/site-packages/ts/configs/metrics.yaml]
2025-05-05T01:22:29,834 [INFO ] W-9000-my_model_1.0-stdout MODEL_LOG - s_name_part0=/tmp/.ts.sock, s_name_part1=9000, pid=10749
2025-05-05T01:22:29,835 [INFO ] W-9000-my_model_1.0-stdout MODEL_LOG - Listening on port: /tmp/.ts.sock.9000
2025-05-05T01:22:29,843 [INFO ] W-9000-my_model_1.0-stdout MODEL_LOG - Successfully loaded /home/pupu/Desktop/project/.venv/lib/python3.12/site-packages/ts/configs/metrics.yaml.
2025-05-05T01:22:29,843 [INFO ] W-9000-my_model_1.0-stdout MODEL_LOG - [PID]10749
2025-05-05T01:22:29,843 [INFO ] W-9000-my_model_1.0-stdout MODEL_LOG - Torch worker started.
2025-05-05T01:22:29,843 [DEBUG] W-9000-my_model_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-my_model_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2025-05-05T01:22:29,843 [DEBUG] W-9000-my_model_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-my_model_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2025-05-05T01:22:29,843 [INFO ] W-9000-my_model_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9000
2025-05-05T01:22:29,843 [INFO ] W-9000-my_model_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9000
2025-05-05T01:22:29,843 [INFO ] W-9000-my_model_1.0-stdout MODEL_LOG - Python runtime: 3.12.3
2025-05-05T01:22:29,845 [DEBUG] W-9000-my_model_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req.cmd LOAD repeats 1 to backend at: 1746390149845
2025-05-05T01:22:29,845 [DEBUG] W-9000-my_model_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req.cmd LOAD repeats 1 to backend at: 1746390149845
2025-05-05T01:22:29,845 [INFO ] W-9000-my_model_1.0-stdout MODEL_LOG - Connection accepted: /tmp/.ts.sock.9000.
2025-05-05T01:22:29,846 [INFO ] W-9000-my_model_1.0 org.pytorch.serve.wlm.WorkerThread - Looping backend response at: 1746390149846
2025-05-05T01:22:29,846 [INFO ] W-9000-my_model_1.0 org.pytorch.serve.wlm.WorkerThread - Looping backend response at: 1746390149846
2025-05-05T01:22:29,864 [INFO ] W-9000-my_model_1.0-stdout MODEL_LOG - model_name: my_model, batchSize: 1
2025-05-05T01:22:31,203 [INFO ] W-9000-my_model_1.0-stdout MODEL_LOG - OpenVINO is not enabled
2025-05-05T01:22:31,203 [INFO ] W-9000-my_model_1.0-stdout MODEL_LOG - proceeding without onnxruntime
2025-05-05T01:22:31,204 [INFO ] W-9000-my_model_1.0-stdout MODEL_LOG - Torch TensorRT not enabled
2025-05-05T01:22:31,205 [INFO ] W-9000-my_model_1.0-stdout MODEL_LOG - Failed to load model my_model, exception No model weights could be loaded
2025-05-05T01:22:31,205 [INFO ] W-9000-my_model_1.0-stdout MODEL_LOG - Traceback (most recent call last):
2025-05-05T01:22:31,205 [INFO ] W-9000-my_model_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 1359
2025-05-05T01:22:31,205 [INFO ] W-9000-my_model_1.0-stdout MODEL_LOG -   File "/home/pupu/Desktop/project/.venv/lib/python3.12/site-packages/ts/model_service_worker.py", line 133, in load_model
2025-05-05T01:22:31,205 [INFO ] W-9000-my_model_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 1359
2025-05-05T01:22:31,205 [DEBUG] W-9000-my_model_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-my_model_1.0 State change WORKER_STARTED -> WORKER_ERROR
2025-05-05T01:22:31,205 [INFO ] W-9000-my_model_1.0-stdout MODEL_LOG -     service = model_loader.load(
2025-05-05T01:22:31,205 [DEBUG] W-9000-my_model_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-my_model_1.0 State change WORKER_STARTED -> WORKER_ERROR
2025-05-05T01:22:31,205 [INFO ] W-9000-my_model_1.0-stdout MODEL_LOG -               ^^^^^^^^^^^^^^^^^^
2025-05-05T01:22:31,205 [INFO ] W-9000-my_model_1.0 TS_METRICS - WorkerThreadTime.Milliseconds:1.0|#Level:Host|#hostname:pupu-Stvullinc,timestamp:1746390151
2025-05-05T01:22:31,206 [INFO ] W-9000-my_model_1.0-stdout MODEL_LOG -   File "/home/pupu/Desktop/project/.venv/lib/python3.12/site-packages/ts/model_loader.py", line 143, in load
2025-05-05T01:22:31,205 [INFO ] epollEventLoopGroup-5-3 org.pytorch.serve.wlm.WorkerThread - 9000 Worker disconnected. WORKER_STARTED
2025-05-05T01:22:31,206 [INFO ] W-9000-my_model_1.0-stdout MODEL_LOG -     initialize_fn(service.context)
2025-05-05T01:22:31,205 [INFO ] epollEventLoopGroup-5-3 org.pytorch.serve.wlm.WorkerThread - 9000 Worker disconnected. WORKER_STARTED
2025-05-05T01:22:31,206 [INFO ] W-9000-my_model_1.0-stdout MODEL_LOG -   File "/tmp/models/e52cc43b92744f23bb0d0daa46d638a6/handler.py", line 31, in initialize
2025-05-05T01:22:31,206 [INFO ] W-9000-my_model_1.0-stdout MODEL_LOG -     super().initialize(context)
2025-05-05T01:22:31,206 [DEBUG] W-9000-my_model_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_ERROR
2025-05-05T01:22:31,206 [DEBUG] W-9000-my_model_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_ERROR
2025-05-05T01:22:31,206 [INFO ] W-9000-my_model_1.0-stdout MODEL_LOG -   File "/home/pupu/Desktop/project/.venv/lib/python3.12/site-packages/ts/torch_handler/base_handler.py", line 223, in initialize
2025-05-05T01:22:31,206 [INFO ] W-9000-my_model_1.0-stdout MODEL_LOG -     raise RuntimeError("No model weights could be loaded")
2025-05-05T01:22:31,206 [DEBUG] W-9000-my_model_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died., responseTimeout:120sec
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1770) ~[?:?]
	at java.util.concurrent.LinkedBlockingDeque.pollFirst(LinkedBlockingDeque.java:515) ~[?:?]
	at java.util.concurrent.LinkedBlockingDeque.poll(LinkedBlockingDeque.java:677) ~[?:?]
	at org.pytorch.serve.wlm.Model.pollBatch(Model.java:482) ~[model-server.jar:?]
	at org.pytorch.serve.wlm.BatchAggregator.pollBatch(BatchAggregator.java:189) ~[model-server.jar:?]
	at org.pytorch.serve.wlm.BatchAggregator.getRequest(BatchAggregator.java:36) ~[model-server.jar:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:195) ~[model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144) ~[?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642) ~[?:?]
	at java.lang.Thread.run(Thread.java:1583) [?:?]
2025-05-05T01:22:31,207 [INFO ] W-9000-my_model_1.0-stdout MODEL_LOG - RuntimeError: No model weights could be loaded
2025-05-05T01:22:31,206 [DEBUG] W-9000-my_model_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died., responseTimeout:120sec
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1770) ~[?:?]
	at java.util.concurrent.LinkedBlockingDeque.pollFirst(LinkedBlockingDeque.java:515) ~[?:?]
	at java.util.concurrent.LinkedBlockingDeque.poll(LinkedBlockingDeque.java:677) ~[?:?]
	at org.pytorch.serve.wlm.Model.pollBatch(Model.java:482) ~[model-server.jar:?]
	at org.pytorch.serve.wlm.BatchAggregator.pollBatch(BatchAggregator.java:189) ~[model-server.jar:?]
	at org.pytorch.serve.wlm.BatchAggregator.getRequest(BatchAggregator.java:36) ~[model-server.jar:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:195) ~[model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144) ~[?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642) ~[?:?]
	at java.lang.Thread.run(Thread.java:1583) [?:?]
2025-05-05T01:22:31,207 [INFO ] W-9000-my_model_1.0-stdout MODEL_LOG - Backend worker process died.
2025-05-05T01:22:31,207 [DEBUG] W-9000-my_model_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-my_model_1.0 State change WORKER_ERROR -> WORKER_STOPPED
2025-05-05T01:22:31,207 [DEBUG] W-9000-my_model_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-my_model_1.0 State change WORKER_ERROR -> WORKER_STOPPED
2025-05-05T01:22:31,207 [INFO ] W-9000-my_model_1.0-stdout MODEL_LOG - Traceback (most recent call last):
2025-05-05T01:22:31,207 [WARN ] W-9000-my_model_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery failed again
2025-05-05T01:22:31,207 [WARN ] W-9000-my_model_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery failed again
2025-05-05T01:22:31,207 [INFO ] W-9000-my_model_1.0-stdout MODEL_LOG -   File "/home/pupu/Desktop/project/.venv/lib/python3.12/site-packages/ts/model_service_worker.py", line 301, in <module>
2025-05-05T01:22:31,207 [INFO ] W-9000-my_model_1.0-stdout MODEL_LOG -     worker.run_server()
2025-05-05T01:22:31,207 [INFO ] W-9000-my_model_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9000 in 2 seconds.
2025-05-05T01:22:31,207 [INFO ] W-9000-my_model_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9000 in 2 seconds.
2025-05-05T01:22:31,208 [INFO ] W-9000-my_model_1.0-stdout MODEL_LOG -   File "/home/pupu/Desktop/project/.venv/lib/python3.12/site-packages/ts/model_service_worker.py", line 268, in run_server
2025-05-05T01:22:31,208 [INFO ] W-9000-my_model_1.0-stdout MODEL_LOG -     self.handle_connection(cl_socket)
2025-05-05T01:22:31,208 [INFO ] W-9000-my_model_1.0-stdout MODEL_LOG -   File "/home/pupu/Desktop/project/.venv/lib/python3.12/site-packages/ts/model_service_worker.py", line 201, in handle_connection
2025-05-05T01:22:31,208 [INFO ] W-9000-my_model_1.0-stdout MODEL_LOG -     raise RuntimeError("{} - {}".format(code, result))
2025-05-05T01:22:31,208 [INFO ] W-9000-my_model_1.0-stdout MODEL_LOG - RuntimeError: 500 - Unknown exception
2025-05-05T01:22:31,208 [INFO ] W-9000-my_model_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-my_model_1.0-stdout
2025-05-05T01:22:31,208 [INFO ] W-9000-my_model_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-my_model_1.0-stdout
2025-05-05T01:22:31,265 [INFO ] W-9000-my_model_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-my_model_1.0-stderr
2025-05-05T01:22:31,265 [INFO ] W-9000-my_model_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-my_model_1.0-stderr
2025-05-05T01:22:33,209 [DEBUG] W-9000-my_model_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/home/pupu/Desktop/project/.venv/bin/python, /home/pupu/Desktop/project/.venv/lib/python3.12/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9000, --metrics-config, /home/pupu/Desktop/project/.venv/lib/python3.12/site-packages/ts/configs/metrics.yaml]
2025-05-05T01:22:33,209 [DEBUG] W-9000-my_model_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/home/pupu/Desktop/project/.venv/bin/python, /home/pupu/Desktop/project/.venv/lib/python3.12/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9000, --metrics-config, /home/pupu/Desktop/project/.venv/lib/python3.12/site-packages/ts/configs/metrics.yaml]
2025-05-05T01:22:34,890 [INFO ] W-9000-my_model_1.0-stdout MODEL_LOG - s_name_part0=/tmp/.ts.sock, s_name_part1=9000, pid=10772
2025-05-05T01:22:34,890 [INFO ] W-9000-my_model_1.0-stdout MODEL_LOG - Listening on port: /tmp/.ts.sock.9000
2025-05-05T01:22:34,898 [INFO ] W-9000-my_model_1.0-stdout MODEL_LOG - Successfully loaded /home/pupu/Desktop/project/.venv/lib/python3.12/site-packages/ts/configs/metrics.yaml.
2025-05-05T01:22:34,898 [INFO ] W-9000-my_model_1.0-stdout MODEL_LOG - [PID]10772
2025-05-05T01:22:34,898 [INFO ] W-9000-my_model_1.0-stdout MODEL_LOG - Torch worker started.
2025-05-05T01:22:34,898 [DEBUG] W-9000-my_model_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-my_model_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2025-05-05T01:22:34,898 [DEBUG] W-9000-my_model_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-my_model_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2025-05-05T01:22:34,898 [INFO ] W-9000-my_model_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9000
2025-05-05T01:22:34,898 [INFO ] W-9000-my_model_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9000
2025-05-05T01:22:34,899 [INFO ] W-9000-my_model_1.0-stdout MODEL_LOG - Python runtime: 3.12.3
2025-05-05T01:22:34,901 [DEBUG] W-9000-my_model_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req.cmd LOAD repeats 1 to backend at: 1746390154901
2025-05-05T01:22:34,901 [INFO ] W-9000-my_model_1.0-stdout MODEL_LOG - Connection accepted: /tmp/.ts.sock.9000.
2025-05-05T01:22:34,901 [DEBUG] W-9000-my_model_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req.cmd LOAD repeats 1 to backend at: 1746390154901
2025-05-05T01:22:34,901 [INFO ] W-9000-my_model_1.0 org.pytorch.serve.wlm.WorkerThread - Looping backend response at: 1746390154901
2025-05-05T01:22:34,901 [INFO ] W-9000-my_model_1.0 org.pytorch.serve.wlm.WorkerThread - Looping backend response at: 1746390154901
2025-05-05T01:22:34,926 [INFO ] W-9000-my_model_1.0-stdout MODEL_LOG - model_name: my_model, batchSize: 1
2025-05-05T01:22:36,170 [INFO ] W-9000-my_model_1.0-stdout MODEL_LOG - OpenVINO is not enabled
2025-05-05T01:22:36,171 [INFO ] W-9000-my_model_1.0-stdout MODEL_LOG - proceeding without onnxruntime
2025-05-05T01:22:36,171 [INFO ] W-9000-my_model_1.0-stdout MODEL_LOG - Torch TensorRT not enabled
2025-05-05T01:22:36,172 [INFO ] W-9000-my_model_1.0-stdout MODEL_LOG - Failed to load model my_model, exception No model weights could be loaded
2025-05-05T01:22:36,172 [INFO ] W-9000-my_model_1.0-stdout MODEL_LOG - Traceback (most recent call last):
2025-05-05T01:22:36,172 [INFO ] W-9000-my_model_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 1271
2025-05-05T01:22:36,172 [INFO ] W-9000-my_model_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 1271
2025-05-05T01:22:36,172 [DEBUG] W-9000-my_model_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-my_model_1.0 State change WORKER_STARTED -> WORKER_ERROR
2025-05-05T01:22:36,172 [INFO ] W-9000-my_model_1.0-stdout MODEL_LOG -   File "/home/pupu/Desktop/project/.venv/lib/python3.12/site-packages/ts/model_service_worker.py", line 133, in load_model
2025-05-05T01:22:36,172 [DEBUG] W-9000-my_model_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-my_model_1.0 State change WORKER_STARTED -> WORKER_ERROR
2025-05-05T01:22:36,173 [INFO ] W-9000-my_model_1.0-stdout MODEL_LOG -     service = model_loader.load(
2025-05-05T01:22:36,173 [INFO ] W-9000-my_model_1.0 TS_METRICS - WorkerThreadTime.Milliseconds:1.0|#Level:Host|#hostname:pupu-Stvullinc,timestamp:1746390156
2025-05-05T01:22:36,173 [INFO ] W-9000-my_model_1.0-stdout MODEL_LOG -               ^^^^^^^^^^^^^^^^^^
2025-05-05T01:22:36,173 [INFO ] epollEventLoopGroup-5-4 org.pytorch.serve.wlm.WorkerThread - 9000 Worker disconnected. WORKER_ERROR
2025-05-05T01:22:36,173 [INFO ] W-9000-my_model_1.0-stdout MODEL_LOG -   File "/home/pupu/Desktop/project/.venv/lib/python3.12/site-packages/ts/model_loader.py", line 143, in load
2025-05-05T01:22:36,173 [INFO ] epollEventLoopGroup-5-4 org.pytorch.serve.wlm.WorkerThread - 9000 Worker disconnected. WORKER_ERROR
2025-05-05T01:22:36,173 [INFO ] W-9000-my_model_1.0-stdout MODEL_LOG -     initialize_fn(service.context)
2025-05-05T01:22:36,173 [DEBUG] W-9000-my_model_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_ERROR
2025-05-05T01:22:36,173 [DEBUG] W-9000-my_model_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_ERROR
2025-05-05T01:22:36,173 [INFO ] W-9000-my_model_1.0-stdout MODEL_LOG -   File "/tmp/models/e52cc43b92744f23bb0d0daa46d638a6/handler.py", line 31, in initialize
2025-05-05T01:22:36,173 [DEBUG] W-9000-my_model_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died., responseTimeout:120sec
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1770) ~[?:?]
	at java.util.concurrent.LinkedBlockingDeque.pollFirst(LinkedBlockingDeque.java:515) ~[?:?]
	at java.util.concurrent.LinkedBlockingDeque.poll(LinkedBlockingDeque.java:677) ~[?:?]
	at org.pytorch.serve.wlm.Model.pollBatch(Model.java:482) ~[model-server.jar:?]
	at org.pytorch.serve.wlm.BatchAggregator.pollBatch(BatchAggregator.java:189) ~[model-server.jar:?]
	at org.pytorch.serve.wlm.BatchAggregator.getRequest(BatchAggregator.java:36) ~[model-server.jar:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:195) ~[model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144) ~[?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642) ~[?:?]
	at java.lang.Thread.run(Thread.java:1583) [?:?]
2025-05-05T01:22:36,174 [INFO ] W-9000-my_model_1.0-stdout MODEL_LOG -     super().initialize(context)
2025-05-05T01:22:36,173 [DEBUG] W-9000-my_model_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died., responseTimeout:120sec
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1770) ~[?:?]
	at java.util.concurrent.LinkedBlockingDeque.pollFirst(LinkedBlockingDeque.java:515) ~[?:?]
	at java.util.concurrent.LinkedBlockingDeque.poll(LinkedBlockingDeque.java:677) ~[?:?]
	at org.pytorch.serve.wlm.Model.pollBatch(Model.java:482) ~[model-server.jar:?]
	at org.pytorch.serve.wlm.BatchAggregator.pollBatch(BatchAggregator.java:189) ~[model-server.jar:?]
	at org.pytorch.serve.wlm.BatchAggregator.getRequest(BatchAggregator.java:36) ~[model-server.jar:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:195) ~[model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144) ~[?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642) ~[?:?]
	at java.lang.Thread.run(Thread.java:1583) [?:?]
2025-05-05T01:22:36,174 [DEBUG] W-9000-my_model_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-my_model_1.0 State change WORKER_ERROR -> WORKER_STOPPED
2025-05-05T01:22:36,174 [INFO ] W-9000-my_model_1.0-stdout MODEL_LOG -   File "/home/pupu/Desktop/project/.venv/lib/python3.12/site-packages/ts/torch_handler/base_handler.py", line 223, in initialize
2025-05-05T01:22:36,174 [DEBUG] W-9000-my_model_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-my_model_1.0 State change WORKER_ERROR -> WORKER_STOPPED
2025-05-05T01:22:36,174 [WARN ] W-9000-my_model_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery failed again
2025-05-05T01:22:36,174 [WARN ] W-9000-my_model_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery failed again
2025-05-05T01:22:36,174 [INFO ] W-9000-my_model_1.0-stdout MODEL_LOG -     raise RuntimeError("No model weights could be loaded")
2025-05-05T01:22:36,174 [INFO ] W-9000-my_model_1.0-stdout MODEL_LOG - RuntimeError: No model weights could be loaded
2025-05-05T01:22:36,175 [INFO ] W-9000-my_model_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9000 in 3 seconds.
2025-05-05T01:22:36,175 [INFO ] W-9000-my_model_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9000 in 3 seconds.
2025-05-05T01:22:36,175 [INFO ] W-9000-my_model_1.0-stdout MODEL_LOG - Backend worker process died.
2025-05-05T01:22:36,175 [INFO ] W-9000-my_model_1.0-stdout MODEL_LOG - Traceback (most recent call last):
2025-05-05T01:22:36,175 [INFO ] W-9000-my_model_1.0-stdout MODEL_LOG -   File "/home/pupu/Desktop/project/.venv/lib/python3.12/site-packages/ts/model_service_worker.py", line 301, in <module>
2025-05-05T01:22:36,175 [INFO ] W-9000-my_model_1.0-stdout MODEL_LOG -     worker.run_server()
2025-05-05T01:22:36,175 [INFO ] W-9000-my_model_1.0-stdout MODEL_LOG -   File "/home/pupu/Desktop/project/.venv/lib/python3.12/site-packages/ts/model_service_worker.py", line 268, in run_server
2025-05-05T01:22:36,176 [INFO ] W-9000-my_model_1.0-stdout MODEL_LOG -     self.handle_connection(cl_socket)
2025-05-05T01:22:36,176 [INFO ] W-9000-my_model_1.0-stdout MODEL_LOG -   File "/home/pupu/Desktop/project/.venv/lib/python3.12/site-packages/ts/model_service_worker.py", line 201, in handle_connection
2025-05-05T01:22:36,176 [INFO ] W-9000-my_model_1.0-stdout MODEL_LOG -     raise RuntimeError("{} - {}".format(code, result))
2025-05-05T01:22:36,176 [INFO ] W-9000-my_model_1.0-stdout MODEL_LOG - RuntimeError: 500 - Unknown exception
2025-05-05T01:22:36,176 [INFO ] W-9000-my_model_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-my_model_1.0-stdout
2025-05-05T01:22:36,176 [INFO ] W-9000-my_model_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-my_model_1.0-stdout
2025-05-05T01:22:36,261 [INFO ] W-9000-my_model_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-my_model_1.0-stderr
2025-05-05T01:22:36,261 [INFO ] W-9000-my_model_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-my_model_1.0-stderr
2025-05-05T01:22:39,176 [DEBUG] W-9000-my_model_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/home/pupu/Desktop/project/.venv/bin/python, /home/pupu/Desktop/project/.venv/lib/python3.12/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9000, --metrics-config, /home/pupu/Desktop/project/.venv/lib/python3.12/site-packages/ts/configs/metrics.yaml]
2025-05-05T01:22:39,176 [DEBUG] W-9000-my_model_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/home/pupu/Desktop/project/.venv/bin/python, /home/pupu/Desktop/project/.venv/lib/python3.12/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9000, --metrics-config, /home/pupu/Desktop/project/.venv/lib/python3.12/site-packages/ts/configs/metrics.yaml]
2025-05-05T01:22:40,843 [INFO ] W-9000-my_model_1.0-stdout MODEL_LOG - s_name_part0=/tmp/.ts.sock, s_name_part1=9000, pid=10795
2025-05-05T01:22:40,844 [INFO ] W-9000-my_model_1.0-stdout MODEL_LOG - Listening on port: /tmp/.ts.sock.9000
2025-05-05T01:22:40,855 [INFO ] W-9000-my_model_1.0-stdout MODEL_LOG - Successfully loaded /home/pupu/Desktop/project/.venv/lib/python3.12/site-packages/ts/configs/metrics.yaml.
2025-05-05T01:22:40,856 [INFO ] W-9000-my_model_1.0-stdout MODEL_LOG - [PID]10795
2025-05-05T01:22:40,856 [INFO ] W-9000-my_model_1.0-stdout MODEL_LOG - Torch worker started.
2025-05-05T01:22:40,856 [DEBUG] W-9000-my_model_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-my_model_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2025-05-05T01:22:40,856 [DEBUG] W-9000-my_model_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-my_model_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2025-05-05T01:22:40,856 [INFO ] W-9000-my_model_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9000
2025-05-05T01:22:40,856 [INFO ] W-9000-my_model_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9000
2025-05-05T01:22:40,856 [INFO ] W-9000-my_model_1.0-stdout MODEL_LOG - Python runtime: 3.12.3
2025-05-05T01:22:40,858 [DEBUG] W-9000-my_model_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req.cmd LOAD repeats 1 to backend at: 1746390160858
2025-05-05T01:22:40,858 [INFO ] W-9000-my_model_1.0-stdout MODEL_LOG - Connection accepted: /tmp/.ts.sock.9000.
2025-05-05T01:22:40,858 [DEBUG] W-9000-my_model_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req.cmd LOAD repeats 1 to backend at: 1746390160858
2025-05-05T01:22:40,859 [INFO ] W-9000-my_model_1.0 org.pytorch.serve.wlm.WorkerThread - Looping backend response at: 1746390160858
2025-05-05T01:22:40,859 [INFO ] W-9000-my_model_1.0 org.pytorch.serve.wlm.WorkerThread - Looping backend response at: 1746390160858
2025-05-05T01:22:40,881 [INFO ] W-9000-my_model_1.0-stdout MODEL_LOG - model_name: my_model, batchSize: 1
2025-05-05T01:22:42,203 [INFO ] W-9000-my_model_1.0-stdout MODEL_LOG - OpenVINO is not enabled
2025-05-05T01:22:42,203 [INFO ] W-9000-my_model_1.0-stdout MODEL_LOG - proceeding without onnxruntime
2025-05-05T01:22:42,203 [INFO ] W-9000-my_model_1.0-stdout MODEL_LOG - Torch TensorRT not enabled
2025-05-05T01:22:42,205 [INFO ] W-9000-my_model_1.0-stdout MODEL_LOG - Failed to load model my_model, exception No model weights could be loaded
2025-05-05T01:22:42,205 [INFO ] W-9000-my_model_1.0-stdout MODEL_LOG - Traceback (most recent call last):
2025-05-05T01:22:42,205 [INFO ] W-9000-my_model_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 1346
2025-05-05T01:22:42,205 [INFO ] W-9000-my_model_1.0-stdout MODEL_LOG -   File "/home/pupu/Desktop/project/.venv/lib/python3.12/site-packages/ts/model_service_worker.py", line 133, in load_model
2025-05-05T01:22:42,205 [INFO ] W-9000-my_model_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 1346
2025-05-05T01:22:42,205 [DEBUG] W-9000-my_model_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-my_model_1.0 State change WORKER_STARTED -> WORKER_ERROR
2025-05-05T01:22:42,205 [INFO ] W-9000-my_model_1.0-stdout MODEL_LOG -     service = model_loader.load(
2025-05-05T01:22:42,205 [DEBUG] W-9000-my_model_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-my_model_1.0 State change WORKER_STARTED -> WORKER_ERROR
2025-05-05T01:22:42,205 [INFO ] W-9000-my_model_1.0-stdout MODEL_LOG -               ^^^^^^^^^^^^^^^^^^
2025-05-05T01:22:42,205 [INFO ] W-9000-my_model_1.0 TS_METRICS - WorkerThreadTime.Milliseconds:1.0|#Level:Host|#hostname:pupu-Stvullinc,timestamp:1746390162
2025-05-05T01:22:42,205 [INFO ] W-9000-my_model_1.0-stdout MODEL_LOG -   File "/home/pupu/Desktop/project/.venv/lib/python3.12/site-packages/ts/model_loader.py", line 143, in load
2025-05-05T01:22:42,205 [INFO ] epollEventLoopGroup-5-5 org.pytorch.serve.wlm.WorkerThread - 9000 Worker disconnected. WORKER_ERROR
2025-05-05T01:22:42,205 [INFO ] W-9000-my_model_1.0-stdout MODEL_LOG -     initialize_fn(service.context)
2025-05-05T01:22:42,206 [INFO ] W-9000-my_model_1.0-stdout MODEL_LOG -   File "/tmp/models/e52cc43b92744f23bb0d0daa46d638a6/handler.py", line 31, in initialize
2025-05-05T01:22:42,205 [INFO ] epollEventLoopGroup-5-5 org.pytorch.serve.wlm.WorkerThread - 9000 Worker disconnected. WORKER_ERROR
2025-05-05T01:22:42,206 [INFO ] W-9000-my_model_1.0-stdout MODEL_LOG -     super().initialize(context)
2025-05-05T01:22:42,206 [DEBUG] W-9000-my_model_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_ERROR
2025-05-05T01:22:42,206 [DEBUG] W-9000-my_model_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_ERROR
2025-05-05T01:22:42,206 [INFO ] W-9000-my_model_1.0-stdout MODEL_LOG -   File "/home/pupu/Desktop/project/.venv/lib/python3.12/site-packages/ts/torch_handler/base_handler.py", line 223, in initialize
2025-05-05T01:22:42,206 [INFO ] W-9000-my_model_1.0-stdout MODEL_LOG -     raise RuntimeError("No model weights could be loaded")
2025-05-05T01:22:42,206 [DEBUG] W-9000-my_model_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died., responseTimeout:120sec
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1770) ~[?:?]
	at java.util.concurrent.LinkedBlockingDeque.pollFirst(LinkedBlockingDeque.java:515) ~[?:?]
	at java.util.concurrent.LinkedBlockingDeque.poll(LinkedBlockingDeque.java:677) ~[?:?]
	at org.pytorch.serve.wlm.Model.pollBatch(Model.java:482) ~[model-server.jar:?]
	at org.pytorch.serve.wlm.BatchAggregator.pollBatch(BatchAggregator.java:189) ~[model-server.jar:?]
	at org.pytorch.serve.wlm.BatchAggregator.getRequest(BatchAggregator.java:36) ~[model-server.jar:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:195) ~[model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144) ~[?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642) ~[?:?]
	at java.lang.Thread.run(Thread.java:1583) [?:?]
2025-05-05T01:22:42,206 [INFO ] W-9000-my_model_1.0-stdout MODEL_LOG - RuntimeError: No model weights could be loaded
2025-05-05T01:22:42,206 [DEBUG] W-9000-my_model_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died., responseTimeout:120sec
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1770) ~[?:?]
	at java.util.concurrent.LinkedBlockingDeque.pollFirst(LinkedBlockingDeque.java:515) ~[?:?]
	at java.util.concurrent.LinkedBlockingDeque.poll(LinkedBlockingDeque.java:677) ~[?:?]
	at org.pytorch.serve.wlm.Model.pollBatch(Model.java:482) ~[model-server.jar:?]
	at org.pytorch.serve.wlm.BatchAggregator.pollBatch(BatchAggregator.java:189) ~[model-server.jar:?]
	at org.pytorch.serve.wlm.BatchAggregator.getRequest(BatchAggregator.java:36) ~[model-server.jar:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:195) ~[model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144) ~[?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642) ~[?:?]
	at java.lang.Thread.run(Thread.java:1583) [?:?]
2025-05-05T01:22:42,206 [INFO ] W-9000-my_model_1.0-stdout MODEL_LOG - Backend worker process died.
2025-05-05T01:22:42,207 [DEBUG] W-9000-my_model_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-my_model_1.0 State change WORKER_ERROR -> WORKER_STOPPED
2025-05-05T01:22:42,207 [INFO ] W-9000-my_model_1.0-stdout MODEL_LOG - Traceback (most recent call last):
2025-05-05T01:22:42,207 [DEBUG] W-9000-my_model_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-my_model_1.0 State change WORKER_ERROR -> WORKER_STOPPED
2025-05-05T01:22:42,207 [WARN ] W-9000-my_model_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery failed again
2025-05-05T01:22:42,207 [INFO ] W-9000-my_model_1.0-stdout MODEL_LOG -   File "/home/pupu/Desktop/project/.venv/lib/python3.12/site-packages/ts/model_service_worker.py", line 301, in <module>
2025-05-05T01:22:42,207 [WARN ] W-9000-my_model_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery failed again
2025-05-05T01:22:42,207 [INFO ] W-9000-my_model_1.0-stdout MODEL_LOG -     worker.run_server()
2025-05-05T01:22:42,207 [INFO ] W-9000-my_model_1.0-stdout MODEL_LOG -   File "/home/pupu/Desktop/project/.venv/lib/python3.12/site-packages/ts/model_service_worker.py", line 268, in run_server
2025-05-05T01:22:42,207 [INFO ] W-9000-my_model_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9000 in 5 seconds.
2025-05-05T01:22:42,207 [INFO ] W-9000-my_model_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9000 in 5 seconds.
2025-05-05T01:22:42,207 [INFO ] W-9000-my_model_1.0-stdout MODEL_LOG -     self.handle_connection(cl_socket)
2025-05-05T01:22:42,207 [INFO ] W-9000-my_model_1.0-stdout MODEL_LOG -   File "/home/pupu/Desktop/project/.venv/lib/python3.12/site-packages/ts/model_service_worker.py", line 201, in handle_connection
2025-05-05T01:22:42,208 [INFO ] W-9000-my_model_1.0-stdout MODEL_LOG -     raise RuntimeError("{} - {}".format(code, result))
2025-05-05T01:22:42,208 [INFO ] W-9000-my_model_1.0-stdout MODEL_LOG - RuntimeError: 500 - Unknown exception
2025-05-05T01:22:42,208 [INFO ] W-9000-my_model_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-my_model_1.0-stdout
2025-05-05T01:22:42,208 [INFO ] W-9000-my_model_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-my_model_1.0-stdout
2025-05-05T01:22:42,283 [INFO ] W-9000-my_model_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-my_model_1.0-stderr
2025-05-05T01:22:42,283 [INFO ] W-9000-my_model_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-my_model_1.0-stderr
2025-05-05T01:25:47,705 [DEBUG] main org.pytorch.serve.util.ConfigManager - xpu-smi not available or failed: Cannot run program "xpu-smi": error=2, No such file or directory
2025-05-05T01:25:47,705 [DEBUG] main org.pytorch.serve.util.ConfigManager - xpu-smi not available or failed: Cannot run program "xpu-smi": error=2, No such file or directory
2025-05-05T01:25:47,711 [WARN ] main org.pytorch.serve.util.ConfigManager - Your torchserve instance can access any URL to load models. When deploying to production, make sure to limit the set of allowed_urls in config.properties
2025-05-05T01:25:47,711 [WARN ] main org.pytorch.serve.util.ConfigManager - Your torchserve instance can access any URL to load models. When deploying to production, make sure to limit the set of allowed_urls in config.properties
2025-05-05T01:25:47,734 [INFO ] main org.pytorch.serve.servingsdk.impl.PluginsManager - Initializing plugins manager...
2025-05-05T01:25:47,734 [INFO ] main org.pytorch.serve.servingsdk.impl.PluginsManager - Initializing plugins manager...
2025-05-05T01:25:47,805 [INFO ] main org.pytorch.serve.metrics.configuration.MetricConfiguration - Successfully loaded metrics configuration from /home/pupu/Desktop/project/.venv/lib/python3.12/site-packages/ts/configs/metrics.yaml
2025-05-05T01:25:47,805 [INFO ] main org.pytorch.serve.metrics.configuration.MetricConfiguration - Successfully loaded metrics configuration from /home/pupu/Desktop/project/.venv/lib/python3.12/site-packages/ts/configs/metrics.yaml
2025-05-05T01:25:47,883 [INFO ] main org.pytorch.serve.ModelServer - 
Torchserve version: 0.12.0
TS Home: /home/pupu/Desktop/project/.venv/lib/python3.12/site-packages
Current directory: /home/pupu/Desktop/project
Temp directory: /tmp
Metrics config path: /home/pupu/Desktop/project/.venv/lib/python3.12/site-packages/ts/configs/metrics.yaml
Number of GPUs: 1
Number of CPUs: 8
Max heap size: 3990 M
Python executable: /home/pupu/Desktop/project/.venv/bin/python
Config file: N/A
Inference address: http://127.0.0.1:8080
Management address: http://127.0.0.1:8081
Metrics address: http://127.0.0.1:8082
Model Store: /home/pupu/Desktop/project/there_is_our_server
Initial Models: my_model=my_model.mar
Log dir: /home/pupu/Desktop/project/logs
Metrics dir: /home/pupu/Desktop/project/logs
Netty threads: 0
Netty client threads: 0
Default workers per model: 1
Blacklist Regex: N/A
Maximum Response Size: 6553500
Maximum Request Size: 6553500
Limit Maximum Image Pixels: true
Prefer direct buffer: false
Allowed Urls: [file://.*|http(s)?://.*]
Custom python dependency for model allowed: false
Enable metrics API: true
Metrics mode: LOG
Disable system metrics: false
Workflow Store: /home/pupu/Desktop/project/there_is_our_server
CPP log config: N/A
Model config: N/A
System metrics command: default
Model API enabled: false
2025-05-05T01:25:47,883 [INFO ] main org.pytorch.serve.ModelServer - 
Torchserve version: 0.12.0
TS Home: /home/pupu/Desktop/project/.venv/lib/python3.12/site-packages
Current directory: /home/pupu/Desktop/project
Temp directory: /tmp
Metrics config path: /home/pupu/Desktop/project/.venv/lib/python3.12/site-packages/ts/configs/metrics.yaml
Number of GPUs: 1
Number of CPUs: 8
Max heap size: 3990 M
Python executable: /home/pupu/Desktop/project/.venv/bin/python
Config file: N/A
Inference address: http://127.0.0.1:8080
Management address: http://127.0.0.1:8081
Metrics address: http://127.0.0.1:8082
Model Store: /home/pupu/Desktop/project/there_is_our_server
Initial Models: my_model=my_model.mar
Log dir: /home/pupu/Desktop/project/logs
Metrics dir: /home/pupu/Desktop/project/logs
Netty threads: 0
Netty client threads: 0
Default workers per model: 1
Blacklist Regex: N/A
Maximum Response Size: 6553500
Maximum Request Size: 6553500
Limit Maximum Image Pixels: true
Prefer direct buffer: false
Allowed Urls: [file://.*|http(s)?://.*]
Custom python dependency for model allowed: false
Enable metrics API: true
Metrics mode: LOG
Disable system metrics: false
Workflow Store: /home/pupu/Desktop/project/there_is_our_server
CPP log config: N/A
Model config: N/A
System metrics command: default
Model API enabled: false
2025-05-05T01:25:47,891 [INFO ] main org.pytorch.serve.servingsdk.impl.PluginsManager -  Loading snapshot serializer plugin...
2025-05-05T01:25:47,891 [INFO ] main org.pytorch.serve.servingsdk.impl.PluginsManager -  Loading snapshot serializer plugin...
2025-05-05T01:25:47,912 [INFO ] main org.pytorch.serve.ModelServer - Loading initial models: my_model.mar
2025-05-05T01:25:47,912 [INFO ] main org.pytorch.serve.ModelServer - Loading initial models: my_model.mar
2025-05-05T01:25:49,041 [DEBUG] main org.pytorch.serve.wlm.ModelVersionedRefs - Adding new version 1.0 for model my_model
2025-05-05T01:25:49,041 [DEBUG] main org.pytorch.serve.wlm.ModelVersionedRefs - Adding new version 1.0 for model my_model
2025-05-05T01:25:49,041 [DEBUG] main org.pytorch.serve.wlm.ModelVersionedRefs - Setting default version to 1.0 for model my_model
2025-05-05T01:25:49,041 [DEBUG] main org.pytorch.serve.wlm.ModelVersionedRefs - Setting default version to 1.0 for model my_model
2025-05-05T01:25:49,041 [INFO ] main org.pytorch.serve.wlm.ModelManager - Model my_model loaded.
2025-05-05T01:25:49,041 [INFO ] main org.pytorch.serve.wlm.ModelManager - Model my_model loaded.
2025-05-05T01:25:49,042 [DEBUG] main org.pytorch.serve.wlm.ModelManager - updateModel: my_model, count: 1
2025-05-05T01:25:49,042 [DEBUG] main org.pytorch.serve.wlm.ModelManager - updateModel: my_model, count: 1
2025-05-05T01:25:49,049 [DEBUG] W-9000-my_model_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/home/pupu/Desktop/project/.venv/bin/python, /home/pupu/Desktop/project/.venv/lib/python3.12/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9000, --metrics-config, /home/pupu/Desktop/project/.venv/lib/python3.12/site-packages/ts/configs/metrics.yaml]
2025-05-05T01:25:49,049 [DEBUG] W-9000-my_model_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/home/pupu/Desktop/project/.venv/bin/python, /home/pupu/Desktop/project/.venv/lib/python3.12/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9000, --metrics-config, /home/pupu/Desktop/project/.venv/lib/python3.12/site-packages/ts/configs/metrics.yaml]
2025-05-05T01:25:49,050 [INFO ] main org.pytorch.serve.ModelServer - Initialize Inference server with: EpollServerSocketChannel.
2025-05-05T01:25:49,050 [INFO ] main org.pytorch.serve.ModelServer - Initialize Inference server with: EpollServerSocketChannel.
2025-05-05T01:25:49,110 [INFO ] main org.pytorch.serve.ModelServer - Inference API bind to: http://127.0.0.1:8080
2025-05-05T01:25:49,110 [INFO ] main org.pytorch.serve.ModelServer - Inference API bind to: http://127.0.0.1:8080
2025-05-05T01:25:49,110 [INFO ] main org.pytorch.serve.ModelServer - Initialize Management server with: EpollServerSocketChannel.
2025-05-05T01:25:49,110 [INFO ] main org.pytorch.serve.ModelServer - Initialize Management server with: EpollServerSocketChannel.
2025-05-05T01:25:49,111 [INFO ] main org.pytorch.serve.ModelServer - Management API bind to: http://127.0.0.1:8081
2025-05-05T01:25:49,111 [INFO ] main org.pytorch.serve.ModelServer - Management API bind to: http://127.0.0.1:8081
2025-05-05T01:25:49,112 [INFO ] main org.pytorch.serve.ModelServer - Initialize Metrics server with: EpollServerSocketChannel.
2025-05-05T01:25:49,112 [INFO ] main org.pytorch.serve.ModelServer - Initialize Metrics server with: EpollServerSocketChannel.
2025-05-05T01:25:49,112 [INFO ] main org.pytorch.serve.ModelServer - Metrics API bind to: http://127.0.0.1:8082
2025-05-05T01:25:49,112 [INFO ] main org.pytorch.serve.ModelServer - Metrics API bind to: http://127.0.0.1:8082
2025-05-05T01:25:49,311 [WARN ] pool-3-thread-1 org.pytorch.serve.metrics.MetricCollector - worker pid is not available yet.
2025-05-05T01:25:49,311 [WARN ] pool-3-thread-1 org.pytorch.serve.metrics.MetricCollector - worker pid is not available yet.
2025-05-05T01:25:49,850 [INFO ] pool-3-thread-1 TS_METRICS - CPUUtilization.Percent:33.3|#Level:Host|#hostname:pupu-Stvullinc,timestamp:1746390349
2025-05-05T01:25:49,851 [INFO ] pool-3-thread-1 TS_METRICS - DiskAvailable.Gigabytes:75.43606567382812|#Level:Host|#hostname:pupu-Stvullinc,timestamp:1746390349
2025-05-05T01:25:49,851 [INFO ] pool-3-thread-1 TS_METRICS - DiskUsage.Gigabytes:62.74578094482422|#Level:Host|#hostname:pupu-Stvullinc,timestamp:1746390349
2025-05-05T01:25:49,852 [INFO ] pool-3-thread-1 TS_METRICS - DiskUtilization.Percent:45.4|#Level:Host|#hostname:pupu-Stvullinc,timestamp:1746390349
2025-05-05T01:25:49,852 [INFO ] pool-3-thread-1 TS_METRICS - GPUMemoryUtilization.Percent:4.0771484375|#Level:Host,DeviceId:0|#hostname:pupu-Stvullinc,timestamp:1746390349
2025-05-05T01:25:49,852 [INFO ] pool-3-thread-1 TS_METRICS - GPUMemoryUsed.Megabytes:334.0|#Level:Host,DeviceId:0|#hostname:pupu-Stvullinc,timestamp:1746390349
2025-05-05T01:25:49,853 [INFO ] pool-3-thread-1 TS_METRICS - GPUUtilization.Percent:3.0|#Level:Host,DeviceId:0|#hostname:pupu-Stvullinc,timestamp:1746390349
2025-05-05T01:25:49,853 [INFO ] pool-3-thread-1 TS_METRICS - MemoryAvailable.Megabytes:9214.6640625|#Level:Host|#hostname:pupu-Stvullinc,timestamp:1746390349
2025-05-05T01:25:49,853 [INFO ] pool-3-thread-1 TS_METRICS - MemoryUsed.Megabytes:6319.6484375|#Level:Host|#hostname:pupu-Stvullinc,timestamp:1746390349
2025-05-05T01:25:49,853 [INFO ] pool-3-thread-1 TS_METRICS - MemoryUtilization.Percent:42.3|#Level:Host|#hostname:pupu-Stvullinc,timestamp:1746390349
2025-05-05T01:25:50,813 [INFO ] W-9000-my_model_1.0-stdout MODEL_LOG - s_name_part0=/tmp/.ts.sock, s_name_part1=9000, pid=10945
2025-05-05T01:25:50,814 [INFO ] W-9000-my_model_1.0-stdout MODEL_LOG - Listening on port: /tmp/.ts.sock.9000
2025-05-05T01:25:50,821 [INFO ] W-9000-my_model_1.0-stdout MODEL_LOG - Successfully loaded /home/pupu/Desktop/project/.venv/lib/python3.12/site-packages/ts/configs/metrics.yaml.
2025-05-05T01:25:50,821 [INFO ] W-9000-my_model_1.0-stdout MODEL_LOG - [PID]10945
2025-05-05T01:25:50,822 [INFO ] W-9000-my_model_1.0-stdout MODEL_LOG - Torch worker started.
2025-05-05T01:25:50,822 [INFO ] W-9000-my_model_1.0-stdout MODEL_LOG - Python runtime: 3.12.3
2025-05-05T01:25:50,822 [DEBUG] W-9000-my_model_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-my_model_1.0 State change null -> WORKER_STARTED
2025-05-05T01:25:50,822 [DEBUG] W-9000-my_model_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-my_model_1.0 State change null -> WORKER_STARTED
2025-05-05T01:25:50,829 [INFO ] W-9000-my_model_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9000
2025-05-05T01:25:50,829 [INFO ] W-9000-my_model_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9000
2025-05-05T01:25:50,838 [INFO ] W-9000-my_model_1.0-stdout MODEL_LOG - Connection accepted: /tmp/.ts.sock.9000.
2025-05-05T01:25:50,842 [DEBUG] W-9000-my_model_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req.cmd LOAD repeats 1 to backend at: 1746390350842
2025-05-05T01:25:50,842 [DEBUG] W-9000-my_model_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req.cmd LOAD repeats 1 to backend at: 1746390350842
2025-05-05T01:25:50,845 [INFO ] W-9000-my_model_1.0 org.pytorch.serve.wlm.WorkerThread - Looping backend response at: 1746390350845
2025-05-05T01:25:50,845 [INFO ] W-9000-my_model_1.0 org.pytorch.serve.wlm.WorkerThread - Looping backend response at: 1746390350845
2025-05-05T01:25:50,876 [INFO ] W-9000-my_model_1.0-stdout MODEL_LOG - model_name: my_model, batchSize: 1
2025-05-05T01:25:52,124 [INFO ] W-9000-my_model_1.0-stdout MODEL_LOG - OpenVINO is not enabled
2025-05-05T01:25:52,125 [INFO ] W-9000-my_model_1.0-stdout MODEL_LOG - proceeding without onnxruntime
2025-05-05T01:25:52,125 [INFO ] W-9000-my_model_1.0-stdout MODEL_LOG - Torch TensorRT not enabled
2025-05-05T01:25:53,416 [INFO ] W-9000-my_model_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 2570
2025-05-05T01:25:53,416 [INFO ] W-9000-my_model_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 2570
2025-05-05T01:25:53,417 [DEBUG] W-9000-my_model_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-my_model_1.0 State change WORKER_STARTED -> WORKER_MODEL_LOADED
2025-05-05T01:25:53,417 [DEBUG] W-9000-my_model_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-my_model_1.0 State change WORKER_STARTED -> WORKER_MODEL_LOADED
2025-05-05T01:25:53,417 [INFO ] W-9000-my_model_1.0 TS_METRICS - WorkerLoadTime.Milliseconds:4371.0|#WorkerName:W-9000-my_model_1.0,Level:Host|#hostname:pupu-Stvullinc,timestamp:1746390353
2025-05-05T01:25:53,418 [INFO ] W-9000-my_model_1.0 TS_METRICS - WorkerThreadTime.Milliseconds:6.0|#Level:Host|#hostname:pupu-Stvullinc,timestamp:1746390353
2025-05-05T01:26:33,389 [INFO ] epollEventLoopGroup-4-1 ACCESS_LOG - /127.0.0.1:56596 "GET / HTTP/1.1" 405 5
2025-05-05T01:26:33,394 [INFO ] epollEventLoopGroup-4-1 TS_METRICS - Requests4XX.Count:1.0|#Level:Host|#hostname:pupu-Stvullinc,timestamp:1746390393
2025-05-05T01:26:33,443 [INFO ] epollEventLoopGroup-4-1 ACCESS_LOG - /127.0.0.1:56600 "GET /favicon.ico HTTP/1.1" 404 1
2025-05-05T01:26:33,443 [INFO ] epollEventLoopGroup-4-1 TS_METRICS - Requests4XX.Count:1.0|#Level:Host|#hostname:pupu-Stvullinc,timestamp:1746390393
2025-05-05T01:26:49,814 [INFO ] pool-3-thread-1 TS_METRICS - CPUUtilization.Percent:0.0|#Level:Host|#hostname:pupu-Stvullinc,timestamp:1746390409
2025-05-05T01:26:49,815 [INFO ] pool-3-thread-1 TS_METRICS - DiskAvailable.Gigabytes:75.43581771850586|#Level:Host|#hostname:pupu-Stvullinc,timestamp:1746390409
2025-05-05T01:26:49,816 [INFO ] pool-3-thread-1 TS_METRICS - DiskUsage.Gigabytes:62.746028900146484|#Level:Host|#hostname:pupu-Stvullinc,timestamp:1746390409
2025-05-05T01:26:49,816 [INFO ] pool-3-thread-1 TS_METRICS - DiskUtilization.Percent:45.4|#Level:Host|#hostname:pupu-Stvullinc,timestamp:1746390409
2025-05-05T01:26:49,816 [INFO ] pool-3-thread-1 TS_METRICS - GPUMemoryUtilization.Percent:7.18994140625|#Level:Host,DeviceId:0|#hostname:pupu-Stvullinc,timestamp:1746390409
2025-05-05T01:26:49,817 [INFO ] pool-3-thread-1 TS_METRICS - GPUMemoryUsed.Megabytes:589.0|#Level:Host,DeviceId:0|#hostname:pupu-Stvullinc,timestamp:1746390409
2025-05-05T01:26:49,817 [INFO ] pool-3-thread-1 TS_METRICS - GPUUtilization.Percent:6.0|#Level:Host,DeviceId:0|#hostname:pupu-Stvullinc,timestamp:1746390409
2025-05-05T01:26:49,817 [INFO ] pool-3-thread-1 TS_METRICS - MemoryAvailable.Megabytes:8992.375|#Level:Host|#hostname:pupu-Stvullinc,timestamp:1746390409
2025-05-05T01:26:49,818 [INFO ] pool-3-thread-1 TS_METRICS - MemoryUsed.Megabytes:6530.02734375|#Level:Host|#hostname:pupu-Stvullinc,timestamp:1746390409
2025-05-05T01:26:49,818 [INFO ] pool-3-thread-1 TS_METRICS - MemoryUtilization.Percent:43.7|#Level:Host|#hostname:pupu-Stvullinc,timestamp:1746390409
2025-05-05T01:26:59,499 [INFO ] epollEventLoopGroup-3-1 TS_METRICS - ts_inference_requests_total.Count:1.0|#model_name:my_model,model_version:default|#hostname:pupu-Stvullinc,timestamp:1746390419
2025-05-05T01:26:59,501 [DEBUG] W-9000-my_model_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req.cmd PREDICT repeats 1 to backend at: 1746390419501
2025-05-05T01:26:59,501 [DEBUG] W-9000-my_model_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req.cmd PREDICT repeats 1 to backend at: 1746390419501
2025-05-05T01:26:59,502 [INFO ] W-9000-my_model_1.0 org.pytorch.serve.wlm.WorkerThread - Looping backend response at: 1746390419502
2025-05-05T01:26:59,502 [INFO ] W-9000-my_model_1.0 org.pytorch.serve.wlm.WorkerThread - Looping backend response at: 1746390419502
2025-05-05T01:26:59,503 [INFO ] W-9000-my_model_1.0-stdout MODEL_LOG - Backend received inference at: 1746390419
2025-05-05T01:26:59,667 [INFO ] W-9000-my_model_1.0-stdout MODEL_LOG - Invoking custom service failed.
2025-05-05T01:26:59,667 [INFO ] W-9000-my_model_1.0-stdout MODEL_LOG - Traceback (most recent call last):
2025-05-05T01:26:59,668 [INFO ] W-9000-my_model_1.0-stdout MODEL_LOG -   File "/home/pupu/Desktop/project/.venv/lib/python3.12/site-packages/ts/service.py", line 135, in predict
2025-05-05T01:26:59,668 [INFO ] W-9000-my_model_1.0-stdout MODEL_LOG -     ret = self._entry_point(input_batch, self.context)
2025-05-05T01:26:59,668 [INFO ] W-9000-my_model_1.0-stdout MODEL_LOG -           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2025-05-05T01:26:59,669 [INFO ] W-9000-my_model_1.0 ACCESS_LOG - /127.0.0.1:53610 "POST /predictions/my_model HTTP/1.1" 503 171
2025-05-05T01:26:59,669 [INFO ] W-9000-my_model_1.0-stdout MODEL_LOG -   File "/home/pupu/Desktop/project/.venv/lib/python3.12/site-packages/ts/torch_handler/base_handler.py", line 455, in handle
2025-05-05T01:26:59,669 [INFO ] W-9000-my_model_1.0-stdout MODEL_LOG -     data_preprocess = self.preprocess(data)
2025-05-05T01:26:59,669 [INFO ] W-9000-my_model_1.0 TS_METRICS - Requests5XX.Count:1.0|#Level:Host|#hostname:pupu-Stvullinc,timestamp:1746390419
2025-05-05T01:26:59,670 [DEBUG] W-9000-my_model_1.0 org.pytorch.serve.job.RestJob - Waiting time ns: 195155, Inference time ns: 169621474
2025-05-05T01:26:59,670 [INFO ] W-9000-my_model_1.0-stdout MODEL_LOG -                       ^^^^^^^^^^^^^^^^^^^^^
2025-05-05T01:26:59,670 [DEBUG] W-9000-my_model_1.0 org.pytorch.serve.job.RestJob - Waiting time ns: 195155, Inference time ns: 169621474
2025-05-05T01:26:59,670 [INFO ] W-9000-my_model_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 166
2025-05-05T01:26:59,670 [INFO ] W-9000-my_model_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 166
2025-05-05T01:26:59,671 [INFO ] W-9000-my_model_1.0-stdout MODEL_LOG -   File "/tmp/models/e1259b592ce0457288ac566e5c725fe6/handler.py", line 54, in preprocess
2025-05-05T01:26:59,671 [INFO ] W-9000-my_model_1.0 TS_METRICS - WorkerThreadTime.Milliseconds:4.0|#Level:Host|#hostname:pupu-Stvullinc,timestamp:1746390419
2025-05-05T01:26:59,671 [INFO ] W-9000-my_model_1.0-stdout MODEL_LOG -     img = Image.open(io.BytesIO(img)).convert("RGB")
2025-05-05T01:26:59,671 [INFO ] W-9000-my_model_1.0-stdout MODEL_LOG -           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
2025-05-05T01:26:59,671 [INFO ] W-9000-my_model_1.0-stdout MODEL_LOG -   File "/home/pupu/Desktop/project/.venv/lib/python3.12/site-packages/PIL/Image.py", line 3572, in open
2025-05-05T01:26:59,672 [INFO ] W-9000-my_model_1.0-stdout MODEL_LOG -     raise UnidentifiedImageError(msg)
2025-05-05T01:26:59,672 [INFO ] W-9000-my_model_1.0-stdout MODEL_LOG - PIL.UnidentifiedImageError: cannot identify image file <_io.BytesIO object at 0x769c3ed2a8e0>
2025-05-05T01:27:04,185 [INFO ] epollEventLoopGroup-3-2 TS_METRICS - ts_inference_requests_total.Count:1.0|#model_name:my_model,model_version:default|#hostname:pupu-Stvullinc,timestamp:1746390424
2025-05-05T01:27:04,186 [DEBUG] W-9000-my_model_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req.cmd PREDICT repeats 1 to backend at: 1746390424186
2025-05-05T01:27:04,186 [DEBUG] W-9000-my_model_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req.cmd PREDICT repeats 1 to backend at: 1746390424186
2025-05-05T01:27:04,186 [INFO ] W-9000-my_model_1.0 org.pytorch.serve.wlm.WorkerThread - Looping backend response at: 1746390424186
2025-05-05T01:27:04,186 [INFO ] W-9000-my_model_1.0 org.pytorch.serve.wlm.WorkerThread - Looping backend response at: 1746390424186
2025-05-05T01:27:04,187 [INFO ] W-9000-my_model_1.0-stdout MODEL_LOG - Backend received inference at: 1746390424
2025-05-05T01:27:04,188 [INFO ] W-9000-my_model_1.0-stdout MODEL_LOG - Invoking custom service failed.
2025-05-05T01:27:04,188 [INFO ] W-9000-my_model_1.0-stdout MODEL_LOG - Traceback (most recent call last):
2025-05-05T01:27:04,189 [INFO ] W-9000-my_model_1.0-stdout MODEL_LOG -   File "/home/pupu/Desktop/project/.venv/lib/python3.12/site-packages/ts/service.py", line 135, in predict
2025-05-05T01:27:04,189 [INFO ] W-9000-my_model_1.0 ACCESS_LOG - /127.0.0.1:54922 "POST /predictions/my_model HTTP/1.1" 503 4
2025-05-05T01:27:04,189 [INFO ] W-9000-my_model_1.0-stdout MODEL_LOG -     ret = self._entry_point(input_batch, self.context)
2025-05-05T01:27:04,189 [INFO ] W-9000-my_model_1.0 TS_METRICS - Requests5XX.Count:1.0|#Level:Host|#hostname:pupu-Stvullinc,timestamp:1746390424
2025-05-05T01:27:04,189 [INFO ] W-9000-my_model_1.0-stdout MODEL_LOG -           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2025-05-05T01:27:04,189 [DEBUG] W-9000-my_model_1.0 org.pytorch.serve.job.RestJob - Waiting time ns: 103123, Inference time ns: 3220415
2025-05-05T01:27:04,189 [INFO ] W-9000-my_model_1.0-stdout MODEL_LOG -   File "/home/pupu/Desktop/project/.venv/lib/python3.12/site-packages/ts/torch_handler/base_handler.py", line 455, in handle
2025-05-05T01:27:04,189 [DEBUG] W-9000-my_model_1.0 org.pytorch.serve.job.RestJob - Waiting time ns: 103123, Inference time ns: 3220415
2025-05-05T01:27:04,189 [INFO ] W-9000-my_model_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 1
2025-05-05T01:27:04,189 [INFO ] W-9000-my_model_1.0-stdout MODEL_LOG -     data_preprocess = self.preprocess(data)
2025-05-05T01:27:04,189 [INFO ] W-9000-my_model_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 1
2025-05-05T01:27:04,190 [INFO ] W-9000-my_model_1.0-stdout MODEL_LOG -                       ^^^^^^^^^^^^^^^^^^^^^
2025-05-05T01:27:04,190 [INFO ] W-9000-my_model_1.0 TS_METRICS - WorkerThreadTime.Milliseconds:3.0|#Level:Host|#hostname:pupu-Stvullinc,timestamp:1746390424
2025-05-05T01:27:04,190 [INFO ] W-9000-my_model_1.0-stdout MODEL_LOG -   File "/tmp/models/e1259b592ce0457288ac566e5c725fe6/handler.py", line 54, in preprocess
2025-05-05T01:27:04,190 [INFO ] W-9000-my_model_1.0-stdout MODEL_LOG -     img = Image.open(io.BytesIO(img)).convert("RGB")
2025-05-05T01:27:04,190 [INFO ] W-9000-my_model_1.0-stdout MODEL_LOG -           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
2025-05-05T01:27:04,191 [INFO ] W-9000-my_model_1.0-stdout MODEL_LOG -   File "/home/pupu/Desktop/project/.venv/lib/python3.12/site-packages/PIL/Image.py", line 3572, in open
2025-05-05T01:27:04,191 [INFO ] W-9000-my_model_1.0-stdout MODEL_LOG -     raise UnidentifiedImageError(msg)
2025-05-05T01:27:04,191 [INFO ] W-9000-my_model_1.0-stdout MODEL_LOG - PIL.UnidentifiedImageError: cannot identify image file <_io.BytesIO object at 0x769c25f81da0>
2025-05-05T01:27:05,139 [INFO ] epollEventLoopGroup-3-3 TS_METRICS - ts_inference_requests_total.Count:1.0|#model_name:my_model,model_version:default|#hostname:pupu-Stvullinc,timestamp:1746390425
2025-05-05T01:27:05,140 [DEBUG] W-9000-my_model_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req.cmd PREDICT repeats 1 to backend at: 1746390425139
2025-05-05T01:27:05,140 [DEBUG] W-9000-my_model_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req.cmd PREDICT repeats 1 to backend at: 1746390425139
2025-05-05T01:27:05,140 [INFO ] W-9000-my_model_1.0 org.pytorch.serve.wlm.WorkerThread - Looping backend response at: 1746390425140
2025-05-05T01:27:05,140 [INFO ] W-9000-my_model_1.0 org.pytorch.serve.wlm.WorkerThread - Looping backend response at: 1746390425140
2025-05-05T01:27:05,141 [INFO ] W-9000-my_model_1.0-stdout MODEL_LOG - Backend received inference at: 1746390425
2025-05-05T01:27:05,141 [INFO ] W-9000-my_model_1.0-stdout MODEL_LOG - Invoking custom service failed.
2025-05-05T01:27:05,142 [INFO ] W-9000-my_model_1.0-stdout MODEL_LOG - Traceback (most recent call last):
2025-05-05T01:27:05,142 [INFO ] W-9000-my_model_1.0-stdout MODEL_LOG -   File "/home/pupu/Desktop/project/.venv/lib/python3.12/site-packages/ts/service.py", line 135, in predict
2025-05-05T01:27:05,142 [INFO ] W-9000-my_model_1.0 ACCESS_LOG - /127.0.0.1:54932 "POST /predictions/my_model HTTP/1.1" 503 4
2025-05-05T01:27:05,142 [INFO ] W-9000-my_model_1.0-stdout MODEL_LOG -     ret = self._entry_point(input_batch, self.context)
2025-05-05T01:27:05,142 [INFO ] W-9000-my_model_1.0 TS_METRICS - Requests5XX.Count:1.0|#Level:Host|#hostname:pupu-Stvullinc,timestamp:1746390425
2025-05-05T01:27:05,142 [INFO ] W-9000-my_model_1.0-stdout MODEL_LOG -           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2025-05-05T01:27:05,142 [DEBUG] W-9000-my_model_1.0 org.pytorch.serve.job.RestJob - Waiting time ns: 94604, Inference time ns: 2923110
2025-05-05T01:27:05,142 [INFO ] W-9000-my_model_1.0-stdout MODEL_LOG -   File "/home/pupu/Desktop/project/.venv/lib/python3.12/site-packages/ts/torch_handler/base_handler.py", line 455, in handle
2025-05-05T01:27:05,142 [DEBUG] W-9000-my_model_1.0 org.pytorch.serve.job.RestJob - Waiting time ns: 94604, Inference time ns: 2923110
2025-05-05T01:27:05,142 [INFO ] W-9000-my_model_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 2
2025-05-05T01:27:05,142 [INFO ] W-9000-my_model_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 2
2025-05-05T01:27:05,142 [INFO ] W-9000-my_model_1.0-stdout MODEL_LOG -     data_preprocess = self.preprocess(data)
2025-05-05T01:27:05,143 [INFO ] W-9000-my_model_1.0-stdout MODEL_LOG -                       ^^^^^^^^^^^^^^^^^^^^^
2025-05-05T01:27:05,143 [INFO ] W-9000-my_model_1.0 TS_METRICS - WorkerThreadTime.Milliseconds:2.0|#Level:Host|#hostname:pupu-Stvullinc,timestamp:1746390425
2025-05-05T01:27:05,143 [INFO ] W-9000-my_model_1.0-stdout MODEL_LOG -   File "/tmp/models/e1259b592ce0457288ac566e5c725fe6/handler.py", line 54, in preprocess
2025-05-05T01:27:05,143 [INFO ] W-9000-my_model_1.0-stdout MODEL_LOG -     img = Image.open(io.BytesIO(img)).convert("RGB")
2025-05-05T01:27:05,143 [INFO ] W-9000-my_model_1.0-stdout MODEL_LOG -           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
2025-05-05T01:27:05,143 [INFO ] W-9000-my_model_1.0-stdout MODEL_LOG -   File "/home/pupu/Desktop/project/.venv/lib/python3.12/site-packages/PIL/Image.py", line 3572, in open
2025-05-05T01:27:05,144 [INFO ] W-9000-my_model_1.0-stdout MODEL_LOG -     raise UnidentifiedImageError(msg)
2025-05-05T01:27:05,144 [INFO ] W-9000-my_model_1.0-stdout MODEL_LOG - PIL.UnidentifiedImageError: cannot identify image file <_io.BytesIO object at 0x769c25f81d00>
2025-05-05T01:27:49,807 [INFO ] pool-3-thread-1 TS_METRICS - CPUUtilization.Percent:0.0|#Level:Host|#hostname:pupu-Stvullinc,timestamp:1746390469
2025-05-05T01:27:49,807 [INFO ] pool-3-thread-1 TS_METRICS - DiskAvailable.Gigabytes:75.43573760986328|#Level:Host|#hostname:pupu-Stvullinc,timestamp:1746390469
2025-05-05T01:27:49,808 [INFO ] pool-3-thread-1 TS_METRICS - DiskUsage.Gigabytes:62.74610900878906|#Level:Host|#hostname:pupu-Stvullinc,timestamp:1746390469
2025-05-05T01:27:49,808 [INFO ] pool-3-thread-1 TS_METRICS - DiskUtilization.Percent:45.4|#Level:Host|#hostname:pupu-Stvullinc,timestamp:1746390469
2025-05-05T01:27:49,809 [INFO ] pool-3-thread-1 TS_METRICS - GPUMemoryUtilization.Percent:7.080078125|#Level:Host,DeviceId:0|#hostname:pupu-Stvullinc,timestamp:1746390469
2025-05-05T01:27:49,809 [INFO ] pool-3-thread-1 TS_METRICS - GPUMemoryUsed.Megabytes:580.0|#Level:Host,DeviceId:0|#hostname:pupu-Stvullinc,timestamp:1746390469
2025-05-05T01:27:49,809 [INFO ] pool-3-thread-1 TS_METRICS - GPUUtilization.Percent:22.0|#Level:Host,DeviceId:0|#hostname:pupu-Stvullinc,timestamp:1746390469
2025-05-05T01:27:49,809 [INFO ] pool-3-thread-1 TS_METRICS - MemoryAvailable.Megabytes:8931.5625|#Level:Host|#hostname:pupu-Stvullinc,timestamp:1746390469
2025-05-05T01:27:49,810 [INFO ] pool-3-thread-1 TS_METRICS - MemoryUsed.Megabytes:6589.90625|#Level:Host|#hostname:pupu-Stvullinc,timestamp:1746390469
2025-05-05T01:27:49,810 [INFO ] pool-3-thread-1 TS_METRICS - MemoryUtilization.Percent:44.0|#Level:Host|#hostname:pupu-Stvullinc,timestamp:1746390469
2025-05-05T01:28:36,232 [INFO ] epollEventLoopGroup-3-4 TS_METRICS - ts_inference_requests_total.Count:1.0|#model_name:my_model,model_version:default|#hostname:pupu-Stvullinc,timestamp:1746390516
2025-05-05T01:28:36,233 [DEBUG] W-9000-my_model_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req.cmd PREDICT repeats 1 to backend at: 1746390516233
2025-05-05T01:28:36,233 [DEBUG] W-9000-my_model_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req.cmd PREDICT repeats 1 to backend at: 1746390516233
2025-05-05T01:28:36,234 [INFO ] W-9000-my_model_1.0 org.pytorch.serve.wlm.WorkerThread - Looping backend response at: 1746390516234
2025-05-05T01:28:36,234 [INFO ] W-9000-my_model_1.0 org.pytorch.serve.wlm.WorkerThread - Looping backend response at: 1746390516234
2025-05-05T01:28:36,235 [INFO ] W-9000-my_model_1.0-stdout MODEL_LOG - Backend received inference at: 1746390516
2025-05-05T01:28:36,236 [INFO ] W-9000-my_model_1.0-stdout MODEL_LOG - Invoking custom service failed.
2025-05-05T01:28:36,236 [INFO ] W-9000-my_model_1.0-stdout MODEL_LOG - Traceback (most recent call last):
2025-05-05T01:28:36,236 [INFO ] W-9000-my_model_1.0-stdout MODEL_LOG -   File "/home/pupu/Desktop/project/.venv/lib/python3.12/site-packages/ts/service.py", line 135, in predict
2025-05-05T01:28:36,236 [INFO ] W-9000-my_model_1.0-stdout MODEL_LOG -     ret = self._entry_point(input_batch, self.context)
2025-05-05T01:28:36,237 [INFO ] W-9000-my_model_1.0-stdout MODEL_LOG -           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2025-05-05T01:28:36,237 [INFO ] W-9000-my_model_1.0-stdout MODEL_LOG -   File "/home/pupu/Desktop/project/.venv/lib/python3.12/site-packages/ts/torch_handler/base_handler.py", line 455, in handle
2025-05-05T01:28:36,237 [INFO ] W-9000-my_model_1.0-stdout MODEL_LOG -     data_preprocess = self.preprocess(data)
2025-05-05T01:28:36,237 [INFO ] W-9000-my_model_1.0-stdout MODEL_LOG -                       ^^^^^^^^^^^^^^^^^^^^^
2025-05-05T01:28:36,238 [INFO ] W-9000-my_model_1.0-stdout MODEL_LOG -   File "/tmp/models/e1259b592ce0457288ac566e5c725fe6/handler.py", line 54, in preprocess
2025-05-05T01:28:36,238 [INFO ] W-9000-my_model_1.0-stdout MODEL_LOG -     img = Image.open(io.BytesIO(img)).convert("RGB")
2025-05-05T01:28:36,238 [INFO ] W-9000-my_model_1.0 ACCESS_LOG - /127.0.0.1:42900 "POST /predictions/my_model HTTP/1.1" 503 6
2025-05-05T01:28:36,238 [INFO ] W-9000-my_model_1.0-stdout MODEL_LOG -           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
2025-05-05T01:28:36,238 [INFO ] W-9000-my_model_1.0 TS_METRICS - Requests5XX.Count:1.0|#Level:Host|#hostname:pupu-Stvullinc,timestamp:1746390516
2025-05-05T01:28:36,238 [INFO ] W-9000-my_model_1.0-stdout MODEL_LOG -   File "/home/pupu/Desktop/project/.venv/lib/python3.12/site-packages/PIL/Image.py", line 3572, in open
2025-05-05T01:28:36,238 [DEBUG] W-9000-my_model_1.0 org.pytorch.serve.job.RestJob - Waiting time ns: 84367, Inference time ns: 5554501
2025-05-05T01:28:36,238 [INFO ] W-9000-my_model_1.0-stdout MODEL_LOG -     raise UnidentifiedImageError(msg)
2025-05-05T01:28:36,238 [DEBUG] W-9000-my_model_1.0 org.pytorch.serve.job.RestJob - Waiting time ns: 84367, Inference time ns: 5554501
2025-05-05T01:28:36,239 [INFO ] W-9000-my_model_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 3
2025-05-05T01:28:36,239 [INFO ] W-9000-my_model_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 3
2025-05-05T01:28:36,239 [INFO ] W-9000-my_model_1.0-stdout MODEL_LOG - PIL.UnidentifiedImageError: cannot identify image file <_io.BytesIO object at 0x769c25f274c0>
2025-05-05T01:28:36,239 [INFO ] W-9000-my_model_1.0 TS_METRICS - WorkerThreadTime.Milliseconds:3.0|#Level:Host|#hostname:pupu-Stvullinc,timestamp:1746390516
2025-05-05T01:28:49,817 [INFO ] pool-3-thread-1 TS_METRICS - CPUUtilization.Percent:0.0|#Level:Host|#hostname:pupu-Stvullinc,timestamp:1746390529
2025-05-05T01:28:49,818 [INFO ] pool-3-thread-1 TS_METRICS - DiskAvailable.Gigabytes:75.43560409545898|#Level:Host|#hostname:pupu-Stvullinc,timestamp:1746390529
2025-05-05T01:28:49,818 [INFO ] pool-3-thread-1 TS_METRICS - DiskUsage.Gigabytes:62.74624252319336|#Level:Host|#hostname:pupu-Stvullinc,timestamp:1746390529
2025-05-05T01:28:49,819 [INFO ] pool-3-thread-1 TS_METRICS - DiskUtilization.Percent:45.4|#Level:Host|#hostname:pupu-Stvullinc,timestamp:1746390529
2025-05-05T01:28:49,819 [INFO ] pool-3-thread-1 TS_METRICS - GPUMemoryUtilization.Percent:7.06787109375|#Level:Host,DeviceId:0|#hostname:pupu-Stvullinc,timestamp:1746390529
2025-05-05T01:28:49,819 [INFO ] pool-3-thread-1 TS_METRICS - GPUMemoryUsed.Megabytes:579.0|#Level:Host,DeviceId:0|#hostname:pupu-Stvullinc,timestamp:1746390529
2025-05-05T01:28:49,820 [INFO ] pool-3-thread-1 TS_METRICS - GPUUtilization.Percent:3.0|#Level:Host,DeviceId:0|#hostname:pupu-Stvullinc,timestamp:1746390529
2025-05-05T01:28:49,820 [INFO ] pool-3-thread-1 TS_METRICS - MemoryAvailable.Megabytes:8887.93359375|#Level:Host|#hostname:pupu-Stvullinc,timestamp:1746390529
2025-05-05T01:28:49,820 [INFO ] pool-3-thread-1 TS_METRICS - MemoryUsed.Megabytes:6640.6015625|#Level:Host|#hostname:pupu-Stvullinc,timestamp:1746390529
2025-05-05T01:28:49,821 [INFO ] pool-3-thread-1 TS_METRICS - MemoryUtilization.Percent:44.3|#Level:Host|#hostname:pupu-Stvullinc,timestamp:1746390529
2025-05-05T01:29:49,828 [INFO ] pool-3-thread-1 TS_METRICS - CPUUtilization.Percent:25.0|#Level:Host|#hostname:pupu-Stvullinc,timestamp:1746390589
2025-05-05T01:29:49,828 [INFO ] pool-3-thread-1 TS_METRICS - DiskAvailable.Gigabytes:75.43523406982422|#Level:Host|#hostname:pupu-Stvullinc,timestamp:1746390589
2025-05-05T01:29:49,829 [INFO ] pool-3-thread-1 TS_METRICS - DiskUsage.Gigabytes:62.746612548828125|#Level:Host|#hostname:pupu-Stvullinc,timestamp:1746390589
2025-05-05T01:29:49,829 [INFO ] pool-3-thread-1 TS_METRICS - DiskUtilization.Percent:45.4|#Level:Host|#hostname:pupu-Stvullinc,timestamp:1746390589
2025-05-05T01:29:49,829 [INFO ] pool-3-thread-1 TS_METRICS - GPUMemoryUtilization.Percent:7.470703125|#Level:Host,DeviceId:0|#hostname:pupu-Stvullinc,timestamp:1746390589
2025-05-05T01:29:49,830 [INFO ] pool-3-thread-1 TS_METRICS - GPUMemoryUsed.Megabytes:612.0|#Level:Host,DeviceId:0|#hostname:pupu-Stvullinc,timestamp:1746390589
2025-05-05T01:29:49,830 [INFO ] pool-3-thread-1 TS_METRICS - GPUUtilization.Percent:11.0|#Level:Host,DeviceId:0|#hostname:pupu-Stvullinc,timestamp:1746390589
2025-05-05T01:29:49,830 [INFO ] pool-3-thread-1 TS_METRICS - MemoryAvailable.Megabytes:8833.5703125|#Level:Host|#hostname:pupu-Stvullinc,timestamp:1746390589
2025-05-05T01:29:49,831 [INFO ] pool-3-thread-1 TS_METRICS - MemoryUsed.Megabytes:6688.015625|#Level:Host|#hostname:pupu-Stvullinc,timestamp:1746390589
2025-05-05T01:29:49,831 [INFO ] pool-3-thread-1 TS_METRICS - MemoryUtilization.Percent:44.6|#Level:Host|#hostname:pupu-Stvullinc,timestamp:1746390589
2025-05-05T01:30:49,837 [INFO ] pool-3-thread-1 TS_METRICS - CPUUtilization.Percent:0.0|#Level:Host|#hostname:pupu-Stvullinc,timestamp:1746390649
2025-05-05T01:30:49,837 [INFO ] pool-3-thread-1 TS_METRICS - DiskAvailable.Gigabytes:75.4351921081543|#Level:Host|#hostname:pupu-Stvullinc,timestamp:1746390649
2025-05-05T01:30:49,837 [INFO ] pool-3-thread-1 TS_METRICS - DiskUsage.Gigabytes:62.74665451049805|#Level:Host|#hostname:pupu-Stvullinc,timestamp:1746390649
2025-05-05T01:30:49,837 [INFO ] pool-3-thread-1 TS_METRICS - DiskUtilization.Percent:45.4|#Level:Host|#hostname:pupu-Stvullinc,timestamp:1746390649
2025-05-05T01:30:49,838 [INFO ] pool-3-thread-1 TS_METRICS - GPUMemoryUtilization.Percent:7.45849609375|#Level:Host,DeviceId:0|#hostname:pupu-Stvullinc,timestamp:1746390649
2025-05-05T01:30:49,838 [INFO ] pool-3-thread-1 TS_METRICS - GPUMemoryUsed.Megabytes:611.0|#Level:Host,DeviceId:0|#hostname:pupu-Stvullinc,timestamp:1746390649
2025-05-05T01:30:49,838 [INFO ] pool-3-thread-1 TS_METRICS - GPUUtilization.Percent:20.0|#Level:Host,DeviceId:0|#hostname:pupu-Stvullinc,timestamp:1746390649
2025-05-05T01:30:49,839 [INFO ] pool-3-thread-1 TS_METRICS - MemoryAvailable.Megabytes:8831.03125|#Level:Host|#hostname:pupu-Stvullinc,timestamp:1746390649
2025-05-05T01:30:49,839 [INFO ] pool-3-thread-1 TS_METRICS - MemoryUsed.Megabytes:6689.4296875|#Level:Host|#hostname:pupu-Stvullinc,timestamp:1746390649
2025-05-05T01:30:49,839 [INFO ] pool-3-thread-1 TS_METRICS - MemoryUtilization.Percent:44.7|#Level:Host|#hostname:pupu-Stvullinc,timestamp:1746390649
2025-05-05T01:31:49,809 [INFO ] pool-3-thread-1 TS_METRICS - CPUUtilization.Percent:33.3|#Level:Host|#hostname:pupu-Stvullinc,timestamp:1746390709
2025-05-05T01:31:49,809 [INFO ] pool-3-thread-1 TS_METRICS - DiskAvailable.Gigabytes:75.43511962890625|#Level:Host|#hostname:pupu-Stvullinc,timestamp:1746390709
2025-05-05T01:31:49,810 [INFO ] pool-3-thread-1 TS_METRICS - DiskUsage.Gigabytes:62.746726989746094|#Level:Host|#hostname:pupu-Stvullinc,timestamp:1746390709
2025-05-05T01:31:49,810 [INFO ] pool-3-thread-1 TS_METRICS - DiskUtilization.Percent:45.4|#Level:Host|#hostname:pupu-Stvullinc,timestamp:1746390709
2025-05-05T01:31:49,810 [INFO ] pool-3-thread-1 TS_METRICS - GPUMemoryUtilization.Percent:7.32421875|#Level:Host,DeviceId:0|#hostname:pupu-Stvullinc,timestamp:1746390709
2025-05-05T01:31:49,810 [INFO ] pool-3-thread-1 TS_METRICS - GPUMemoryUsed.Megabytes:600.0|#Level:Host,DeviceId:0|#hostname:pupu-Stvullinc,timestamp:1746390709
2025-05-05T01:31:49,810 [INFO ] pool-3-thread-1 TS_METRICS - GPUUtilization.Percent:23.0|#Level:Host,DeviceId:0|#hostname:pupu-Stvullinc,timestamp:1746390709
2025-05-05T01:31:49,811 [INFO ] pool-3-thread-1 TS_METRICS - MemoryAvailable.Megabytes:8735.75|#Level:Host|#hostname:pupu-Stvullinc,timestamp:1746390709
2025-05-05T01:31:49,811 [INFO ] pool-3-thread-1 TS_METRICS - MemoryUsed.Megabytes:6788.828125|#Level:Host|#hostname:pupu-Stvullinc,timestamp:1746390709
2025-05-05T01:31:49,811 [INFO ] pool-3-thread-1 TS_METRICS - MemoryUtilization.Percent:45.3|#Level:Host|#hostname:pupu-Stvullinc,timestamp:1746390709
2025-05-05T01:32:49,790 [INFO ] pool-3-thread-1 TS_METRICS - CPUUtilization.Percent:0.0|#Level:Host|#hostname:pupu-Stvullinc,timestamp:1746390769
2025-05-05T01:32:49,790 [INFO ] pool-3-thread-1 TS_METRICS - DiskAvailable.Gigabytes:75.43502426147461|#Level:Host|#hostname:pupu-Stvullinc,timestamp:1746390769
2025-05-05T01:32:49,790 [INFO ] pool-3-thread-1 TS_METRICS - DiskUsage.Gigabytes:62.746822357177734|#Level:Host|#hostname:pupu-Stvullinc,timestamp:1746390769
2025-05-05T01:32:49,791 [INFO ] pool-3-thread-1 TS_METRICS - DiskUtilization.Percent:45.4|#Level:Host|#hostname:pupu-Stvullinc,timestamp:1746390769
2025-05-05T01:32:49,791 [INFO ] pool-3-thread-1 TS_METRICS - GPUMemoryUtilization.Percent:7.31201171875|#Level:Host,DeviceId:0|#hostname:pupu-Stvullinc,timestamp:1746390769
2025-05-05T01:32:49,791 [INFO ] pool-3-thread-1 TS_METRICS - GPUMemoryUsed.Megabytes:599.0|#Level:Host,DeviceId:0|#hostname:pupu-Stvullinc,timestamp:1746390769
2025-05-05T01:32:49,791 [INFO ] pool-3-thread-1 TS_METRICS - GPUUtilization.Percent:0.0|#Level:Host,DeviceId:0|#hostname:pupu-Stvullinc,timestamp:1746390769
2025-05-05T01:32:49,792 [INFO ] pool-3-thread-1 TS_METRICS - MemoryAvailable.Megabytes:8739.3359375|#Level:Host|#hostname:pupu-Stvullinc,timestamp:1746390769
2025-05-05T01:32:49,792 [INFO ] pool-3-thread-1 TS_METRICS - MemoryUsed.Megabytes:6785.0078125|#Level:Host|#hostname:pupu-Stvullinc,timestamp:1746390769
2025-05-05T01:32:49,792 [INFO ] pool-3-thread-1 TS_METRICS - MemoryUtilization.Percent:45.2|#Level:Host|#hostname:pupu-Stvullinc,timestamp:1746390769
2025-05-05T01:33:49,799 [INFO ] pool-3-thread-1 TS_METRICS - CPUUtilization.Percent:0.0|#Level:Host|#hostname:pupu-Stvullinc,timestamp:1746390829
2025-05-05T01:33:49,799 [INFO ] pool-3-thread-1 TS_METRICS - DiskAvailable.Gigabytes:75.4349479675293|#Level:Host|#hostname:pupu-Stvullinc,timestamp:1746390829
2025-05-05T01:33:49,800 [INFO ] pool-3-thread-1 TS_METRICS - DiskUsage.Gigabytes:62.74689865112305|#Level:Host|#hostname:pupu-Stvullinc,timestamp:1746390829
2025-05-05T01:33:49,800 [INFO ] pool-3-thread-1 TS_METRICS - DiskUtilization.Percent:45.4|#Level:Host|#hostname:pupu-Stvullinc,timestamp:1746390829
2025-05-05T01:33:49,800 [INFO ] pool-3-thread-1 TS_METRICS - GPUMemoryUtilization.Percent:7.2021484375|#Level:Host,DeviceId:0|#hostname:pupu-Stvullinc,timestamp:1746390829
2025-05-05T01:33:49,801 [INFO ] pool-3-thread-1 TS_METRICS - GPUMemoryUsed.Megabytes:590.0|#Level:Host,DeviceId:0|#hostname:pupu-Stvullinc,timestamp:1746390829
2025-05-05T01:33:49,801 [INFO ] pool-3-thread-1 TS_METRICS - GPUUtilization.Percent:1.0|#Level:Host,DeviceId:0|#hostname:pupu-Stvullinc,timestamp:1746390829
2025-05-05T01:33:49,801 [INFO ] pool-3-thread-1 TS_METRICS - MemoryAvailable.Megabytes:8761.80859375|#Level:Host|#hostname:pupu-Stvullinc,timestamp:1746390829
2025-05-05T01:33:49,802 [INFO ] pool-3-thread-1 TS_METRICS - MemoryUsed.Megabytes:6762.09375|#Level:Host|#hostname:pupu-Stvullinc,timestamp:1746390829
2025-05-05T01:33:49,802 [INFO ] pool-3-thread-1 TS_METRICS - MemoryUtilization.Percent:45.1|#Level:Host|#hostname:pupu-Stvullinc,timestamp:1746390829
2025-05-05T01:34:49,840 [INFO ] pool-3-thread-1 TS_METRICS - CPUUtilization.Percent:0.0|#Level:Host|#hostname:pupu-Stvullinc,timestamp:1746390889
2025-05-05T01:34:49,840 [INFO ] pool-3-thread-1 TS_METRICS - DiskAvailable.Gigabytes:75.43528747558594|#Level:Host|#hostname:pupu-Stvullinc,timestamp:1746390889
2025-05-05T01:34:49,841 [INFO ] pool-3-thread-1 TS_METRICS - DiskUsage.Gigabytes:62.746559143066406|#Level:Host|#hostname:pupu-Stvullinc,timestamp:1746390889
2025-05-05T01:34:49,841 [INFO ] pool-3-thread-1 TS_METRICS - DiskUtilization.Percent:45.4|#Level:Host|#hostname:pupu-Stvullinc,timestamp:1746390889
2025-05-05T01:34:49,841 [INFO ] pool-3-thread-1 TS_METRICS - GPUMemoryUtilization.Percent:7.2021484375|#Level:Host,DeviceId:0|#hostname:pupu-Stvullinc,timestamp:1746390889
2025-05-05T01:34:49,841 [INFO ] pool-3-thread-1 TS_METRICS - GPUMemoryUsed.Megabytes:590.0|#Level:Host,DeviceId:0|#hostname:pupu-Stvullinc,timestamp:1746390889
2025-05-05T01:34:49,841 [INFO ] pool-3-thread-1 TS_METRICS - GPUUtilization.Percent:1.0|#Level:Host,DeviceId:0|#hostname:pupu-Stvullinc,timestamp:1746390889
2025-05-05T01:34:49,841 [INFO ] pool-3-thread-1 TS_METRICS - MemoryAvailable.Megabytes:8715.25390625|#Level:Host|#hostname:pupu-Stvullinc,timestamp:1746390889
2025-05-05T01:34:49,842 [INFO ] pool-3-thread-1 TS_METRICS - MemoryUsed.Megabytes:6809.078125|#Level:Host|#hostname:pupu-Stvullinc,timestamp:1746390889
2025-05-05T01:34:49,842 [INFO ] pool-3-thread-1 TS_METRICS - MemoryUtilization.Percent:45.4|#Level:Host|#hostname:pupu-Stvullinc,timestamp:1746390889
2025-05-05T01:35:50,029 [INFO ] pool-3-thread-1 TS_METRICS - CPUUtilization.Percent:100.0|#Level:Host|#hostname:pupu-Stvullinc,timestamp:1746390950
2025-05-05T01:35:50,029 [INFO ] pool-3-thread-1 TS_METRICS - DiskAvailable.Gigabytes:75.43523406982422|#Level:Host|#hostname:pupu-Stvullinc,timestamp:1746390950
2025-05-05T01:35:50,030 [INFO ] pool-3-thread-1 TS_METRICS - DiskUsage.Gigabytes:62.746612548828125|#Level:Host|#hostname:pupu-Stvullinc,timestamp:1746390950
2025-05-05T01:35:50,030 [INFO ] pool-3-thread-1 TS_METRICS - DiskUtilization.Percent:45.4|#Level:Host|#hostname:pupu-Stvullinc,timestamp:1746390950
2025-05-05T01:35:50,030 [INFO ] pool-3-thread-1 TS_METRICS - GPUMemoryUtilization.Percent:7.18994140625|#Level:Host,DeviceId:0|#hostname:pupu-Stvullinc,timestamp:1746390950
2025-05-05T01:35:50,030 [INFO ] pool-3-thread-1 TS_METRICS - GPUMemoryUsed.Megabytes:589.0|#Level:Host,DeviceId:0|#hostname:pupu-Stvullinc,timestamp:1746390950
2025-05-05T01:35:50,030 [INFO ] pool-3-thread-1 TS_METRICS - GPUUtilization.Percent:7.0|#Level:Host,DeviceId:0|#hostname:pupu-Stvullinc,timestamp:1746390950
2025-05-05T01:35:50,030 [INFO ] pool-3-thread-1 TS_METRICS - MemoryAvailable.Megabytes:8748.88671875|#Level:Host|#hostname:pupu-Stvullinc,timestamp:1746390950
2025-05-05T01:35:50,031 [INFO ] pool-3-thread-1 TS_METRICS - MemoryUsed.Megabytes:6779.47265625|#Level:Host|#hostname:pupu-Stvullinc,timestamp:1746390950
2025-05-05T01:35:50,031 [INFO ] pool-3-thread-1 TS_METRICS - MemoryUtilization.Percent:45.2|#Level:Host|#hostname:pupu-Stvullinc,timestamp:1746390950
2025-05-05T01:36:49,989 [INFO ] pool-3-thread-1 TS_METRICS - CPUUtilization.Percent:100.0|#Level:Host|#hostname:pupu-Stvullinc,timestamp:1746391009
2025-05-05T01:36:49,989 [INFO ] pool-3-thread-1 TS_METRICS - DiskAvailable.Gigabytes:75.4351692199707|#Level:Host|#hostname:pupu-Stvullinc,timestamp:1746391009
2025-05-05T01:36:49,989 [INFO ] pool-3-thread-1 TS_METRICS - DiskUsage.Gigabytes:62.74667739868164|#Level:Host|#hostname:pupu-Stvullinc,timestamp:1746391009
2025-05-05T01:36:49,989 [INFO ] pool-3-thread-1 TS_METRICS - DiskUtilization.Percent:45.4|#Level:Host|#hostname:pupu-Stvullinc,timestamp:1746391009
2025-05-05T01:36:49,989 [INFO ] pool-3-thread-1 TS_METRICS - GPUMemoryUtilization.Percent:6.99462890625|#Level:Host,DeviceId:0|#hostname:pupu-Stvullinc,timestamp:1746391009
2025-05-05T01:36:49,990 [INFO ] pool-3-thread-1 TS_METRICS - GPUMemoryUsed.Megabytes:573.0|#Level:Host,DeviceId:0|#hostname:pupu-Stvullinc,timestamp:1746391009
2025-05-05T01:36:49,990 [INFO ] pool-3-thread-1 TS_METRICS - GPUUtilization.Percent:4.0|#Level:Host,DeviceId:0|#hostname:pupu-Stvullinc,timestamp:1746391009
2025-05-05T01:36:49,990 [INFO ] pool-3-thread-1 TS_METRICS - MemoryAvailable.Megabytes:8744.03515625|#Level:Host|#hostname:pupu-Stvullinc,timestamp:1746391009
2025-05-05T01:36:49,990 [INFO ] pool-3-thread-1 TS_METRICS - MemoryUsed.Megabytes:6781.32421875|#Level:Host|#hostname:pupu-Stvullinc,timestamp:1746391009
2025-05-05T01:36:49,990 [INFO ] pool-3-thread-1 TS_METRICS - MemoryUtilization.Percent:45.2|#Level:Host|#hostname:pupu-Stvullinc,timestamp:1746391009
2025-05-05T01:37:49,836 [INFO ] pool-3-thread-1 TS_METRICS - CPUUtilization.Percent:0.0|#Level:Host|#hostname:pupu-Stvullinc,timestamp:1746391069
2025-05-05T01:37:49,836 [INFO ] pool-3-thread-1 TS_METRICS - DiskAvailable.Gigabytes:75.43509292602539|#Level:Host|#hostname:pupu-Stvullinc,timestamp:1746391069
2025-05-05T01:37:49,837 [INFO ] pool-3-thread-1 TS_METRICS - DiskUsage.Gigabytes:62.74675369262695|#Level:Host|#hostname:pupu-Stvullinc,timestamp:1746391069
2025-05-05T01:37:49,837 [INFO ] pool-3-thread-1 TS_METRICS - DiskUtilization.Percent:45.4|#Level:Host|#hostname:pupu-Stvullinc,timestamp:1746391069
2025-05-05T01:37:49,838 [INFO ] pool-3-thread-1 TS_METRICS - GPUMemoryUtilization.Percent:6.99462890625|#Level:Host,DeviceId:0|#hostname:pupu-Stvullinc,timestamp:1746391069
2025-05-05T01:37:49,838 [INFO ] pool-3-thread-1 TS_METRICS - GPUMemoryUsed.Megabytes:573.0|#Level:Host,DeviceId:0|#hostname:pupu-Stvullinc,timestamp:1746391069
2025-05-05T01:37:49,838 [INFO ] pool-3-thread-1 TS_METRICS - GPUUtilization.Percent:5.0|#Level:Host,DeviceId:0|#hostname:pupu-Stvullinc,timestamp:1746391069
2025-05-05T01:37:49,839 [INFO ] pool-3-thread-1 TS_METRICS - MemoryAvailable.Megabytes:8746.7265625|#Level:Host|#hostname:pupu-Stvullinc,timestamp:1746391069
2025-05-05T01:37:49,839 [INFO ] pool-3-thread-1 TS_METRICS - MemoryUsed.Megabytes:6781.6328125|#Level:Host|#hostname:pupu-Stvullinc,timestamp:1746391069
2025-05-05T01:37:49,840 [INFO ] pool-3-thread-1 TS_METRICS - MemoryUtilization.Percent:45.2|#Level:Host|#hostname:pupu-Stvullinc,timestamp:1746391069
2025-05-05T01:38:49,823 [INFO ] pool-3-thread-1 TS_METRICS - CPUUtilization.Percent:0.0|#Level:Host|#hostname:pupu-Stvullinc,timestamp:1746391129
2025-05-05T01:38:49,823 [INFO ] pool-3-thread-1 TS_METRICS - DiskAvailable.Gigabytes:75.43502807617188|#Level:Host|#hostname:pupu-Stvullinc,timestamp:1746391129
2025-05-05T01:38:49,823 [INFO ] pool-3-thread-1 TS_METRICS - DiskUsage.Gigabytes:62.74681854248047|#Level:Host|#hostname:pupu-Stvullinc,timestamp:1746391129
2025-05-05T01:38:49,823 [INFO ] pool-3-thread-1 TS_METRICS - DiskUtilization.Percent:45.4|#Level:Host|#hostname:pupu-Stvullinc,timestamp:1746391129
2025-05-05T01:38:49,824 [INFO ] pool-3-thread-1 TS_METRICS - GPUMemoryUtilization.Percent:6.93359375|#Level:Host,DeviceId:0|#hostname:pupu-Stvullinc,timestamp:1746391129
2025-05-05T01:38:49,824 [INFO ] pool-3-thread-1 TS_METRICS - GPUMemoryUsed.Megabytes:568.0|#Level:Host,DeviceId:0|#hostname:pupu-Stvullinc,timestamp:1746391129
2025-05-05T01:38:49,824 [INFO ] pool-3-thread-1 TS_METRICS - GPUUtilization.Percent:8.0|#Level:Host,DeviceId:0|#hostname:pupu-Stvullinc,timestamp:1746391129
2025-05-05T01:38:49,824 [INFO ] pool-3-thread-1 TS_METRICS - MemoryAvailable.Megabytes:8697.8046875|#Level:Host|#hostname:pupu-Stvullinc,timestamp:1746391129
2025-05-05T01:38:49,824 [INFO ] pool-3-thread-1 TS_METRICS - MemoryUsed.Megabytes:6830.5546875|#Level:Host|#hostname:pupu-Stvullinc,timestamp:1746391129
2025-05-05T01:38:49,824 [INFO ] pool-3-thread-1 TS_METRICS - MemoryUtilization.Percent:45.5|#Level:Host|#hostname:pupu-Stvullinc,timestamp:1746391129
2025-05-05T01:39:49,809 [INFO ] pool-3-thread-1 TS_METRICS - CPUUtilization.Percent:33.3|#Level:Host|#hostname:pupu-Stvullinc,timestamp:1746391189
2025-05-05T01:39:49,809 [INFO ] pool-3-thread-1 TS_METRICS - DiskAvailable.Gigabytes:75.43494415283203|#Level:Host|#hostname:pupu-Stvullinc,timestamp:1746391189
2025-05-05T01:39:49,809 [INFO ] pool-3-thread-1 TS_METRICS - DiskUsage.Gigabytes:62.74690246582031|#Level:Host|#hostname:pupu-Stvullinc,timestamp:1746391189
2025-05-05T01:39:49,810 [INFO ] pool-3-thread-1 TS_METRICS - DiskUtilization.Percent:45.4|#Level:Host|#hostname:pupu-Stvullinc,timestamp:1746391189
2025-05-05T01:39:49,810 [INFO ] pool-3-thread-1 TS_METRICS - GPUMemoryUtilization.Percent:6.93359375|#Level:Host,DeviceId:0|#hostname:pupu-Stvullinc,timestamp:1746391189
2025-05-05T01:39:49,810 [INFO ] pool-3-thread-1 TS_METRICS - GPUMemoryUsed.Megabytes:568.0|#Level:Host,DeviceId:0|#hostname:pupu-Stvullinc,timestamp:1746391189
2025-05-05T01:39:49,810 [INFO ] pool-3-thread-1 TS_METRICS - GPUUtilization.Percent:1.0|#Level:Host,DeviceId:0|#hostname:pupu-Stvullinc,timestamp:1746391189
2025-05-05T01:39:49,811 [INFO ] pool-3-thread-1 TS_METRICS - MemoryAvailable.Megabytes:8674.74609375|#Level:Host|#hostname:pupu-Stvullinc,timestamp:1746391189
2025-05-05T01:39:49,811 [INFO ] pool-3-thread-1 TS_METRICS - MemoryUsed.Megabytes:6853.609375|#Level:Host|#hostname:pupu-Stvullinc,timestamp:1746391189
2025-05-05T01:39:49,811 [INFO ] pool-3-thread-1 TS_METRICS - MemoryUtilization.Percent:45.6|#Level:Host|#hostname:pupu-Stvullinc,timestamp:1746391189
2025-05-05T01:40:49,912 [INFO ] pool-3-thread-1 TS_METRICS - CPUUtilization.Percent:80.0|#Level:Host|#hostname:pupu-Stvullinc,timestamp:1746391249
2025-05-05T01:40:49,912 [INFO ] pool-3-thread-1 TS_METRICS - DiskAvailable.Gigabytes:75.43487167358398|#Level:Host|#hostname:pupu-Stvullinc,timestamp:1746391249
2025-05-05T01:40:49,912 [INFO ] pool-3-thread-1 TS_METRICS - DiskUsage.Gigabytes:62.74697494506836|#Level:Host|#hostname:pupu-Stvullinc,timestamp:1746391249
2025-05-05T01:40:49,913 [INFO ] pool-3-thread-1 TS_METRICS - DiskUtilization.Percent:45.4|#Level:Host|#hostname:pupu-Stvullinc,timestamp:1746391249
2025-05-05T01:40:49,913 [INFO ] pool-3-thread-1 TS_METRICS - GPUMemoryUtilization.Percent:6.94580078125|#Level:Host,DeviceId:0|#hostname:pupu-Stvullinc,timestamp:1746391249
2025-05-05T01:40:49,913 [INFO ] pool-3-thread-1 TS_METRICS - GPUMemoryUsed.Megabytes:569.0|#Level:Host,DeviceId:0|#hostname:pupu-Stvullinc,timestamp:1746391249
2025-05-05T01:40:49,913 [INFO ] pool-3-thread-1 TS_METRICS - GPUUtilization.Percent:4.0|#Level:Host,DeviceId:0|#hostname:pupu-Stvullinc,timestamp:1746391249
2025-05-05T01:40:49,914 [INFO ] pool-3-thread-1 TS_METRICS - MemoryAvailable.Megabytes:8667.375|#Level:Host|#hostname:pupu-Stvullinc,timestamp:1746391249
2025-05-05T01:40:49,914 [INFO ] pool-3-thread-1 TS_METRICS - MemoryUsed.Megabytes:6860.98828125|#Level:Host|#hostname:pupu-Stvullinc,timestamp:1746391249
2025-05-05T01:40:49,914 [INFO ] pool-3-thread-1 TS_METRICS - MemoryUtilization.Percent:45.7|#Level:Host|#hostname:pupu-Stvullinc,timestamp:1746391249
2025-05-05T01:41:39,942 [INFO ] epollEventLoopGroup-3-5 TS_METRICS - ts_inference_requests_total.Count:1.0|#model_name:my_model,model_version:default|#hostname:pupu-Stvullinc,timestamp:1746391299
2025-05-05T01:41:39,943 [DEBUG] W-9000-my_model_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req.cmd PREDICT repeats 1 to backend at: 1746391299943
2025-05-05T01:41:39,943 [DEBUG] W-9000-my_model_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req.cmd PREDICT repeats 1 to backend at: 1746391299943
2025-05-05T01:41:39,944 [INFO ] W-9000-my_model_1.0 org.pytorch.serve.wlm.WorkerThread - Looping backend response at: 1746391299944
2025-05-05T01:41:39,944 [INFO ] W-9000-my_model_1.0 org.pytorch.serve.wlm.WorkerThread - Looping backend response at: 1746391299944
2025-05-05T01:41:39,945 [INFO ] W-9000-my_model_1.0-stdout MODEL_LOG - Backend received inference at: 1746391299
2025-05-05T01:41:39,946 [INFO ] W-9000-my_model_1.0-stdout MODEL_LOG - Invoking custom service failed.
2025-05-05T01:41:39,946 [INFO ] W-9000-my_model_1.0-stdout MODEL_LOG - Traceback (most recent call last):
2025-05-05T01:41:39,946 [INFO ] W-9000-my_model_1.0-stdout MODEL_LOG -   File "/home/pupu/Desktop/project/.venv/lib/python3.12/site-packages/ts/service.py", line 135, in predict
2025-05-05T01:41:39,947 [INFO ] W-9000-my_model_1.0-stdout MODEL_LOG -     ret = self._entry_point(input_batch, self.context)
2025-05-05T01:41:39,947 [INFO ] W-9000-my_model_1.0 ACCESS_LOG - /127.0.0.1:42748 "POST /predictions/my_model HTTP/1.1" 503 5
2025-05-05T01:41:39,947 [INFO ] W-9000-my_model_1.0-stdout MODEL_LOG -           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2025-05-05T01:41:39,947 [INFO ] W-9000-my_model_1.0 TS_METRICS - Requests5XX.Count:1.0|#Level:Host|#hostname:pupu-Stvullinc,timestamp:1746391299
2025-05-05T01:41:39,947 [INFO ] W-9000-my_model_1.0-stdout MODEL_LOG -   File "/home/pupu/Desktop/project/.venv/lib/python3.12/site-packages/ts/torch_handler/base_handler.py", line 455, in handle
2025-05-05T01:41:39,947 [INFO ] W-9000-my_model_1.0-stdout MODEL_LOG -     data_preprocess = self.preprocess(data)
2025-05-05T01:41:39,947 [DEBUG] W-9000-my_model_1.0 org.pytorch.serve.job.RestJob - Waiting time ns: 114931, Inference time ns: 4044755
2025-05-05T01:41:39,947 [DEBUG] W-9000-my_model_1.0 org.pytorch.serve.job.RestJob - Waiting time ns: 114931, Inference time ns: 4044755
2025-05-05T01:41:39,947 [INFO ] W-9000-my_model_1.0-stdout MODEL_LOG -                       ^^^^^^^^^^^^^^^^^^^^^
2025-05-05T01:41:39,947 [INFO ] W-9000-my_model_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 2
2025-05-05T01:41:39,947 [INFO ] W-9000-my_model_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 2
2025-05-05T01:41:39,947 [INFO ] W-9000-my_model_1.0-stdout MODEL_LOG -   File "/tmp/models/e1259b592ce0457288ac566e5c725fe6/handler.py", line 54, in preprocess
2025-05-05T01:41:39,947 [INFO ] W-9000-my_model_1.0 TS_METRICS - WorkerThreadTime.Milliseconds:2.0|#Level:Host|#hostname:pupu-Stvullinc,timestamp:1746391299
2025-05-05T01:41:39,948 [INFO ] W-9000-my_model_1.0-stdout MODEL_LOG -     img = Image.open(io.BytesIO(img)).convert("RGB")
2025-05-05T01:41:39,948 [INFO ] W-9000-my_model_1.0-stdout MODEL_LOG -           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
2025-05-05T01:41:39,948 [INFO ] W-9000-my_model_1.0-stdout MODEL_LOG -   File "/home/pupu/Desktop/project/.venv/lib/python3.12/site-packages/PIL/Image.py", line 3572, in open
2025-05-05T01:41:39,948 [INFO ] W-9000-my_model_1.0-stdout MODEL_LOG -     raise UnidentifiedImageError(msg)
2025-05-05T01:41:39,948 [INFO ] W-9000-my_model_1.0-stdout MODEL_LOG - PIL.UnidentifiedImageError: cannot identify image file <_io.BytesIO object at 0x769c25f81da0>
2025-05-05T01:41:43,177 [INFO ] epollEventLoopGroup-3-6 TS_METRICS - ts_inference_requests_total.Count:1.0|#model_name:my_model,model_version:default|#hostname:pupu-Stvullinc,timestamp:1746391303
2025-05-05T01:41:43,178 [DEBUG] W-9000-my_model_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req.cmd PREDICT repeats 1 to backend at: 1746391303177
2025-05-05T01:41:43,178 [DEBUG] W-9000-my_model_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req.cmd PREDICT repeats 1 to backend at: 1746391303177
2025-05-05T01:41:43,178 [INFO ] W-9000-my_model_1.0 org.pytorch.serve.wlm.WorkerThread - Looping backend response at: 1746391303178
2025-05-05T01:41:43,178 [INFO ] W-9000-my_model_1.0 org.pytorch.serve.wlm.WorkerThread - Looping backend response at: 1746391303178
2025-05-05T01:41:43,179 [INFO ] W-9000-my_model_1.0-stdout MODEL_LOG - Backend received inference at: 1746391303
2025-05-05T01:41:43,179 [INFO ] W-9000-my_model_1.0-stdout MODEL_LOG - Invoking custom service failed.
2025-05-05T01:41:43,180 [INFO ] W-9000-my_model_1.0-stdout MODEL_LOG - Traceback (most recent call last):
2025-05-05T01:41:43,180 [INFO ] W-9000-my_model_1.0-stdout MODEL_LOG -   File "/home/pupu/Desktop/project/.venv/lib/python3.12/site-packages/ts/service.py", line 135, in predict
2025-05-05T01:41:43,180 [INFO ] W-9000-my_model_1.0-stdout MODEL_LOG -     ret = self._entry_point(input_batch, self.context)
2025-05-05T01:41:43,180 [INFO ] W-9000-my_model_1.0 ACCESS_LOG - /127.0.0.1:46582 "POST /predictions/my_model HTTP/1.1" 503 4
2025-05-05T01:41:43,180 [INFO ] W-9000-my_model_1.0-stdout MODEL_LOG -           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2025-05-05T01:41:43,180 [INFO ] W-9000-my_model_1.0 TS_METRICS - Requests5XX.Count:1.0|#Level:Host|#hostname:pupu-Stvullinc,timestamp:1746391303
2025-05-05T01:41:43,180 [INFO ] W-9000-my_model_1.0-stdout MODEL_LOG -   File "/home/pupu/Desktop/project/.venv/lib/python3.12/site-packages/ts/torch_handler/base_handler.py", line 455, in handle
2025-05-05T01:41:43,181 [DEBUG] W-9000-my_model_1.0 org.pytorch.serve.job.RestJob - Waiting time ns: 85664, Inference time ns: 3268536
2025-05-05T01:41:43,181 [DEBUG] W-9000-my_model_1.0 org.pytorch.serve.job.RestJob - Waiting time ns: 85664, Inference time ns: 3268536
2025-05-05T01:41:43,181 [INFO ] W-9000-my_model_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 2
2025-05-05T01:41:43,181 [INFO ] W-9000-my_model_1.0-stdout MODEL_LOG -     data_preprocess = self.preprocess(data)
2025-05-05T01:41:43,181 [INFO ] W-9000-my_model_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 2
2025-05-05T01:41:43,181 [INFO ] W-9000-my_model_1.0-stdout MODEL_LOG -                       ^^^^^^^^^^^^^^^^^^^^^
2025-05-05T01:41:43,181 [INFO ] W-9000-my_model_1.0 TS_METRICS - WorkerThreadTime.Milliseconds:2.0|#Level:Host|#hostname:pupu-Stvullinc,timestamp:1746391303
2025-05-05T01:41:43,181 [INFO ] W-9000-my_model_1.0-stdout MODEL_LOG -   File "/tmp/models/e1259b592ce0457288ac566e5c725fe6/handler.py", line 54, in preprocess
2025-05-05T01:41:43,181 [INFO ] W-9000-my_model_1.0-stdout MODEL_LOG -     img = Image.open(io.BytesIO(img)).convert("RGB")
2025-05-05T01:41:43,182 [INFO ] W-9000-my_model_1.0-stdout MODEL_LOG -           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
2025-05-05T01:41:43,182 [INFO ] W-9000-my_model_1.0-stdout MODEL_LOG -   File "/home/pupu/Desktop/project/.venv/lib/python3.12/site-packages/PIL/Image.py", line 3572, in open
2025-05-05T01:41:43,182 [INFO ] W-9000-my_model_1.0-stdout MODEL_LOG -     raise UnidentifiedImageError(msg)
2025-05-05T01:41:43,182 [INFO ] W-9000-my_model_1.0-stdout MODEL_LOG - PIL.UnidentifiedImageError: cannot identify image file <_io.BytesIO object at 0x769c25f81d00>
2025-05-05T01:41:49,812 [INFO ] pool-3-thread-1 TS_METRICS - CPUUtilization.Percent:0.0|#Level:Host|#hostname:pupu-Stvullinc,timestamp:1746391309
2025-05-05T01:41:49,813 [INFO ] pool-3-thread-1 TS_METRICS - DiskAvailable.Gigabytes:75.49604034423828|#Level:Host|#hostname:pupu-Stvullinc,timestamp:1746391309
2025-05-05T01:41:49,813 [INFO ] pool-3-thread-1 TS_METRICS - DiskUsage.Gigabytes:62.68580627441406|#Level:Host|#hostname:pupu-Stvullinc,timestamp:1746391309
2025-05-05T01:41:49,813 [INFO ] pool-3-thread-1 TS_METRICS - DiskUtilization.Percent:45.4|#Level:Host|#hostname:pupu-Stvullinc,timestamp:1746391309
2025-05-05T01:41:49,813 [INFO ] pool-3-thread-1 TS_METRICS - GPUMemoryUtilization.Percent:6.94580078125|#Level:Host,DeviceId:0|#hostname:pupu-Stvullinc,timestamp:1746391309
2025-05-05T01:41:49,813 [INFO ] pool-3-thread-1 TS_METRICS - GPUMemoryUsed.Megabytes:569.0|#Level:Host,DeviceId:0|#hostname:pupu-Stvullinc,timestamp:1746391309
2025-05-05T01:41:49,814 [INFO ] pool-3-thread-1 TS_METRICS - GPUUtilization.Percent:9.0|#Level:Host,DeviceId:0|#hostname:pupu-Stvullinc,timestamp:1746391309
2025-05-05T01:41:49,814 [INFO ] pool-3-thread-1 TS_METRICS - MemoryAvailable.Megabytes:8655.765625|#Level:Host|#hostname:pupu-Stvullinc,timestamp:1746391309
2025-05-05T01:41:49,814 [INFO ] pool-3-thread-1 TS_METRICS - MemoryUsed.Megabytes:6872.58984375|#Level:Host|#hostname:pupu-Stvullinc,timestamp:1746391309
2025-05-05T01:41:49,814 [INFO ] pool-3-thread-1 TS_METRICS - MemoryUtilization.Percent:45.8|#Level:Host|#hostname:pupu-Stvullinc,timestamp:1746391309
2025-05-05T01:42:13,827 [DEBUG] main org.pytorch.serve.util.ConfigManager - xpu-smi not available or failed: Cannot run program "xpu-smi": error=2, No such file or directory
2025-05-05T01:42:13,827 [DEBUG] main org.pytorch.serve.util.ConfigManager - xpu-smi not available or failed: Cannot run program "xpu-smi": error=2, No such file or directory
2025-05-05T01:42:13,833 [WARN ] main org.pytorch.serve.util.ConfigManager - Your torchserve instance can access any URL to load models. When deploying to production, make sure to limit the set of allowed_urls in config.properties
2025-05-05T01:42:13,833 [WARN ] main org.pytorch.serve.util.ConfigManager - Your torchserve instance can access any URL to load models. When deploying to production, make sure to limit the set of allowed_urls in config.properties
2025-05-05T01:42:13,854 [INFO ] main org.pytorch.serve.servingsdk.impl.PluginsManager - Initializing plugins manager...
2025-05-05T01:42:13,854 [INFO ] main org.pytorch.serve.servingsdk.impl.PluginsManager - Initializing plugins manager...
2025-05-05T01:42:13,915 [INFO ] main org.pytorch.serve.metrics.configuration.MetricConfiguration - Successfully loaded metrics configuration from /home/pupu/Desktop/project/.venv/lib/python3.12/site-packages/ts/configs/metrics.yaml
2025-05-05T01:42:13,915 [INFO ] main org.pytorch.serve.metrics.configuration.MetricConfiguration - Successfully loaded metrics configuration from /home/pupu/Desktop/project/.venv/lib/python3.12/site-packages/ts/configs/metrics.yaml
2025-05-05T01:42:14,000 [INFO ] main org.pytorch.serve.ModelServer - 
Torchserve version: 0.12.0
TS Home: /home/pupu/Desktop/project/.venv/lib/python3.12/site-packages
Current directory: /home/pupu/Desktop/project
Temp directory: /tmp
Metrics config path: /home/pupu/Desktop/project/.venv/lib/python3.12/site-packages/ts/configs/metrics.yaml
Number of GPUs: 1
Number of CPUs: 8
Max heap size: 3990 M
Python executable: /home/pupu/Desktop/project/.venv/bin/python
Config file: N/A
Inference address: http://127.0.0.1:8080
Management address: http://127.0.0.1:8081
Metrics address: http://127.0.0.1:8082
Model Store: /home/pupu/Desktop/project/there_is_our_server
Initial Models: my_model=my_model.mar
Log dir: /home/pupu/Desktop/project/logs
Metrics dir: /home/pupu/Desktop/project/logs
Netty threads: 0
Netty client threads: 0
Default workers per model: 1
Blacklist Regex: N/A
Maximum Response Size: 6553500
Maximum Request Size: 6553500
Limit Maximum Image Pixels: true
Prefer direct buffer: false
Allowed Urls: [file://.*|http(s)?://.*]
Custom python dependency for model allowed: false
Enable metrics API: true
Metrics mode: LOG
Disable system metrics: false
Workflow Store: /home/pupu/Desktop/project/there_is_our_server
CPP log config: N/A
Model config: N/A
System metrics command: default
Model API enabled: false
2025-05-05T01:42:14,000 [INFO ] main org.pytorch.serve.ModelServer - 
Torchserve version: 0.12.0
TS Home: /home/pupu/Desktop/project/.venv/lib/python3.12/site-packages
Current directory: /home/pupu/Desktop/project
Temp directory: /tmp
Metrics config path: /home/pupu/Desktop/project/.venv/lib/python3.12/site-packages/ts/configs/metrics.yaml
Number of GPUs: 1
Number of CPUs: 8
Max heap size: 3990 M
Python executable: /home/pupu/Desktop/project/.venv/bin/python
Config file: N/A
Inference address: http://127.0.0.1:8080
Management address: http://127.0.0.1:8081
Metrics address: http://127.0.0.1:8082
Model Store: /home/pupu/Desktop/project/there_is_our_server
Initial Models: my_model=my_model.mar
Log dir: /home/pupu/Desktop/project/logs
Metrics dir: /home/pupu/Desktop/project/logs
Netty threads: 0
Netty client threads: 0
Default workers per model: 1
Blacklist Regex: N/A
Maximum Response Size: 6553500
Maximum Request Size: 6553500
Limit Maximum Image Pixels: true
Prefer direct buffer: false
Allowed Urls: [file://.*|http(s)?://.*]
Custom python dependency for model allowed: false
Enable metrics API: true
Metrics mode: LOG
Disable system metrics: false
Workflow Store: /home/pupu/Desktop/project/there_is_our_server
CPP log config: N/A
Model config: N/A
System metrics command: default
Model API enabled: false
2025-05-05T01:42:14,009 [INFO ] main org.pytorch.serve.servingsdk.impl.PluginsManager -  Loading snapshot serializer plugin...
2025-05-05T01:42:14,009 [INFO ] main org.pytorch.serve.servingsdk.impl.PluginsManager -  Loading snapshot serializer plugin...
2025-05-05T01:42:14,030 [INFO ] main org.pytorch.serve.ModelServer - Loading initial models: my_model.mar
2025-05-05T01:42:14,030 [INFO ] main org.pytorch.serve.ModelServer - Loading initial models: my_model.mar
2025-05-05T01:42:15,246 [DEBUG] main org.pytorch.serve.wlm.ModelVersionedRefs - Adding new version 1.0 for model my_model
2025-05-05T01:42:15,246 [DEBUG] main org.pytorch.serve.wlm.ModelVersionedRefs - Adding new version 1.0 for model my_model
2025-05-05T01:42:15,247 [DEBUG] main org.pytorch.serve.wlm.ModelVersionedRefs - Setting default version to 1.0 for model my_model
2025-05-05T01:42:15,247 [DEBUG] main org.pytorch.serve.wlm.ModelVersionedRefs - Setting default version to 1.0 for model my_model
2025-05-05T01:42:15,247 [INFO ] main org.pytorch.serve.wlm.ModelManager - Model my_model loaded.
2025-05-05T01:42:15,247 [INFO ] main org.pytorch.serve.wlm.ModelManager - Model my_model loaded.
2025-05-05T01:42:15,247 [DEBUG] main org.pytorch.serve.wlm.ModelManager - updateModel: my_model, count: 1
2025-05-05T01:42:15,247 [DEBUG] main org.pytorch.serve.wlm.ModelManager - updateModel: my_model, count: 1
2025-05-05T01:42:15,255 [DEBUG] W-9000-my_model_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/home/pupu/Desktop/project/.venv/bin/python, /home/pupu/Desktop/project/.venv/lib/python3.12/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9000, --metrics-config, /home/pupu/Desktop/project/.venv/lib/python3.12/site-packages/ts/configs/metrics.yaml]
2025-05-05T01:42:15,255 [DEBUG] W-9000-my_model_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/home/pupu/Desktop/project/.venv/bin/python, /home/pupu/Desktop/project/.venv/lib/python3.12/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9000, --metrics-config, /home/pupu/Desktop/project/.venv/lib/python3.12/site-packages/ts/configs/metrics.yaml]
2025-05-05T01:42:15,256 [INFO ] main org.pytorch.serve.ModelServer - Initialize Inference server with: EpollServerSocketChannel.
2025-05-05T01:42:15,256 [INFO ] main org.pytorch.serve.ModelServer - Initialize Inference server with: EpollServerSocketChannel.
2025-05-05T01:42:15,313 [INFO ] main org.pytorch.serve.ModelServer - Inference API bind to: http://127.0.0.1:8080
2025-05-05T01:42:15,313 [INFO ] main org.pytorch.serve.ModelServer - Inference API bind to: http://127.0.0.1:8080
2025-05-05T01:42:15,313 [INFO ] main org.pytorch.serve.ModelServer - Initialize Management server with: EpollServerSocketChannel.
2025-05-05T01:42:15,313 [INFO ] main org.pytorch.serve.ModelServer - Initialize Management server with: EpollServerSocketChannel.
2025-05-05T01:42:15,314 [INFO ] main org.pytorch.serve.ModelServer - Management API bind to: http://127.0.0.1:8081
2025-05-05T01:42:15,314 [INFO ] main org.pytorch.serve.ModelServer - Management API bind to: http://127.0.0.1:8081
2025-05-05T01:42:15,314 [INFO ] main org.pytorch.serve.ModelServer - Initialize Metrics server with: EpollServerSocketChannel.
2025-05-05T01:42:15,314 [INFO ] main org.pytorch.serve.ModelServer - Initialize Metrics server with: EpollServerSocketChannel.
2025-05-05T01:42:15,315 [INFO ] main org.pytorch.serve.ModelServer - Metrics API bind to: http://127.0.0.1:8082
2025-05-05T01:42:15,315 [INFO ] main org.pytorch.serve.ModelServer - Metrics API bind to: http://127.0.0.1:8082
2025-05-05T01:42:15,516 [WARN ] pool-3-thread-1 org.pytorch.serve.metrics.MetricCollector - worker pid is not available yet.
2025-05-05T01:42:15,516 [WARN ] pool-3-thread-1 org.pytorch.serve.metrics.MetricCollector - worker pid is not available yet.
2025-05-05T01:42:16,073 [INFO ] pool-3-thread-1 TS_METRICS - CPUUtilization.Percent:50.0|#Level:Host|#hostname:pupu-Stvullinc,timestamp:1746391336
2025-05-05T01:42:16,074 [INFO ] pool-3-thread-1 TS_METRICS - DiskAvailable.Gigabytes:75.43469619750977|#Level:Host|#hostname:pupu-Stvullinc,timestamp:1746391336
2025-05-05T01:42:16,074 [INFO ] pool-3-thread-1 TS_METRICS - DiskUsage.Gigabytes:62.74715042114258|#Level:Host|#hostname:pupu-Stvullinc,timestamp:1746391336
2025-05-05T01:42:16,074 [INFO ] pool-3-thread-1 TS_METRICS - DiskUtilization.Percent:45.4|#Level:Host|#hostname:pupu-Stvullinc,timestamp:1746391336
2025-05-05T01:42:16,074 [INFO ] pool-3-thread-1 TS_METRICS - GPUMemoryUtilization.Percent:3.662109375|#Level:Host,DeviceId:0|#hostname:pupu-Stvullinc,timestamp:1746391336
2025-05-05T01:42:16,075 [INFO ] pool-3-thread-1 TS_METRICS - GPUMemoryUsed.Megabytes:300.0|#Level:Host,DeviceId:0|#hostname:pupu-Stvullinc,timestamp:1746391336
2025-05-05T01:42:16,075 [INFO ] pool-3-thread-1 TS_METRICS - GPUUtilization.Percent:3.0|#Level:Host,DeviceId:0|#hostname:pupu-Stvullinc,timestamp:1746391336
2025-05-05T01:42:16,075 [INFO ] pool-3-thread-1 TS_METRICS - MemoryAvailable.Megabytes:9059.796875|#Level:Host|#hostname:pupu-Stvullinc,timestamp:1746391336
2025-05-05T01:42:16,076 [INFO ] pool-3-thread-1 TS_METRICS - MemoryUsed.Megabytes:6478.56640625|#Level:Host|#hostname:pupu-Stvullinc,timestamp:1746391336
2025-05-05T01:42:16,076 [INFO ] pool-3-thread-1 TS_METRICS - MemoryUtilization.Percent:43.2|#Level:Host|#hostname:pupu-Stvullinc,timestamp:1746391336
2025-05-05T01:42:17,138 [INFO ] W-9000-my_model_1.0-stdout MODEL_LOG - s_name_part0=/tmp/.ts.sock, s_name_part1=9000, pid=12611
2025-05-05T01:42:17,139 [INFO ] W-9000-my_model_1.0-stdout MODEL_LOG - Listening on port: /tmp/.ts.sock.9000
2025-05-05T01:42:17,146 [INFO ] W-9000-my_model_1.0-stdout MODEL_LOG - Successfully loaded /home/pupu/Desktop/project/.venv/lib/python3.12/site-packages/ts/configs/metrics.yaml.
2025-05-05T01:42:17,146 [INFO ] W-9000-my_model_1.0-stdout MODEL_LOG - [PID]12611
2025-05-05T01:42:17,146 [INFO ] W-9000-my_model_1.0-stdout MODEL_LOG - Torch worker started.
2025-05-05T01:42:17,147 [INFO ] W-9000-my_model_1.0-stdout MODEL_LOG - Python runtime: 3.12.3
2025-05-05T01:42:17,147 [DEBUG] W-9000-my_model_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-my_model_1.0 State change null -> WORKER_STARTED
2025-05-05T01:42:17,147 [DEBUG] W-9000-my_model_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-my_model_1.0 State change null -> WORKER_STARTED
2025-05-05T01:42:17,154 [INFO ] W-9000-my_model_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9000
2025-05-05T01:42:17,154 [INFO ] W-9000-my_model_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9000
2025-05-05T01:42:17,163 [INFO ] W-9000-my_model_1.0-stdout MODEL_LOG - Connection accepted: /tmp/.ts.sock.9000.
2025-05-05T01:42:17,167 [DEBUG] W-9000-my_model_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req.cmd LOAD repeats 1 to backend at: 1746391337167
2025-05-05T01:42:17,167 [DEBUG] W-9000-my_model_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req.cmd LOAD repeats 1 to backend at: 1746391337167
2025-05-05T01:42:17,170 [INFO ] W-9000-my_model_1.0 org.pytorch.serve.wlm.WorkerThread - Looping backend response at: 1746391337170
2025-05-05T01:42:17,170 [INFO ] W-9000-my_model_1.0 org.pytorch.serve.wlm.WorkerThread - Looping backend response at: 1746391337170
2025-05-05T01:42:17,201 [INFO ] W-9000-my_model_1.0-stdout MODEL_LOG - model_name: my_model, batchSize: 1
2025-05-05T01:42:18,523 [INFO ] W-9000-my_model_1.0-stdout MODEL_LOG - OpenVINO is not enabled
2025-05-05T01:42:18,523 [INFO ] W-9000-my_model_1.0-stdout MODEL_LOG - proceeding without onnxruntime
2025-05-05T01:42:18,523 [INFO ] W-9000-my_model_1.0-stdout MODEL_LOG - Torch TensorRT not enabled
2025-05-05T01:42:19,368 [INFO ] W-9000-my_model_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 2198
2025-05-05T01:42:19,368 [INFO ] W-9000-my_model_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 2198
2025-05-05T01:42:19,369 [DEBUG] W-9000-my_model_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-my_model_1.0 State change WORKER_STARTED -> WORKER_MODEL_LOADED
2025-05-05T01:42:19,369 [DEBUG] W-9000-my_model_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-my_model_1.0 State change WORKER_STARTED -> WORKER_MODEL_LOADED
2025-05-05T01:42:19,370 [INFO ] W-9000-my_model_1.0 TS_METRICS - WorkerLoadTime.Milliseconds:4118.0|#WorkerName:W-9000-my_model_1.0,Level:Host|#hostname:pupu-Stvullinc,timestamp:1746391339
2025-05-05T01:42:19,371 [INFO ] W-9000-my_model_1.0 TS_METRICS - WorkerThreadTime.Milliseconds:6.0|#Level:Host|#hostname:pupu-Stvullinc,timestamp:1746391339
2025-05-05T01:42:54,611 [INFO ] epollEventLoopGroup-3-1 TS_METRICS - ts_inference_requests_total.Count:1.0|#model_name:my_model,model_version:default|#hostname:pupu-Stvullinc,timestamp:1746391374
2025-05-05T01:42:54,613 [DEBUG] W-9000-my_model_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req.cmd PREDICT repeats 1 to backend at: 1746391374613
2025-05-05T01:42:54,613 [DEBUG] W-9000-my_model_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req.cmd PREDICT repeats 1 to backend at: 1746391374613
2025-05-05T01:42:54,613 [INFO ] W-9000-my_model_1.0 org.pytorch.serve.wlm.WorkerThread - Looping backend response at: 1746391374613
2025-05-05T01:42:54,613 [INFO ] W-9000-my_model_1.0 org.pytorch.serve.wlm.WorkerThread - Looping backend response at: 1746391374613
2025-05-05T01:42:54,615 [INFO ] W-9000-my_model_1.0-stdout MODEL_LOG - Backend received inference at: 1746391374
2025-05-05T01:42:56,733 [INFO ] W-9000-my_model_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - result=[METRICS]HandlerTime.Milliseconds:2118.08|#ModelName:my_model,Level:Model|#type:GAUGE|#hostname:pupu-Stvullinc,1746391376,761da876-1a6b-45cd-9842-9906dc3b7d2a, pattern=[METRICS]
2025-05-05T01:42:56,733 [INFO ] W-9000-my_model_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - result=[METRICS]HandlerTime.Milliseconds:2118.08|#ModelName:my_model,Level:Model|#type:GAUGE|#hostname:pupu-Stvullinc,1746391376,761da876-1a6b-45cd-9842-9906dc3b7d2a, pattern=[METRICS]
2025-05-05T01:42:56,735 [INFO ] W-9000-my_model_1.0 org.pytorch.serve.wlm.BatchAggregator - Sending response for jobId 761da876-1a6b-45cd-9842-9906dc3b7d2a
2025-05-05T01:42:56,735 [INFO ] W-9000-my_model_1.0 org.pytorch.serve.wlm.BatchAggregator - Sending response for jobId 761da876-1a6b-45cd-9842-9906dc3b7d2a
2025-05-05T01:42:56,736 [INFO ] W-9000-my_model_1.0 ACCESS_LOG - /127.0.0.1:35422 "POST /predictions/my_model HTTP/1.1" 200 2127
2025-05-05T01:42:56,737 [INFO ] W-9000-my_model_1.0 TS_METRICS - Requests2XX.Count:1.0|#Level:Host|#hostname:pupu-Stvullinc,timestamp:1746391376
2025-05-05T01:42:56,738 [INFO ] W-9000-my_model_1.0 TS_METRICS - ts_inference_latency_microseconds.Microseconds:2122420.664|#model_name:my_model,model_version:default|#hostname:pupu-Stvullinc,timestamp:1746391376
2025-05-05T01:42:56,738 [INFO ] W-9000-my_model_1.0 TS_METRICS - ts_queue_latency_microseconds.Microseconds:158.136|#model_name:my_model,model_version:default|#hostname:pupu-Stvullinc,timestamp:1746391376
2025-05-05T01:42:56,738 [DEBUG] W-9000-my_model_1.0 org.pytorch.serve.job.RestJob - Waiting time ns: 158136, Backend time ns: 2125250853
2025-05-05T01:42:56,738 [DEBUG] W-9000-my_model_1.0 org.pytorch.serve.job.RestJob - Waiting time ns: 158136, Backend time ns: 2125250853
2025-05-05T01:42:56,738 [INFO ] W-9000-my_model_1.0 TS_METRICS - QueueTime.Milliseconds:0.0|#Level:Host|#hostname:pupu-Stvullinc,timestamp:1746391376
2025-05-05T01:42:56,738 [INFO ] W-9000-my_model_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 2120
2025-05-05T01:42:56,738 [INFO ] W-9000-my_model_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 2120
2025-05-05T01:42:56,739 [INFO ] W-9000-my_model_1.0 TS_METRICS - WorkerThreadTime.Milliseconds:5.0|#Level:Host|#hostname:pupu-Stvullinc,timestamp:1746391376
2025-05-05T01:42:56,734 [INFO ] W-9000-my_model_1.0-stdout MODEL_METRICS - HandlerTime.ms:2118.08|#ModelName:my_model,Level:Model|#hostname:pupu-Stvullinc,requestID:761da876-1a6b-45cd-9842-9906dc3b7d2a,timestamp:1746391376
2025-05-05T01:42:56,741 [INFO ] W-9000-my_model_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - result=[METRICS]PredictionTime.Milliseconds:2118.46|#ModelName:my_model,Level:Model|#type:GAUGE|#hostname:pupu-Stvullinc,1746391376,761da876-1a6b-45cd-9842-9906dc3b7d2a, pattern=[METRICS]
2025-05-05T01:42:56,741 [INFO ] W-9000-my_model_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - result=[METRICS]PredictionTime.Milliseconds:2118.46|#ModelName:my_model,Level:Model|#type:GAUGE|#hostname:pupu-Stvullinc,1746391376,761da876-1a6b-45cd-9842-9906dc3b7d2a, pattern=[METRICS]
2025-05-05T01:42:56,741 [INFO ] W-9000-my_model_1.0-stdout MODEL_METRICS - PredictionTime.ms:2118.46|#ModelName:my_model,Level:Model|#hostname:pupu-Stvullinc,requestID:761da876-1a6b-45cd-9842-9906dc3b7d2a,timestamp:1746391376
2025-05-05T01:43:16,122 [INFO ] pool-3-thread-1 TS_METRICS - CPUUtilization.Percent:25.0|#Level:Host|#hostname:pupu-Stvullinc,timestamp:1746391396
2025-05-05T01:43:16,122 [INFO ] pool-3-thread-1 TS_METRICS - DiskAvailable.Gigabytes:75.4344711303711|#Level:Host|#hostname:pupu-Stvullinc,timestamp:1746391396
2025-05-05T01:43:16,123 [INFO ] pool-3-thread-1 TS_METRICS - DiskUsage.Gigabytes:62.74737548828125|#Level:Host|#hostname:pupu-Stvullinc,timestamp:1746391396
2025-05-05T01:43:16,123 [INFO ] pool-3-thread-1 TS_METRICS - DiskUtilization.Percent:45.4|#Level:Host|#hostname:pupu-Stvullinc,timestamp:1746391396
2025-05-05T01:43:16,123 [INFO ] pool-3-thread-1 TS_METRICS - GPUMemoryUtilization.Percent:7.40966796875|#Level:Host,DeviceId:0|#hostname:pupu-Stvullinc,timestamp:1746391396
2025-05-05T01:43:16,123 [INFO ] pool-3-thread-1 TS_METRICS - GPUMemoryUsed.Megabytes:607.0|#Level:Host,DeviceId:0|#hostname:pupu-Stvullinc,timestamp:1746391396
2025-05-05T01:43:16,123 [INFO ] pool-3-thread-1 TS_METRICS - GPUUtilization.Percent:13.0|#Level:Host,DeviceId:0|#hostname:pupu-Stvullinc,timestamp:1746391396
2025-05-05T01:43:16,124 [INFO ] pool-3-thread-1 TS_METRICS - MemoryAvailable.Megabytes:8749.39453125|#Level:Host|#hostname:pupu-Stvullinc,timestamp:1746391396
2025-05-05T01:43:16,124 [INFO ] pool-3-thread-1 TS_METRICS - MemoryUsed.Megabytes:6769.6875|#Level:Host|#hostname:pupu-Stvullinc,timestamp:1746391396
2025-05-05T01:43:16,124 [INFO ] pool-3-thread-1 TS_METRICS - MemoryUtilization.Percent:45.2|#Level:Host|#hostname:pupu-Stvullinc,timestamp:1746391396
2025-05-05T01:44:16,045 [INFO ] pool-3-thread-1 TS_METRICS - CPUUtilization.Percent:0.0|#Level:Host|#hostname:pupu-Stvullinc,timestamp:1746391456
2025-05-05T01:44:16,045 [INFO ] pool-3-thread-1 TS_METRICS - DiskAvailable.Gigabytes:75.43424224853516|#Level:Host|#hostname:pupu-Stvullinc,timestamp:1746391456
2025-05-05T01:44:16,046 [INFO ] pool-3-thread-1 TS_METRICS - DiskUsage.Gigabytes:62.74760437011719|#Level:Host|#hostname:pupu-Stvullinc,timestamp:1746391456
2025-05-05T01:44:16,046 [INFO ] pool-3-thread-1 TS_METRICS - DiskUtilization.Percent:45.4|#Level:Host|#hostname:pupu-Stvullinc,timestamp:1746391456
2025-05-05T01:44:16,046 [INFO ] pool-3-thread-1 TS_METRICS - GPUMemoryUtilization.Percent:8.447265625|#Level:Host,DeviceId:0|#hostname:pupu-Stvullinc,timestamp:1746391456
2025-05-05T01:44:16,047 [INFO ] pool-3-thread-1 TS_METRICS - GPUMemoryUsed.Megabytes:692.0|#Level:Host,DeviceId:0|#hostname:pupu-Stvullinc,timestamp:1746391456
2025-05-05T01:44:16,047 [INFO ] pool-3-thread-1 TS_METRICS - GPUUtilization.Percent:36.0|#Level:Host,DeviceId:0|#hostname:pupu-Stvullinc,timestamp:1746391456
2025-05-05T01:44:16,047 [INFO ] pool-3-thread-1 TS_METRICS - MemoryAvailable.Megabytes:8718.4921875|#Level:Host|#hostname:pupu-Stvullinc,timestamp:1746391456
2025-05-05T01:44:16,048 [INFO ] pool-3-thread-1 TS_METRICS - MemoryUsed.Megabytes:6788.5546875|#Level:Host|#hostname:pupu-Stvullinc,timestamp:1746391456
2025-05-05T01:44:16,048 [INFO ] pool-3-thread-1 TS_METRICS - MemoryUtilization.Percent:45.4|#Level:Host|#hostname:pupu-Stvullinc,timestamp:1746391456
2025-05-05T01:45:16,020 [INFO ] pool-3-thread-1 TS_METRICS - CPUUtilization.Percent:0.0|#Level:Host|#hostname:pupu-Stvullinc,timestamp:1746391516
2025-05-05T01:45:16,021 [INFO ] pool-3-thread-1 TS_METRICS - DiskAvailable.Gigabytes:75.43415832519531|#Level:Host|#hostname:pupu-Stvullinc,timestamp:1746391516
2025-05-05T01:45:16,021 [INFO ] pool-3-thread-1 TS_METRICS - DiskUsage.Gigabytes:62.74768829345703|#Level:Host|#hostname:pupu-Stvullinc,timestamp:1746391516
2025-05-05T01:45:16,021 [INFO ] pool-3-thread-1 TS_METRICS - DiskUtilization.Percent:45.4|#Level:Host|#hostname:pupu-Stvullinc,timestamp:1746391516
2025-05-05T01:45:16,022 [INFO ] pool-3-thread-1 TS_METRICS - GPUMemoryUtilization.Percent:8.21533203125|#Level:Host,DeviceId:0|#hostname:pupu-Stvullinc,timestamp:1746391516
2025-05-05T01:45:16,022 [INFO ] pool-3-thread-1 TS_METRICS - GPUMemoryUsed.Megabytes:673.0|#Level:Host,DeviceId:0|#hostname:pupu-Stvullinc,timestamp:1746391516
2025-05-05T01:45:16,022 [INFO ] pool-3-thread-1 TS_METRICS - GPUUtilization.Percent:1.0|#Level:Host,DeviceId:0|#hostname:pupu-Stvullinc,timestamp:1746391516
2025-05-05T01:45:16,023 [INFO ] pool-3-thread-1 TS_METRICS - MemoryAvailable.Megabytes:8719.72265625|#Level:Host|#hostname:pupu-Stvullinc,timestamp:1746391516
2025-05-05T01:45:16,023 [INFO ] pool-3-thread-1 TS_METRICS - MemoryUsed.Megabytes:6800.38671875|#Level:Host|#hostname:pupu-Stvullinc,timestamp:1746391516
2025-05-05T01:45:16,023 [INFO ] pool-3-thread-1 TS_METRICS - MemoryUtilization.Percent:45.4|#Level:Host|#hostname:pupu-Stvullinc,timestamp:1746391516
2025-05-05T01:46:16,075 [INFO ] pool-3-thread-1 TS_METRICS - CPUUtilization.Percent:33.3|#Level:Host|#hostname:pupu-Stvullinc,timestamp:1746391576
2025-05-05T01:46:16,075 [INFO ] pool-3-thread-1 TS_METRICS - DiskAvailable.Gigabytes:75.43399429321289|#Level:Host|#hostname:pupu-Stvullinc,timestamp:1746391576
2025-05-05T01:46:16,076 [INFO ] pool-3-thread-1 TS_METRICS - DiskUsage.Gigabytes:62.74785232543945|#Level:Host|#hostname:pupu-Stvullinc,timestamp:1746391576
2025-05-05T01:46:16,076 [INFO ] pool-3-thread-1 TS_METRICS - DiskUtilization.Percent:45.4|#Level:Host|#hostname:pupu-Stvullinc,timestamp:1746391576
2025-05-05T01:46:16,077 [INFO ] pool-3-thread-1 TS_METRICS - GPUMemoryUtilization.Percent:8.056640625|#Level:Host,DeviceId:0|#hostname:pupu-Stvullinc,timestamp:1746391576
2025-05-05T01:46:16,077 [INFO ] pool-3-thread-1 TS_METRICS - GPUMemoryUsed.Megabytes:660.0|#Level:Host,DeviceId:0|#hostname:pupu-Stvullinc,timestamp:1746391576
2025-05-05T01:46:16,077 [INFO ] pool-3-thread-1 TS_METRICS - GPUUtilization.Percent:39.0|#Level:Host,DeviceId:0|#hostname:pupu-Stvullinc,timestamp:1746391576
2025-05-05T01:46:16,078 [INFO ] pool-3-thread-1 TS_METRICS - MemoryAvailable.Megabytes:8725.81640625|#Level:Host|#hostname:pupu-Stvullinc,timestamp:1746391576
2025-05-05T01:46:16,078 [INFO ] pool-3-thread-1 TS_METRICS - MemoryUsed.Megabytes:6789.9765625|#Level:Host|#hostname:pupu-Stvullinc,timestamp:1746391576
2025-05-05T01:46:16,078 [INFO ] pool-3-thread-1 TS_METRICS - MemoryUtilization.Percent:45.3|#Level:Host|#hostname:pupu-Stvullinc,timestamp:1746391576
2025-05-05T01:47:16,018 [INFO ] pool-3-thread-1 TS_METRICS - CPUUtilization.Percent:0.0|#Level:Host|#hostname:pupu-Stvullinc,timestamp:1746391636
2025-05-05T01:47:16,018 [INFO ] pool-3-thread-1 TS_METRICS - DiskAvailable.Gigabytes:75.43389892578125|#Level:Host|#hostname:pupu-Stvullinc,timestamp:1746391636
2025-05-05T01:47:16,019 [INFO ] pool-3-thread-1 TS_METRICS - DiskUsage.Gigabytes:62.747947692871094|#Level:Host|#hostname:pupu-Stvullinc,timestamp:1746391636
2025-05-05T01:47:16,019 [INFO ] pool-3-thread-1 TS_METRICS - DiskUtilization.Percent:45.4|#Level:Host|#hostname:pupu-Stvullinc,timestamp:1746391636
2025-05-05T01:47:16,019 [INFO ] pool-3-thread-1 TS_METRICS - GPUMemoryUtilization.Percent:8.056640625|#Level:Host,DeviceId:0|#hostname:pupu-Stvullinc,timestamp:1746391636
2025-05-05T01:47:16,020 [INFO ] pool-3-thread-1 TS_METRICS - GPUMemoryUsed.Megabytes:660.0|#Level:Host,DeviceId:0|#hostname:pupu-Stvullinc,timestamp:1746391636
2025-05-05T01:47:16,020 [INFO ] pool-3-thread-1 TS_METRICS - GPUUtilization.Percent:14.0|#Level:Host,DeviceId:0|#hostname:pupu-Stvullinc,timestamp:1746391636
2025-05-05T01:47:16,020 [INFO ] pool-3-thread-1 TS_METRICS - MemoryAvailable.Megabytes:8690.7421875|#Level:Host|#hostname:pupu-Stvullinc,timestamp:1746391636
2025-05-05T01:47:16,020 [INFO ] pool-3-thread-1 TS_METRICS - MemoryUsed.Megabytes:6809.3046875|#Level:Host|#hostname:pupu-Stvullinc,timestamp:1746391636
2025-05-05T01:47:16,021 [INFO ] pool-3-thread-1 TS_METRICS - MemoryUtilization.Percent:45.5|#Level:Host|#hostname:pupu-Stvullinc,timestamp:1746391636
2025-05-05T01:48:16,011 [INFO ] pool-3-thread-1 TS_METRICS - CPUUtilization.Percent:0.0|#Level:Host|#hostname:pupu-Stvullinc,timestamp:1746391696
2025-05-05T01:48:16,011 [INFO ] pool-3-thread-1 TS_METRICS - DiskAvailable.Gigabytes:75.43144989013672|#Level:Host|#hostname:pupu-Stvullinc,timestamp:1746391696
2025-05-05T01:48:16,011 [INFO ] pool-3-thread-1 TS_METRICS - DiskUsage.Gigabytes:62.750396728515625|#Level:Host|#hostname:pupu-Stvullinc,timestamp:1746391696
2025-05-05T01:48:16,012 [INFO ] pool-3-thread-1 TS_METRICS - DiskUtilization.Percent:45.4|#Level:Host|#hostname:pupu-Stvullinc,timestamp:1746391696
2025-05-05T01:48:16,012 [INFO ] pool-3-thread-1 TS_METRICS - GPUMemoryUtilization.Percent:8.31298828125|#Level:Host,DeviceId:0|#hostname:pupu-Stvullinc,timestamp:1746391696
2025-05-05T01:48:16,012 [INFO ] pool-3-thread-1 TS_METRICS - GPUMemoryUsed.Megabytes:681.0|#Level:Host,DeviceId:0|#hostname:pupu-Stvullinc,timestamp:1746391696
2025-05-05T01:48:16,013 [INFO ] pool-3-thread-1 TS_METRICS - GPUUtilization.Percent:28.0|#Level:Host,DeviceId:0|#hostname:pupu-Stvullinc,timestamp:1746391696
2025-05-05T01:48:16,013 [INFO ] pool-3-thread-1 TS_METRICS - MemoryAvailable.Megabytes:8691.4453125|#Level:Host|#hostname:pupu-Stvullinc,timestamp:1746391696
2025-05-05T01:48:16,013 [INFO ] pool-3-thread-1 TS_METRICS - MemoryUsed.Megabytes:6824.578125|#Level:Host|#hostname:pupu-Stvullinc,timestamp:1746391696
2025-05-05T01:48:16,013 [INFO ] pool-3-thread-1 TS_METRICS - MemoryUtilization.Percent:45.5|#Level:Host|#hostname:pupu-Stvullinc,timestamp:1746391696
2025-05-05T02:12:48,984 [DEBUG] main org.pytorch.serve.util.ConfigManager - xpu-smi not available or failed: Cannot run program "xpu-smi": error=2, No such file or directory
2025-05-05T02:12:48,984 [DEBUG] main org.pytorch.serve.util.ConfigManager - xpu-smi not available or failed: Cannot run program "xpu-smi": error=2, No such file or directory
2025-05-05T02:12:48,989 [WARN ] main org.pytorch.serve.util.ConfigManager - Your torchserve instance can access any URL to load models. When deploying to production, make sure to limit the set of allowed_urls in config.properties
2025-05-05T02:12:48,989 [WARN ] main org.pytorch.serve.util.ConfigManager - Your torchserve instance can access any URL to load models. When deploying to production, make sure to limit the set of allowed_urls in config.properties
2025-05-05T02:12:49,011 [INFO ] main org.pytorch.serve.servingsdk.impl.PluginsManager - Initializing plugins manager...
2025-05-05T02:12:49,011 [INFO ] main org.pytorch.serve.servingsdk.impl.PluginsManager - Initializing plugins manager...
2025-05-05T02:12:49,077 [INFO ] main org.pytorch.serve.metrics.configuration.MetricConfiguration - Successfully loaded metrics configuration from /home/pupu/Desktop/project/.venv/lib/python3.12/site-packages/ts/configs/metrics.yaml
2025-05-05T02:12:49,077 [INFO ] main org.pytorch.serve.metrics.configuration.MetricConfiguration - Successfully loaded metrics configuration from /home/pupu/Desktop/project/.venv/lib/python3.12/site-packages/ts/configs/metrics.yaml
2025-05-05T02:12:49,157 [INFO ] main org.pytorch.serve.ModelServer - 
Torchserve version: 0.12.0
TS Home: /home/pupu/Desktop/project/.venv/lib/python3.12/site-packages
Current directory: /home/pupu/Desktop/project
Temp directory: /tmp
Metrics config path: /home/pupu/Desktop/project/.venv/lib/python3.12/site-packages/ts/configs/metrics.yaml
Number of GPUs: 1
Number of CPUs: 8
Max heap size: 3990 M
Python executable: /home/pupu/Desktop/project/.venv/bin/python
Config file: N/A
Inference address: http://127.0.0.1:8080
Management address: http://127.0.0.1:8081
Metrics address: http://127.0.0.1:8082
Model Store: /home/pupu/Desktop/project/there_is_our_server
Initial Models: my_model=my_model.mar
Log dir: /home/pupu/Desktop/project/logs
Metrics dir: /home/pupu/Desktop/project/logs
Netty threads: 0
Netty client threads: 0
Default workers per model: 1
Blacklist Regex: N/A
Maximum Response Size: 6553500
Maximum Request Size: 6553500
Limit Maximum Image Pixels: true
Prefer direct buffer: false
Allowed Urls: [file://.*|http(s)?://.*]
Custom python dependency for model allowed: false
Enable metrics API: true
Metrics mode: LOG
Disable system metrics: false
Workflow Store: /home/pupu/Desktop/project/there_is_our_server
CPP log config: N/A
Model config: N/A
System metrics command: default
Model API enabled: false
2025-05-05T02:12:49,157 [INFO ] main org.pytorch.serve.ModelServer - 
Torchserve version: 0.12.0
TS Home: /home/pupu/Desktop/project/.venv/lib/python3.12/site-packages
Current directory: /home/pupu/Desktop/project
Temp directory: /tmp
Metrics config path: /home/pupu/Desktop/project/.venv/lib/python3.12/site-packages/ts/configs/metrics.yaml
Number of GPUs: 1
Number of CPUs: 8
Max heap size: 3990 M
Python executable: /home/pupu/Desktop/project/.venv/bin/python
Config file: N/A
Inference address: http://127.0.0.1:8080
Management address: http://127.0.0.1:8081
Metrics address: http://127.0.0.1:8082
Model Store: /home/pupu/Desktop/project/there_is_our_server
Initial Models: my_model=my_model.mar
Log dir: /home/pupu/Desktop/project/logs
Metrics dir: /home/pupu/Desktop/project/logs
Netty threads: 0
Netty client threads: 0
Default workers per model: 1
Blacklist Regex: N/A
Maximum Response Size: 6553500
Maximum Request Size: 6553500
Limit Maximum Image Pixels: true
Prefer direct buffer: false
Allowed Urls: [file://.*|http(s)?://.*]
Custom python dependency for model allowed: false
Enable metrics API: true
Metrics mode: LOG
Disable system metrics: false
Workflow Store: /home/pupu/Desktop/project/there_is_our_server
CPP log config: N/A
Model config: N/A
System metrics command: default
Model API enabled: false
2025-05-05T02:12:49,166 [INFO ] main org.pytorch.serve.servingsdk.impl.PluginsManager -  Loading snapshot serializer plugin...
2025-05-05T02:12:49,166 [INFO ] main org.pytorch.serve.servingsdk.impl.PluginsManager -  Loading snapshot serializer plugin...
2025-05-05T02:12:49,188 [INFO ] main org.pytorch.serve.ModelServer - Loading initial models: my_model.mar
2025-05-05T02:12:49,188 [INFO ] main org.pytorch.serve.ModelServer - Loading initial models: my_model.mar
2025-05-05T02:12:50,543 [DEBUG] main org.pytorch.serve.wlm.ModelVersionedRefs - Adding new version 1.0 for model my_model
2025-05-05T02:12:50,543 [DEBUG] main org.pytorch.serve.wlm.ModelVersionedRefs - Adding new version 1.0 for model my_model
2025-05-05T02:12:50,543 [DEBUG] main org.pytorch.serve.wlm.ModelVersionedRefs - Setting default version to 1.0 for model my_model
2025-05-05T02:12:50,543 [DEBUG] main org.pytorch.serve.wlm.ModelVersionedRefs - Setting default version to 1.0 for model my_model
2025-05-05T02:12:50,543 [INFO ] main org.pytorch.serve.wlm.ModelManager - Model my_model loaded.
2025-05-05T02:12:50,543 [INFO ] main org.pytorch.serve.wlm.ModelManager - Model my_model loaded.
2025-05-05T02:12:50,543 [DEBUG] main org.pytorch.serve.wlm.ModelManager - updateModel: my_model, count: 1
2025-05-05T02:12:50,543 [DEBUG] main org.pytorch.serve.wlm.ModelManager - updateModel: my_model, count: 1
2025-05-05T02:12:50,550 [DEBUG] W-9000-my_model_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/home/pupu/Desktop/project/.venv/bin/python, /home/pupu/Desktop/project/.venv/lib/python3.12/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9000, --metrics-config, /home/pupu/Desktop/project/.venv/lib/python3.12/site-packages/ts/configs/metrics.yaml]
2025-05-05T02:12:50,550 [DEBUG] W-9000-my_model_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/home/pupu/Desktop/project/.venv/bin/python, /home/pupu/Desktop/project/.venv/lib/python3.12/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9000, --metrics-config, /home/pupu/Desktop/project/.venv/lib/python3.12/site-packages/ts/configs/metrics.yaml]
2025-05-05T02:12:50,551 [INFO ] main org.pytorch.serve.ModelServer - Initialize Inference server with: EpollServerSocketChannel.
2025-05-05T02:12:50,551 [INFO ] main org.pytorch.serve.ModelServer - Initialize Inference server with: EpollServerSocketChannel.
2025-05-05T02:12:50,607 [INFO ] main org.pytorch.serve.ModelServer - Inference API bind to: http://127.0.0.1:8080
2025-05-05T02:12:50,607 [INFO ] main org.pytorch.serve.ModelServer - Inference API bind to: http://127.0.0.1:8080
2025-05-05T02:12:50,607 [INFO ] main org.pytorch.serve.ModelServer - Initialize Management server with: EpollServerSocketChannel.
2025-05-05T02:12:50,607 [INFO ] main org.pytorch.serve.ModelServer - Initialize Management server with: EpollServerSocketChannel.
2025-05-05T02:12:50,608 [INFO ] main org.pytorch.serve.ModelServer - Management API bind to: http://127.0.0.1:8081
2025-05-05T02:12:50,608 [INFO ] main org.pytorch.serve.ModelServer - Management API bind to: http://127.0.0.1:8081
2025-05-05T02:12:50,608 [INFO ] main org.pytorch.serve.ModelServer - Initialize Metrics server with: EpollServerSocketChannel.
2025-05-05T02:12:50,608 [INFO ] main org.pytorch.serve.ModelServer - Initialize Metrics server with: EpollServerSocketChannel.
2025-05-05T02:12:50,609 [INFO ] main org.pytorch.serve.ModelServer - Metrics API bind to: http://127.0.0.1:8082
2025-05-05T02:12:50,609 [INFO ] main org.pytorch.serve.ModelServer - Metrics API bind to: http://127.0.0.1:8082
2025-05-05T02:12:50,806 [WARN ] pool-3-thread-1 org.pytorch.serve.metrics.MetricCollector - worker pid is not available yet.
2025-05-05T02:12:50,806 [WARN ] pool-3-thread-1 org.pytorch.serve.metrics.MetricCollector - worker pid is not available yet.
2025-05-05T02:12:51,858 [INFO ] pool-3-thread-1 TS_METRICS - CPUUtilization.Percent:0.0|#Level:Host|#hostname:pupu-Stvullinc,timestamp:1746393171
2025-05-05T02:12:51,870 [INFO ] pool-3-thread-1 TS_METRICS - DiskAvailable.Gigabytes:75.2695426940918|#Level:Host|#hostname:pupu-Stvullinc,timestamp:1746393171
2025-05-05T02:12:51,870 [INFO ] pool-3-thread-1 TS_METRICS - DiskUsage.Gigabytes:62.91230392456055|#Level:Host|#hostname:pupu-Stvullinc,timestamp:1746393171
2025-05-05T02:12:51,871 [INFO ] pool-3-thread-1 TS_METRICS - DiskUtilization.Percent:45.5|#Level:Host|#hostname:pupu-Stvullinc,timestamp:1746393171
2025-05-05T02:12:51,871 [INFO ] pool-3-thread-1 TS_METRICS - GPUMemoryUtilization.Percent:4.08935546875|#Level:Host,DeviceId:0|#hostname:pupu-Stvullinc,timestamp:1746393171
2025-05-05T02:12:51,871 [INFO ] pool-3-thread-1 TS_METRICS - GPUMemoryUsed.Megabytes:335.0|#Level:Host,DeviceId:0|#hostname:pupu-Stvullinc,timestamp:1746393171
2025-05-05T02:12:51,871 [INFO ] pool-3-thread-1 TS_METRICS - GPUUtilization.Percent:15.0|#Level:Host,DeviceId:0|#hostname:pupu-Stvullinc,timestamp:1746393171
2025-05-05T02:12:51,872 [INFO ] pool-3-thread-1 TS_METRICS - MemoryAvailable.Megabytes:9122.22265625|#Level:Host|#hostname:pupu-Stvullinc,timestamp:1746393171
2025-05-05T02:12:51,872 [INFO ] pool-3-thread-1 TS_METRICS - MemoryUsed.Megabytes:6403.64453125|#Level:Host|#hostname:pupu-Stvullinc,timestamp:1746393171
2025-05-05T02:12:51,872 [INFO ] pool-3-thread-1 TS_METRICS - MemoryUtilization.Percent:42.8|#Level:Host|#hostname:pupu-Stvullinc,timestamp:1746393171
2025-05-05T02:12:52,539 [INFO ] W-9000-my_model_1.0-stdout MODEL_LOG - s_name_part0=/tmp/.ts.sock, s_name_part1=9000, pid=15539
2025-05-05T02:12:52,540 [INFO ] W-9000-my_model_1.0-stdout MODEL_LOG - Listening on port: /tmp/.ts.sock.9000
2025-05-05T02:12:52,547 [INFO ] W-9000-my_model_1.0-stdout MODEL_LOG - Successfully loaded /home/pupu/Desktop/project/.venv/lib/python3.12/site-packages/ts/configs/metrics.yaml.
2025-05-05T02:12:52,548 [INFO ] W-9000-my_model_1.0-stdout MODEL_LOG - [PID]15539
2025-05-05T02:12:52,548 [INFO ] W-9000-my_model_1.0-stdout MODEL_LOG - Torch worker started.
2025-05-05T02:12:52,549 [INFO ] W-9000-my_model_1.0-stdout MODEL_LOG - Python runtime: 3.12.3
2025-05-05T02:12:52,549 [DEBUG] W-9000-my_model_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-my_model_1.0 State change null -> WORKER_STARTED
2025-05-05T02:12:52,549 [DEBUG] W-9000-my_model_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-my_model_1.0 State change null -> WORKER_STARTED
2025-05-05T02:12:52,554 [INFO ] W-9000-my_model_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9000
2025-05-05T02:12:52,554 [INFO ] W-9000-my_model_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9000
2025-05-05T02:12:52,564 [INFO ] W-9000-my_model_1.0-stdout MODEL_LOG - Connection accepted: /tmp/.ts.sock.9000.
2025-05-05T02:12:52,568 [DEBUG] W-9000-my_model_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req.cmd LOAD repeats 1 to backend at: 1746393172568
2025-05-05T02:12:52,568 [DEBUG] W-9000-my_model_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req.cmd LOAD repeats 1 to backend at: 1746393172568
2025-05-05T02:12:52,571 [INFO ] W-9000-my_model_1.0 org.pytorch.serve.wlm.WorkerThread - Looping backend response at: 1746393172571
2025-05-05T02:12:52,571 [INFO ] W-9000-my_model_1.0 org.pytorch.serve.wlm.WorkerThread - Looping backend response at: 1746393172571
2025-05-05T02:12:52,604 [INFO ] W-9000-my_model_1.0-stdout MODEL_LOG - model_name: my_model, batchSize: 1
2025-05-05T02:12:54,160 [INFO ] W-9000-my_model_1.0-stdout MODEL_LOG - OpenVINO is not enabled
2025-05-05T02:12:54,160 [INFO ] W-9000-my_model_1.0-stdout MODEL_LOG - proceeding without onnxruntime
2025-05-05T02:12:54,160 [INFO ] W-9000-my_model_1.0-stdout MODEL_LOG - Torch TensorRT not enabled
2025-05-05T02:12:55,062 [INFO ] W-9000-my_model_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 2490
2025-05-05T02:12:55,062 [INFO ] W-9000-my_model_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 2490
2025-05-05T02:12:55,063 [DEBUG] W-9000-my_model_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-my_model_1.0 State change WORKER_STARTED -> WORKER_MODEL_LOADED
2025-05-05T02:12:55,063 [DEBUG] W-9000-my_model_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-my_model_1.0 State change WORKER_STARTED -> WORKER_MODEL_LOADED
2025-05-05T02:12:55,064 [INFO ] W-9000-my_model_1.0 TS_METRICS - WorkerLoadTime.Milliseconds:4517.0|#WorkerName:W-9000-my_model_1.0,Level:Host|#hostname:pupu-Stvullinc,timestamp:1746393175
2025-05-05T02:12:55,065 [INFO ] W-9000-my_model_1.0 TS_METRICS - WorkerThreadTime.Milliseconds:6.0|#Level:Host|#hostname:pupu-Stvullinc,timestamp:1746393175
2025-05-05T02:13:51,334 [INFO ] pool-3-thread-1 TS_METRICS - CPUUtilization.Percent:33.3|#Level:Host|#hostname:pupu-Stvullinc,timestamp:1746393231
2025-05-05T02:13:51,334 [INFO ] pool-3-thread-1 TS_METRICS - DiskAvailable.Gigabytes:75.26934814453125|#Level:Host|#hostname:pupu-Stvullinc,timestamp:1746393231
2025-05-05T02:13:51,334 [INFO ] pool-3-thread-1 TS_METRICS - DiskUsage.Gigabytes:62.912498474121094|#Level:Host|#hostname:pupu-Stvullinc,timestamp:1746393231
2025-05-05T02:13:51,335 [INFO ] pool-3-thread-1 TS_METRICS - DiskUtilization.Percent:45.5|#Level:Host|#hostname:pupu-Stvullinc,timestamp:1746393231
2025-05-05T02:13:51,335 [INFO ] pool-3-thread-1 TS_METRICS - GPUMemoryUtilization.Percent:6.54296875|#Level:Host,DeviceId:0|#hostname:pupu-Stvullinc,timestamp:1746393231
2025-05-05T02:13:51,335 [INFO ] pool-3-thread-1 TS_METRICS - GPUMemoryUsed.Megabytes:536.0|#Level:Host,DeviceId:0|#hostname:pupu-Stvullinc,timestamp:1746393231
2025-05-05T02:13:51,336 [INFO ] pool-3-thread-1 TS_METRICS - GPUUtilization.Percent:6.0|#Level:Host,DeviceId:0|#hostname:pupu-Stvullinc,timestamp:1746393231
2025-05-05T02:13:51,336 [INFO ] pool-3-thread-1 TS_METRICS - MemoryAvailable.Megabytes:8796.90234375|#Level:Host|#hostname:pupu-Stvullinc,timestamp:1746393231
2025-05-05T02:13:51,336 [INFO ] pool-3-thread-1 TS_METRICS - MemoryUsed.Megabytes:6716.73046875|#Level:Host|#hostname:pupu-Stvullinc,timestamp:1746393231
2025-05-05T02:13:51,337 [INFO ] pool-3-thread-1 TS_METRICS - MemoryUtilization.Percent:44.9|#Level:Host|#hostname:pupu-Stvullinc,timestamp:1746393231
2025-05-05T02:14:51,351 [INFO ] pool-3-thread-1 TS_METRICS - CPUUtilization.Percent:25.0|#Level:Host|#hostname:pupu-Stvullinc,timestamp:1746393291
2025-05-05T02:14:51,351 [INFO ] pool-3-thread-1 TS_METRICS - DiskAvailable.Gigabytes:75.2637710571289|#Level:Host|#hostname:pupu-Stvullinc,timestamp:1746393291
2025-05-05T02:14:51,352 [INFO ] pool-3-thread-1 TS_METRICS - DiskUsage.Gigabytes:62.91807556152344|#Level:Host|#hostname:pupu-Stvullinc,timestamp:1746393291
2025-05-05T02:14:51,352 [INFO ] pool-3-thread-1 TS_METRICS - DiskUtilization.Percent:45.5|#Level:Host|#hostname:pupu-Stvullinc,timestamp:1746393291
2025-05-05T02:14:51,352 [INFO ] pool-3-thread-1 TS_METRICS - GPUMemoryUtilization.Percent:7.5439453125|#Level:Host,DeviceId:0|#hostname:pupu-Stvullinc,timestamp:1746393291
2025-05-05T02:14:51,353 [INFO ] pool-3-thread-1 TS_METRICS - GPUMemoryUsed.Megabytes:618.0|#Level:Host,DeviceId:0|#hostname:pupu-Stvullinc,timestamp:1746393291
2025-05-05T02:14:51,353 [INFO ] pool-3-thread-1 TS_METRICS - GPUUtilization.Percent:13.0|#Level:Host,DeviceId:0|#hostname:pupu-Stvullinc,timestamp:1746393291
2025-05-05T02:14:51,353 [INFO ] pool-3-thread-1 TS_METRICS - MemoryAvailable.Megabytes:8743.703125|#Level:Host|#hostname:pupu-Stvullinc,timestamp:1746393291
2025-05-05T02:14:51,354 [INFO ] pool-3-thread-1 TS_METRICS - MemoryUsed.Megabytes:6752.88671875|#Level:Host|#hostname:pupu-Stvullinc,timestamp:1746393291
2025-05-05T02:14:51,354 [INFO ] pool-3-thread-1 TS_METRICS - MemoryUtilization.Percent:45.2|#Level:Host|#hostname:pupu-Stvullinc,timestamp:1746393291
2025-05-05T02:15:51,319 [INFO ] pool-3-thread-1 TS_METRICS - CPUUtilization.Percent:0.0|#Level:Host|#hostname:pupu-Stvullinc,timestamp:1746393351
2025-05-05T02:15:51,319 [INFO ] pool-3-thread-1 TS_METRICS - DiskAvailable.Gigabytes:75.2635269165039|#Level:Host|#hostname:pupu-Stvullinc,timestamp:1746393351
2025-05-05T02:15:51,320 [INFO ] pool-3-thread-1 TS_METRICS - DiskUsage.Gigabytes:62.91831970214844|#Level:Host|#hostname:pupu-Stvullinc,timestamp:1746393351
2025-05-05T02:15:51,320 [INFO ] pool-3-thread-1 TS_METRICS - DiskUtilization.Percent:45.5|#Level:Host|#hostname:pupu-Stvullinc,timestamp:1746393351
2025-05-05T02:15:51,321 [INFO ] pool-3-thread-1 TS_METRICS - GPUMemoryUtilization.Percent:7.53173828125|#Level:Host,DeviceId:0|#hostname:pupu-Stvullinc,timestamp:1746393351
2025-05-05T02:15:51,321 [INFO ] pool-3-thread-1 TS_METRICS - GPUMemoryUsed.Megabytes:617.0|#Level:Host,DeviceId:0|#hostname:pupu-Stvullinc,timestamp:1746393351
2025-05-05T02:15:51,321 [INFO ] pool-3-thread-1 TS_METRICS - GPUUtilization.Percent:3.0|#Level:Host,DeviceId:0|#hostname:pupu-Stvullinc,timestamp:1746393351
2025-05-05T02:15:51,322 [INFO ] pool-3-thread-1 TS_METRICS - MemoryAvailable.Megabytes:8696.84375|#Level:Host|#hostname:pupu-Stvullinc,timestamp:1746393351
2025-05-05T02:15:51,322 [INFO ] pool-3-thread-1 TS_METRICS - MemoryUsed.Megabytes:6802.04296875|#Level:Host|#hostname:pupu-Stvullinc,timestamp:1746393351
2025-05-05T02:15:51,322 [INFO ] pool-3-thread-1 TS_METRICS - MemoryUtilization.Percent:45.5|#Level:Host|#hostname:pupu-Stvullinc,timestamp:1746393351
2025-05-05T02:16:51,364 [INFO ] pool-3-thread-1 TS_METRICS - CPUUtilization.Percent:33.3|#Level:Host|#hostname:pupu-Stvullinc,timestamp:1746393411
2025-05-05T02:16:51,365 [INFO ] pool-3-thread-1 TS_METRICS - DiskAvailable.Gigabytes:75.2628059387207|#Level:Host|#hostname:pupu-Stvullinc,timestamp:1746393411
2025-05-05T02:16:51,365 [INFO ] pool-3-thread-1 TS_METRICS - DiskUsage.Gigabytes:62.91904067993164|#Level:Host|#hostname:pupu-Stvullinc,timestamp:1746393411
2025-05-05T02:16:51,366 [INFO ] pool-3-thread-1 TS_METRICS - DiskUtilization.Percent:45.5|#Level:Host|#hostname:pupu-Stvullinc,timestamp:1746393411
2025-05-05T02:16:51,366 [INFO ] pool-3-thread-1 TS_METRICS - GPUMemoryUtilization.Percent:7.38525390625|#Level:Host,DeviceId:0|#hostname:pupu-Stvullinc,timestamp:1746393411
2025-05-05T02:16:51,366 [INFO ] pool-3-thread-1 TS_METRICS - GPUMemoryUsed.Megabytes:605.0|#Level:Host,DeviceId:0|#hostname:pupu-Stvullinc,timestamp:1746393411
2025-05-05T02:16:51,366 [INFO ] pool-3-thread-1 TS_METRICS - GPUUtilization.Percent:32.0|#Level:Host,DeviceId:0|#hostname:pupu-Stvullinc,timestamp:1746393411
2025-05-05T02:16:51,367 [INFO ] pool-3-thread-1 TS_METRICS - MemoryAvailable.Megabytes:8751.7421875|#Level:Host|#hostname:pupu-Stvullinc,timestamp:1746393411
2025-05-05T02:16:51,367 [INFO ] pool-3-thread-1 TS_METRICS - MemoryUsed.Megabytes:6748.15234375|#Level:Host|#hostname:pupu-Stvullinc,timestamp:1746393411
2025-05-05T02:16:51,367 [INFO ] pool-3-thread-1 TS_METRICS - MemoryUtilization.Percent:45.2|#Level:Host|#hostname:pupu-Stvullinc,timestamp:1746393411
2025-05-05T02:17:51,378 [INFO ] pool-3-thread-1 TS_METRICS - CPUUtilization.Percent:66.7|#Level:Host|#hostname:pupu-Stvullinc,timestamp:1746393471
2025-05-05T02:17:51,379 [INFO ] pool-3-thread-1 TS_METRICS - DiskAvailable.Gigabytes:75.2626953125|#Level:Host|#hostname:pupu-Stvullinc,timestamp:1746393471
2025-05-05T02:17:51,379 [INFO ] pool-3-thread-1 TS_METRICS - DiskUsage.Gigabytes:62.919151306152344|#Level:Host|#hostname:pupu-Stvullinc,timestamp:1746393471
2025-05-05T02:17:51,379 [INFO ] pool-3-thread-1 TS_METRICS - DiskUtilization.Percent:45.5|#Level:Host|#hostname:pupu-Stvullinc,timestamp:1746393471
2025-05-05T02:17:51,379 [INFO ] pool-3-thread-1 TS_METRICS - GPUMemoryUtilization.Percent:7.36083984375|#Level:Host,DeviceId:0|#hostname:pupu-Stvullinc,timestamp:1746393471
2025-05-05T02:17:51,379 [INFO ] pool-3-thread-1 TS_METRICS - GPUMemoryUsed.Megabytes:603.0|#Level:Host,DeviceId:0|#hostname:pupu-Stvullinc,timestamp:1746393471
2025-05-05T02:17:51,380 [INFO ] pool-3-thread-1 TS_METRICS - GPUUtilization.Percent:9.0|#Level:Host,DeviceId:0|#hostname:pupu-Stvullinc,timestamp:1746393471
2025-05-05T02:17:51,380 [INFO ] pool-3-thread-1 TS_METRICS - MemoryAvailable.Megabytes:8639.24609375|#Level:Host|#hostname:pupu-Stvullinc,timestamp:1746393471
2025-05-05T02:17:51,380 [INFO ] pool-3-thread-1 TS_METRICS - MemoryUsed.Megabytes:6848.765625|#Level:Host|#hostname:pupu-Stvullinc,timestamp:1746393471
2025-05-05T02:17:51,380 [INFO ] pool-3-thread-1 TS_METRICS - MemoryUtilization.Percent:45.9|#Level:Host|#hostname:pupu-Stvullinc,timestamp:1746393471
2025-05-05T02:18:51,351 [INFO ] pool-3-thread-1 TS_METRICS - CPUUtilization.Percent:83.3|#Level:Host|#hostname:pupu-Stvullinc,timestamp:1746393531
2025-05-05T02:18:51,351 [INFO ] pool-3-thread-1 TS_METRICS - DiskAvailable.Gigabytes:75.26251983642578|#Level:Host|#hostname:pupu-Stvullinc,timestamp:1746393531
2025-05-05T02:18:51,351 [INFO ] pool-3-thread-1 TS_METRICS - DiskUsage.Gigabytes:62.91932678222656|#Level:Host|#hostname:pupu-Stvullinc,timestamp:1746393531
2025-05-05T02:18:51,352 [INFO ] pool-3-thread-1 TS_METRICS - DiskUtilization.Percent:45.5|#Level:Host|#hostname:pupu-Stvullinc,timestamp:1746393531
2025-05-05T02:18:51,352 [INFO ] pool-3-thread-1 TS_METRICS - GPUMemoryUtilization.Percent:7.2265625|#Level:Host,DeviceId:0|#hostname:pupu-Stvullinc,timestamp:1746393531
2025-05-05T02:18:51,352 [INFO ] pool-3-thread-1 TS_METRICS - GPUMemoryUsed.Megabytes:592.0|#Level:Host,DeviceId:0|#hostname:pupu-Stvullinc,timestamp:1746393531
2025-05-05T02:18:51,353 [INFO ] pool-3-thread-1 TS_METRICS - GPUUtilization.Percent:6.0|#Level:Host,DeviceId:0|#hostname:pupu-Stvullinc,timestamp:1746393531
2025-05-05T02:18:51,353 [INFO ] pool-3-thread-1 TS_METRICS - MemoryAvailable.Megabytes:8691.38671875|#Level:Host|#hostname:pupu-Stvullinc,timestamp:1746393531
2025-05-05T02:18:51,353 [INFO ] pool-3-thread-1 TS_METRICS - MemoryUsed.Megabytes:6816.69140625|#Level:Host|#hostname:pupu-Stvullinc,timestamp:1746393531
2025-05-05T02:18:51,353 [INFO ] pool-3-thread-1 TS_METRICS - MemoryUtilization.Percent:45.5|#Level:Host|#hostname:pupu-Stvullinc,timestamp:1746393531
2025-05-05T02:19:51,387 [INFO ] pool-3-thread-1 TS_METRICS - CPUUtilization.Percent:33.3|#Level:Host|#hostname:pupu-Stvullinc,timestamp:1746393591
2025-05-05T02:19:51,388 [INFO ] pool-3-thread-1 TS_METRICS - DiskAvailable.Gigabytes:75.26198959350586|#Level:Host|#hostname:pupu-Stvullinc,timestamp:1746393591
2025-05-05T02:19:51,388 [INFO ] pool-3-thread-1 TS_METRICS - DiskUsage.Gigabytes:62.919857025146484|#Level:Host|#hostname:pupu-Stvullinc,timestamp:1746393591
2025-05-05T02:19:51,388 [INFO ] pool-3-thread-1 TS_METRICS - DiskUtilization.Percent:45.5|#Level:Host|#hostname:pupu-Stvullinc,timestamp:1746393591
2025-05-05T02:19:51,389 [INFO ] pool-3-thread-1 TS_METRICS - GPUMemoryUtilization.Percent:7.2509765625|#Level:Host,DeviceId:0|#hostname:pupu-Stvullinc,timestamp:1746393591
2025-05-05T02:19:51,389 [INFO ] pool-3-thread-1 TS_METRICS - GPUMemoryUsed.Megabytes:594.0|#Level:Host,DeviceId:0|#hostname:pupu-Stvullinc,timestamp:1746393591
2025-05-05T02:19:51,389 [INFO ] pool-3-thread-1 TS_METRICS - GPUUtilization.Percent:21.0|#Level:Host,DeviceId:0|#hostname:pupu-Stvullinc,timestamp:1746393591
2025-05-05T02:19:51,389 [INFO ] pool-3-thread-1 TS_METRICS - MemoryAvailable.Megabytes:8664.859375|#Level:Host|#hostname:pupu-Stvullinc,timestamp:1746393591
2025-05-05T02:19:51,389 [INFO ] pool-3-thread-1 TS_METRICS - MemoryUsed.Megabytes:6839.1875|#Level:Host|#hostname:pupu-Stvullinc,timestamp:1746393591
2025-05-05T02:19:51,390 [INFO ] pool-3-thread-1 TS_METRICS - MemoryUtilization.Percent:45.7|#Level:Host|#hostname:pupu-Stvullinc,timestamp:1746393591
2025-05-05T02:20:51,350 [INFO ] pool-3-thread-1 TS_METRICS - CPUUtilization.Percent:0.0|#Level:Host|#hostname:pupu-Stvullinc,timestamp:1746393651
2025-05-05T02:20:51,351 [INFO ] pool-3-thread-1 TS_METRICS - DiskAvailable.Gigabytes:75.26299667358398|#Level:Host|#hostname:pupu-Stvullinc,timestamp:1746393651
2025-05-05T02:20:51,351 [INFO ] pool-3-thread-1 TS_METRICS - DiskUsage.Gigabytes:62.91884994506836|#Level:Host|#hostname:pupu-Stvullinc,timestamp:1746393651
2025-05-05T02:20:51,352 [INFO ] pool-3-thread-1 TS_METRICS - DiskUtilization.Percent:45.5|#Level:Host|#hostname:pupu-Stvullinc,timestamp:1746393651
2025-05-05T02:20:51,352 [INFO ] pool-3-thread-1 TS_METRICS - GPUMemoryUtilization.Percent:7.3486328125|#Level:Host,DeviceId:0|#hostname:pupu-Stvullinc,timestamp:1746393651
2025-05-05T02:20:51,353 [INFO ] pool-3-thread-1 TS_METRICS - GPUMemoryUsed.Megabytes:602.0|#Level:Host,DeviceId:0|#hostname:pupu-Stvullinc,timestamp:1746393651
2025-05-05T02:20:51,353 [INFO ] pool-3-thread-1 TS_METRICS - GPUUtilization.Percent:8.0|#Level:Host,DeviceId:0|#hostname:pupu-Stvullinc,timestamp:1746393651
2025-05-05T02:20:51,353 [INFO ] pool-3-thread-1 TS_METRICS - MemoryAvailable.Megabytes:8666.0625|#Level:Host|#hostname:pupu-Stvullinc,timestamp:1746393651
2025-05-05T02:20:51,353 [INFO ] pool-3-thread-1 TS_METRICS - MemoryUsed.Megabytes:6835.8671875|#Level:Host|#hostname:pupu-Stvullinc,timestamp:1746393651
2025-05-05T02:20:51,354 [INFO ] pool-3-thread-1 TS_METRICS - MemoryUtilization.Percent:45.7|#Level:Host|#hostname:pupu-Stvullinc,timestamp:1746393651
2025-05-05T02:21:51,442 [INFO ] pool-3-thread-1 TS_METRICS - CPUUtilization.Percent:33.3|#Level:Host|#hostname:pupu-Stvullinc,timestamp:1746393711
2025-05-05T02:21:51,442 [INFO ] pool-3-thread-1 TS_METRICS - DiskAvailable.Gigabytes:75.26287460327148|#Level:Host|#hostname:pupu-Stvullinc,timestamp:1746393711
2025-05-05T02:21:51,443 [INFO ] pool-3-thread-1 TS_METRICS - DiskUsage.Gigabytes:62.91897201538086|#Level:Host|#hostname:pupu-Stvullinc,timestamp:1746393711
2025-05-05T02:21:51,443 [INFO ] pool-3-thread-1 TS_METRICS - DiskUtilization.Percent:45.5|#Level:Host|#hostname:pupu-Stvullinc,timestamp:1746393711
2025-05-05T02:21:51,443 [INFO ] pool-3-thread-1 TS_METRICS - GPUMemoryUtilization.Percent:7.23876953125|#Level:Host,DeviceId:0|#hostname:pupu-Stvullinc,timestamp:1746393711
2025-05-05T02:21:51,444 [INFO ] pool-3-thread-1 TS_METRICS - GPUMemoryUsed.Megabytes:593.0|#Level:Host,DeviceId:0|#hostname:pupu-Stvullinc,timestamp:1746393711
2025-05-05T02:21:51,444 [INFO ] pool-3-thread-1 TS_METRICS - GPUUtilization.Percent:4.0|#Level:Host,DeviceId:0|#hostname:pupu-Stvullinc,timestamp:1746393711
2025-05-05T02:21:51,444 [INFO ] pool-3-thread-1 TS_METRICS - MemoryAvailable.Megabytes:8678.578125|#Level:Host|#hostname:pupu-Stvullinc,timestamp:1746393711
2025-05-05T02:21:51,445 [INFO ] pool-3-thread-1 TS_METRICS - MemoryUsed.Megabytes:6833.51171875|#Level:Host|#hostname:pupu-Stvullinc,timestamp:1746393711
2025-05-05T02:21:51,445 [INFO ] pool-3-thread-1 TS_METRICS - MemoryUtilization.Percent:45.6|#Level:Host|#hostname:pupu-Stvullinc,timestamp:1746393711
2025-05-05T02:22:51,375 [INFO ] pool-3-thread-1 TS_METRICS - CPUUtilization.Percent:0.0|#Level:Host|#hostname:pupu-Stvullinc,timestamp:1746393771
2025-05-05T02:22:51,375 [INFO ] pool-3-thread-1 TS_METRICS - DiskAvailable.Gigabytes:75.26156616210938|#Level:Host|#hostname:pupu-Stvullinc,timestamp:1746393771
2025-05-05T02:22:51,376 [INFO ] pool-3-thread-1 TS_METRICS - DiskUsage.Gigabytes:62.92028045654297|#Level:Host|#hostname:pupu-Stvullinc,timestamp:1746393771
2025-05-05T02:22:51,376 [INFO ] pool-3-thread-1 TS_METRICS - DiskUtilization.Percent:45.5|#Level:Host|#hostname:pupu-Stvullinc,timestamp:1746393771
2025-05-05T02:22:51,376 [INFO ] pool-3-thread-1 TS_METRICS - GPUMemoryUtilization.Percent:7.373046875|#Level:Host,DeviceId:0|#hostname:pupu-Stvullinc,timestamp:1746393771
2025-05-05T02:22:51,377 [INFO ] pool-3-thread-1 TS_METRICS - GPUMemoryUsed.Megabytes:604.0|#Level:Host,DeviceId:0|#hostname:pupu-Stvullinc,timestamp:1746393771
2025-05-05T02:22:51,377 [INFO ] pool-3-thread-1 TS_METRICS - GPUUtilization.Percent:13.0|#Level:Host,DeviceId:0|#hostname:pupu-Stvullinc,timestamp:1746393771
2025-05-05T02:22:51,378 [INFO ] pool-3-thread-1 TS_METRICS - MemoryAvailable.Megabytes:8648.31640625|#Level:Host|#hostname:pupu-Stvullinc,timestamp:1746393771
2025-05-05T02:22:51,378 [INFO ] pool-3-thread-1 TS_METRICS - MemoryUsed.Megabytes:6856.54296875|#Level:Host|#hostname:pupu-Stvullinc,timestamp:1746393771
2025-05-05T02:22:51,378 [INFO ] pool-3-thread-1 TS_METRICS - MemoryUtilization.Percent:45.8|#Level:Host|#hostname:pupu-Stvullinc,timestamp:1746393771
2025-05-05T02:23:51,396 [INFO ] pool-3-thread-1 TS_METRICS - CPUUtilization.Percent:80.0|#Level:Host|#hostname:pupu-Stvullinc,timestamp:1746393831
2025-05-05T02:23:51,397 [INFO ] pool-3-thread-1 TS_METRICS - DiskAvailable.Gigabytes:75.26166915893555|#Level:Host|#hostname:pupu-Stvullinc,timestamp:1746393831
2025-05-05T02:23:51,397 [INFO ] pool-3-thread-1 TS_METRICS - DiskUsage.Gigabytes:62.9201774597168|#Level:Host|#hostname:pupu-Stvullinc,timestamp:1746393831
2025-05-05T02:23:51,397 [INFO ] pool-3-thread-1 TS_METRICS - DiskUtilization.Percent:45.5|#Level:Host|#hostname:pupu-Stvullinc,timestamp:1746393831
2025-05-05T02:23:51,398 [INFO ] pool-3-thread-1 TS_METRICS - GPUMemoryUtilization.Percent:7.18994140625|#Level:Host,DeviceId:0|#hostname:pupu-Stvullinc,timestamp:1746393831
2025-05-05T02:23:51,398 [INFO ] pool-3-thread-1 TS_METRICS - GPUMemoryUsed.Megabytes:589.0|#Level:Host,DeviceId:0|#hostname:pupu-Stvullinc,timestamp:1746393831
2025-05-05T02:23:51,398 [INFO ] pool-3-thread-1 TS_METRICS - GPUUtilization.Percent:1.0|#Level:Host,DeviceId:0|#hostname:pupu-Stvullinc,timestamp:1746393831
2025-05-05T02:23:51,398 [INFO ] pool-3-thread-1 TS_METRICS - MemoryAvailable.Megabytes:8692.78515625|#Level:Host|#hostname:pupu-Stvullinc,timestamp:1746393831
2025-05-05T02:23:51,399 [INFO ] pool-3-thread-1 TS_METRICS - MemoryUsed.Megabytes:6812.859375|#Level:Host|#hostname:pupu-Stvullinc,timestamp:1746393831
2025-05-05T02:23:51,399 [INFO ] pool-3-thread-1 TS_METRICS - MemoryUtilization.Percent:45.5|#Level:Host|#hostname:pupu-Stvullinc,timestamp:1746393831
2025-05-05T02:24:51,311 [INFO ] pool-3-thread-1 TS_METRICS - CPUUtilization.Percent:25.0|#Level:Host|#hostname:pupu-Stvullinc,timestamp:1746393891
2025-05-05T02:24:51,311 [INFO ] pool-3-thread-1 TS_METRICS - DiskAvailable.Gigabytes:75.26154708862305|#Level:Host|#hostname:pupu-Stvullinc,timestamp:1746393891
2025-05-05T02:24:51,311 [INFO ] pool-3-thread-1 TS_METRICS - DiskUsage.Gigabytes:62.9202995300293|#Level:Host|#hostname:pupu-Stvullinc,timestamp:1746393891
2025-05-05T02:24:51,312 [INFO ] pool-3-thread-1 TS_METRICS - DiskUtilization.Percent:45.5|#Level:Host|#hostname:pupu-Stvullinc,timestamp:1746393891
2025-05-05T02:24:51,312 [INFO ] pool-3-thread-1 TS_METRICS - GPUMemoryUtilization.Percent:7.1533203125|#Level:Host,DeviceId:0|#hostname:pupu-Stvullinc,timestamp:1746393891
2025-05-05T02:24:51,313 [INFO ] pool-3-thread-1 TS_METRICS - GPUMemoryUsed.Megabytes:586.0|#Level:Host,DeviceId:0|#hostname:pupu-Stvullinc,timestamp:1746393891
2025-05-05T02:24:51,313 [INFO ] pool-3-thread-1 TS_METRICS - GPUUtilization.Percent:1.0|#Level:Host,DeviceId:0|#hostname:pupu-Stvullinc,timestamp:1746393891
2025-05-05T02:24:51,313 [INFO ] pool-3-thread-1 TS_METRICS - MemoryAvailable.Megabytes:8684.44140625|#Level:Host|#hostname:pupu-Stvullinc,timestamp:1746393891
2025-05-05T02:24:51,314 [INFO ] pool-3-thread-1 TS_METRICS - MemoryUsed.Megabytes:6825.21875|#Level:Host|#hostname:pupu-Stvullinc,timestamp:1746393891
2025-05-05T02:24:51,314 [INFO ] pool-3-thread-1 TS_METRICS - MemoryUtilization.Percent:45.6|#Level:Host|#hostname:pupu-Stvullinc,timestamp:1746393891
2025-05-05T02:25:51,304 [INFO ] pool-3-thread-1 TS_METRICS - CPUUtilization.Percent:25.0|#Level:Host|#hostname:pupu-Stvullinc,timestamp:1746393951
2025-05-05T02:25:51,304 [INFO ] pool-3-thread-1 TS_METRICS - DiskAvailable.Gigabytes:75.25739669799805|#Level:Host|#hostname:pupu-Stvullinc,timestamp:1746393951
2025-05-05T02:25:51,305 [INFO ] pool-3-thread-1 TS_METRICS - DiskUsage.Gigabytes:62.9244499206543|#Level:Host|#hostname:pupu-Stvullinc,timestamp:1746393951
2025-05-05T02:25:51,306 [INFO ] pool-3-thread-1 TS_METRICS - DiskUtilization.Percent:45.5|#Level:Host|#hostname:pupu-Stvullinc,timestamp:1746393951
2025-05-05T02:25:51,306 [INFO ] pool-3-thread-1 TS_METRICS - GPUMemoryUtilization.Percent:7.6171875|#Level:Host,DeviceId:0|#hostname:pupu-Stvullinc,timestamp:1746393951
2025-05-05T02:25:51,306 [INFO ] pool-3-thread-1 TS_METRICS - GPUMemoryUsed.Megabytes:624.0|#Level:Host,DeviceId:0|#hostname:pupu-Stvullinc,timestamp:1746393951
2025-05-05T02:25:51,307 [INFO ] pool-3-thread-1 TS_METRICS - GPUUtilization.Percent:19.0|#Level:Host,DeviceId:0|#hostname:pupu-Stvullinc,timestamp:1746393951
2025-05-05T02:25:51,307 [INFO ] pool-3-thread-1 TS_METRICS - MemoryAvailable.Megabytes:8694.84375|#Level:Host|#hostname:pupu-Stvullinc,timestamp:1746393951
2025-05-05T02:25:51,307 [INFO ] pool-3-thread-1 TS_METRICS - MemoryUsed.Megabytes:6800.58984375|#Level:Host|#hostname:pupu-Stvullinc,timestamp:1746393951
2025-05-05T02:25:51,308 [INFO ] pool-3-thread-1 TS_METRICS - MemoryUtilization.Percent:45.5|#Level:Host|#hostname:pupu-Stvullinc,timestamp:1746393951
2025-05-05T02:26:51,331 [INFO ] pool-3-thread-1 TS_METRICS - CPUUtilization.Percent:0.0|#Level:Host|#hostname:pupu-Stvullinc,timestamp:1746394011
2025-05-05T02:26:51,332 [INFO ] pool-3-thread-1 TS_METRICS - DiskAvailable.Gigabytes:75.25717163085938|#Level:Host|#hostname:pupu-Stvullinc,timestamp:1746394011
2025-05-05T02:26:51,332 [INFO ] pool-3-thread-1 TS_METRICS - DiskUsage.Gigabytes:62.92467498779297|#Level:Host|#hostname:pupu-Stvullinc,timestamp:1746394011
2025-05-05T02:26:51,332 [INFO ] pool-3-thread-1 TS_METRICS - DiskUtilization.Percent:45.5|#Level:Host|#hostname:pupu-Stvullinc,timestamp:1746394011
2025-05-05T02:26:51,333 [INFO ] pool-3-thread-1 TS_METRICS - GPUMemoryUtilization.Percent:7.51953125|#Level:Host,DeviceId:0|#hostname:pupu-Stvullinc,timestamp:1746394011
2025-05-05T02:26:51,333 [INFO ] pool-3-thread-1 TS_METRICS - GPUMemoryUsed.Megabytes:616.0|#Level:Host,DeviceId:0|#hostname:pupu-Stvullinc,timestamp:1746394011
2025-05-05T02:26:51,333 [INFO ] pool-3-thread-1 TS_METRICS - GPUUtilization.Percent:12.0|#Level:Host,DeviceId:0|#hostname:pupu-Stvullinc,timestamp:1746394011
2025-05-05T02:26:51,333 [INFO ] pool-3-thread-1 TS_METRICS - MemoryAvailable.Megabytes:8760.31640625|#Level:Host|#hostname:pupu-Stvullinc,timestamp:1746394011
2025-05-05T02:26:51,334 [INFO ] pool-3-thread-1 TS_METRICS - MemoryUsed.Megabytes:6731.9609375|#Level:Host|#hostname:pupu-Stvullinc,timestamp:1746394011
2025-05-05T02:26:51,334 [INFO ] pool-3-thread-1 TS_METRICS - MemoryUtilization.Percent:45.1|#Level:Host|#hostname:pupu-Stvullinc,timestamp:1746394011
2025-05-05T02:27:51,297 [INFO ] pool-3-thread-1 TS_METRICS - CPUUtilization.Percent:33.3|#Level:Host|#hostname:pupu-Stvullinc,timestamp:1746394071
2025-05-05T02:27:51,299 [INFO ] pool-3-thread-1 TS_METRICS - DiskAvailable.Gigabytes:75.25642013549805|#Level:Host|#hostname:pupu-Stvullinc,timestamp:1746394071
2025-05-05T02:27:51,299 [INFO ] pool-3-thread-1 TS_METRICS - DiskUsage.Gigabytes:62.9254264831543|#Level:Host|#hostname:pupu-Stvullinc,timestamp:1746394071
2025-05-05T02:27:51,299 [INFO ] pool-3-thread-1 TS_METRICS - DiskUtilization.Percent:45.5|#Level:Host|#hostname:pupu-Stvullinc,timestamp:1746394071
2025-05-05T02:27:51,300 [INFO ] pool-3-thread-1 TS_METRICS - GPUMemoryUtilization.Percent:7.4462890625|#Level:Host,DeviceId:0|#hostname:pupu-Stvullinc,timestamp:1746394071
2025-05-05T02:27:51,300 [INFO ] pool-3-thread-1 TS_METRICS - GPUMemoryUsed.Megabytes:610.0|#Level:Host,DeviceId:0|#hostname:pupu-Stvullinc,timestamp:1746394071
2025-05-05T02:27:51,300 [INFO ] pool-3-thread-1 TS_METRICS - GPUUtilization.Percent:9.0|#Level:Host,DeviceId:0|#hostname:pupu-Stvullinc,timestamp:1746394071
2025-05-05T02:27:51,300 [INFO ] pool-3-thread-1 TS_METRICS - MemoryAvailable.Megabytes:8766.02734375|#Level:Host|#hostname:pupu-Stvullinc,timestamp:1746394071
2025-05-05T02:27:51,300 [INFO ] pool-3-thread-1 TS_METRICS - MemoryUsed.Megabytes:6734.8125|#Level:Host|#hostname:pupu-Stvullinc,timestamp:1746394071
2025-05-05T02:27:51,300 [INFO ] pool-3-thread-1 TS_METRICS - MemoryUtilization.Percent:45.1|#Level:Host|#hostname:pupu-Stvullinc,timestamp:1746394071
2025-05-05T02:28:51,462 [INFO ] pool-3-thread-1 TS_METRICS - CPUUtilization.Percent:50.0|#Level:Host|#hostname:pupu-Stvullinc,timestamp:1746394131
2025-05-05T02:28:51,465 [INFO ] pool-3-thread-1 TS_METRICS - DiskAvailable.Gigabytes:75.25625610351562|#Level:Host|#hostname:pupu-Stvullinc,timestamp:1746394131
2025-05-05T02:28:51,465 [INFO ] pool-3-thread-1 TS_METRICS - DiskUsage.Gigabytes:62.92559051513672|#Level:Host|#hostname:pupu-Stvullinc,timestamp:1746394131
2025-05-05T02:28:51,465 [INFO ] pool-3-thread-1 TS_METRICS - DiskUtilization.Percent:45.5|#Level:Host|#hostname:pupu-Stvullinc,timestamp:1746394131
2025-05-05T02:28:51,466 [INFO ] pool-3-thread-1 TS_METRICS - GPUMemoryUtilization.Percent:7.31201171875|#Level:Host,DeviceId:0|#hostname:pupu-Stvullinc,timestamp:1746394131
2025-05-05T02:28:51,466 [INFO ] pool-3-thread-1 TS_METRICS - GPUMemoryUsed.Megabytes:599.0|#Level:Host,DeviceId:0|#hostname:pupu-Stvullinc,timestamp:1746394131
2025-05-05T02:28:51,466 [INFO ] pool-3-thread-1 TS_METRICS - GPUUtilization.Percent:10.0|#Level:Host,DeviceId:0|#hostname:pupu-Stvullinc,timestamp:1746394131
2025-05-05T02:28:51,466 [INFO ] pool-3-thread-1 TS_METRICS - MemoryAvailable.Megabytes:8679.19921875|#Level:Host|#hostname:pupu-Stvullinc,timestamp:1746394131
2025-05-05T02:28:51,466 [INFO ] pool-3-thread-1 TS_METRICS - MemoryUsed.Megabytes:6821.63671875|#Level:Host|#hostname:pupu-Stvullinc,timestamp:1746394131
2025-05-05T02:28:51,466 [INFO ] pool-3-thread-1 TS_METRICS - MemoryUtilization.Percent:45.6|#Level:Host|#hostname:pupu-Stvullinc,timestamp:1746394131
2025-05-05T02:29:51,387 [INFO ] pool-3-thread-1 TS_METRICS - CPUUtilization.Percent:0.0|#Level:Host|#hostname:pupu-Stvullinc,timestamp:1746394191
2025-05-05T02:29:51,388 [INFO ] pool-3-thread-1 TS_METRICS - DiskAvailable.Gigabytes:75.256103515625|#Level:Host|#hostname:pupu-Stvullinc,timestamp:1746394191
2025-05-05T02:29:51,388 [INFO ] pool-3-thread-1 TS_METRICS - DiskUsage.Gigabytes:62.925743103027344|#Level:Host|#hostname:pupu-Stvullinc,timestamp:1746394191
2025-05-05T02:29:51,388 [INFO ] pool-3-thread-1 TS_METRICS - DiskUtilization.Percent:45.5|#Level:Host|#hostname:pupu-Stvullinc,timestamp:1746394191
2025-05-05T02:29:51,388 [INFO ] pool-3-thread-1 TS_METRICS - GPUMemoryUtilization.Percent:7.3486328125|#Level:Host,DeviceId:0|#hostname:pupu-Stvullinc,timestamp:1746394191
2025-05-05T02:29:51,388 [INFO ] pool-3-thread-1 TS_METRICS - GPUMemoryUsed.Megabytes:602.0|#Level:Host,DeviceId:0|#hostname:pupu-Stvullinc,timestamp:1746394191
2025-05-05T02:29:51,388 [INFO ] pool-3-thread-1 TS_METRICS - GPUUtilization.Percent:4.0|#Level:Host,DeviceId:0|#hostname:pupu-Stvullinc,timestamp:1746394191
2025-05-05T02:29:51,389 [INFO ] pool-3-thread-1 TS_METRICS - MemoryAvailable.Megabytes:8675.359375|#Level:Host|#hostname:pupu-Stvullinc,timestamp:1746394191
2025-05-05T02:29:51,389 [INFO ] pool-3-thread-1 TS_METRICS - MemoryUsed.Megabytes:6823.33984375|#Level:Host|#hostname:pupu-Stvullinc,timestamp:1746394191
2025-05-05T02:29:51,389 [INFO ] pool-3-thread-1 TS_METRICS - MemoryUtilization.Percent:45.6|#Level:Host|#hostname:pupu-Stvullinc,timestamp:1746394191
2025-05-05T02:30:51,357 [INFO ] pool-3-thread-1 TS_METRICS - CPUUtilization.Percent:33.3|#Level:Host|#hostname:pupu-Stvullinc,timestamp:1746394251
2025-05-05T02:30:51,357 [INFO ] pool-3-thread-1 TS_METRICS - DiskAvailable.Gigabytes:75.25558471679688|#Level:Host|#hostname:pupu-Stvullinc,timestamp:1746394251
2025-05-05T02:30:51,358 [INFO ] pool-3-thread-1 TS_METRICS - DiskUsage.Gigabytes:62.92626190185547|#Level:Host|#hostname:pupu-Stvullinc,timestamp:1746394251
2025-05-05T02:30:51,358 [INFO ] pool-3-thread-1 TS_METRICS - DiskUtilization.Percent:45.5|#Level:Host|#hostname:pupu-Stvullinc,timestamp:1746394251
2025-05-05T02:30:51,358 [INFO ] pool-3-thread-1 TS_METRICS - GPUMemoryUtilization.Percent:7.2509765625|#Level:Host,DeviceId:0|#hostname:pupu-Stvullinc,timestamp:1746394251
2025-05-05T02:30:51,358 [INFO ] pool-3-thread-1 TS_METRICS - GPUMemoryUsed.Megabytes:594.0|#Level:Host,DeviceId:0|#hostname:pupu-Stvullinc,timestamp:1746394251
2025-05-05T02:30:51,358 [INFO ] pool-3-thread-1 TS_METRICS - GPUUtilization.Percent:13.0|#Level:Host,DeviceId:0|#hostname:pupu-Stvullinc,timestamp:1746394251
2025-05-05T02:30:51,358 [INFO ] pool-3-thread-1 TS_METRICS - MemoryAvailable.Megabytes:8656.6953125|#Level:Host|#hostname:pupu-Stvullinc,timestamp:1746394251
2025-05-05T02:30:51,359 [INFO ] pool-3-thread-1 TS_METRICS - MemoryUsed.Megabytes:6842.25|#Level:Host|#hostname:pupu-Stvullinc,timestamp:1746394251
2025-05-05T02:30:51,359 [INFO ] pool-3-thread-1 TS_METRICS - MemoryUtilization.Percent:45.8|#Level:Host|#hostname:pupu-Stvullinc,timestamp:1746394251
2025-05-05T02:31:51,333 [INFO ] pool-3-thread-1 TS_METRICS - CPUUtilization.Percent:0.0|#Level:Host|#hostname:pupu-Stvullinc,timestamp:1746394311
2025-05-05T02:31:51,333 [INFO ] pool-3-thread-1 TS_METRICS - DiskAvailable.Gigabytes:75.2561149597168|#Level:Host|#hostname:pupu-Stvullinc,timestamp:1746394311
2025-05-05T02:31:51,333 [INFO ] pool-3-thread-1 TS_METRICS - DiskUsage.Gigabytes:62.92573165893555|#Level:Host|#hostname:pupu-Stvullinc,timestamp:1746394311
2025-05-05T02:31:51,334 [INFO ] pool-3-thread-1 TS_METRICS - DiskUtilization.Percent:45.5|#Level:Host|#hostname:pupu-Stvullinc,timestamp:1746394311
2025-05-05T02:31:51,334 [INFO ] pool-3-thread-1 TS_METRICS - GPUMemoryUtilization.Percent:7.373046875|#Level:Host,DeviceId:0|#hostname:pupu-Stvullinc,timestamp:1746394311
2025-05-05T02:31:51,334 [INFO ] pool-3-thread-1 TS_METRICS - GPUMemoryUsed.Megabytes:604.0|#Level:Host,DeviceId:0|#hostname:pupu-Stvullinc,timestamp:1746394311
2025-05-05T02:31:51,334 [INFO ] pool-3-thread-1 TS_METRICS - GPUUtilization.Percent:0.0|#Level:Host,DeviceId:0|#hostname:pupu-Stvullinc,timestamp:1746394311
2025-05-05T02:31:51,334 [INFO ] pool-3-thread-1 TS_METRICS - MemoryAvailable.Megabytes:8675.9140625|#Level:Host|#hostname:pupu-Stvullinc,timestamp:1746394311
2025-05-05T02:31:51,334 [INFO ] pool-3-thread-1 TS_METRICS - MemoryUsed.Megabytes:6825.8671875|#Level:Host|#hostname:pupu-Stvullinc,timestamp:1746394311
2025-05-05T02:31:51,334 [INFO ] pool-3-thread-1 TS_METRICS - MemoryUtilization.Percent:45.6|#Level:Host|#hostname:pupu-Stvullinc,timestamp:1746394311
2025-05-05T02:32:51,341 [INFO ] pool-3-thread-1 TS_METRICS - CPUUtilization.Percent:25.0|#Level:Host|#hostname:pupu-Stvullinc,timestamp:1746394371
2025-05-05T02:32:51,341 [INFO ] pool-3-thread-1 TS_METRICS - DiskAvailable.Gigabytes:75.2560920715332|#Level:Host|#hostname:pupu-Stvullinc,timestamp:1746394371
2025-05-05T02:32:51,341 [INFO ] pool-3-thread-1 TS_METRICS - DiskUsage.Gigabytes:62.92575454711914|#Level:Host|#hostname:pupu-Stvullinc,timestamp:1746394371
2025-05-05T02:32:51,342 [INFO ] pool-3-thread-1 TS_METRICS - DiskUtilization.Percent:45.5|#Level:Host|#hostname:pupu-Stvullinc,timestamp:1746394371
2025-05-05T02:32:51,342 [INFO ] pool-3-thread-1 TS_METRICS - GPUMemoryUtilization.Percent:6.9091796875|#Level:Host,DeviceId:0|#hostname:pupu-Stvullinc,timestamp:1746394371
2025-05-05T02:32:51,342 [INFO ] pool-3-thread-1 TS_METRICS - GPUMemoryUsed.Megabytes:566.0|#Level:Host,DeviceId:0|#hostname:pupu-Stvullinc,timestamp:1746394371
2025-05-05T02:32:51,342 [INFO ] pool-3-thread-1 TS_METRICS - GPUUtilization.Percent:0.0|#Level:Host,DeviceId:0|#hostname:pupu-Stvullinc,timestamp:1746394371
2025-05-05T02:32:51,342 [INFO ] pool-3-thread-1 TS_METRICS - MemoryAvailable.Megabytes:8688.4765625|#Level:Host|#hostname:pupu-Stvullinc,timestamp:1746394371
2025-05-05T02:32:51,343 [INFO ] pool-3-thread-1 TS_METRICS - MemoryUsed.Megabytes:6820.14453125|#Level:Host|#hostname:pupu-Stvullinc,timestamp:1746394371
2025-05-05T02:32:51,343 [INFO ] pool-3-thread-1 TS_METRICS - MemoryUtilization.Percent:45.6|#Level:Host|#hostname:pupu-Stvullinc,timestamp:1746394371
2025-05-05T02:33:41,370 [INFO ] epollEventLoopGroup-3-1 TS_METRICS - ts_inference_requests_total.Count:1.0|#model_name:my_model,model_version:default|#hostname:pupu-Stvullinc,timestamp:1746394421
2025-05-05T02:33:41,373 [DEBUG] W-9000-my_model_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req.cmd PREDICT repeats 1 to backend at: 1746394421372
2025-05-05T02:33:41,373 [DEBUG] W-9000-my_model_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req.cmd PREDICT repeats 1 to backend at: 1746394421372
2025-05-05T02:33:41,373 [INFO ] W-9000-my_model_1.0 org.pytorch.serve.wlm.WorkerThread - Looping backend response at: 1746394421373
2025-05-05T02:33:41,373 [INFO ] W-9000-my_model_1.0 org.pytorch.serve.wlm.WorkerThread - Looping backend response at: 1746394421373
2025-05-05T02:33:41,375 [INFO ] W-9000-my_model_1.0-stdout MODEL_LOG - Backend received inference at: 1746394421
2025-05-05T02:33:43,422 [INFO ] W-9000-my_model_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - result=[METRICS]HandlerTime.Milliseconds:2046.61|#ModelName:my_model,Level:Model|#type:GAUGE|#hostname:pupu-Stvullinc,1746394423,d43b150f-1a81-4506-97f8-64737b9f2bc7, pattern=[METRICS]
2025-05-05T02:33:43,422 [INFO ] W-9000-my_model_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - result=[METRICS]HandlerTime.Milliseconds:2046.61|#ModelName:my_model,Level:Model|#type:GAUGE|#hostname:pupu-Stvullinc,1746394423,d43b150f-1a81-4506-97f8-64737b9f2bc7, pattern=[METRICS]
2025-05-05T02:33:43,423 [INFO ] W-9000-my_model_1.0-stdout MODEL_METRICS - HandlerTime.ms:2046.61|#ModelName:my_model,Level:Model|#hostname:pupu-Stvullinc,requestID:d43b150f-1a81-4506-97f8-64737b9f2bc7,timestamp:1746394423
2025-05-05T02:33:43,423 [INFO ] W-9000-my_model_1.0 org.pytorch.serve.wlm.BatchAggregator - Sending response for jobId d43b150f-1a81-4506-97f8-64737b9f2bc7
2025-05-05T02:33:43,423 [INFO ] W-9000-my_model_1.0 org.pytorch.serve.wlm.BatchAggregator - Sending response for jobId d43b150f-1a81-4506-97f8-64737b9f2bc7
2025-05-05T02:33:43,423 [INFO ] W-9000-my_model_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - result=[METRICS]PredictionTime.Milliseconds:2046.86|#ModelName:my_model,Level:Model|#type:GAUGE|#hostname:pupu-Stvullinc,1746394423,d43b150f-1a81-4506-97f8-64737b9f2bc7, pattern=[METRICS]
2025-05-05T02:33:43,423 [INFO ] W-9000-my_model_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - result=[METRICS]PredictionTime.Milliseconds:2046.86|#ModelName:my_model,Level:Model|#type:GAUGE|#hostname:pupu-Stvullinc,1746394423,d43b150f-1a81-4506-97f8-64737b9f2bc7, pattern=[METRICS]
2025-05-05T02:33:43,424 [INFO ] W-9000-my_model_1.0-stdout MODEL_METRICS - PredictionTime.ms:2046.86|#ModelName:my_model,Level:Model|#hostname:pupu-Stvullinc,requestID:d43b150f-1a81-4506-97f8-64737b9f2bc7,timestamp:1746394423
2025-05-05T02:33:43,425 [INFO ] W-9000-my_model_1.0 ACCESS_LOG - /127.0.0.1:42634 "POST /predictions/my_model HTTP/1.1" 200 2059
2025-05-05T02:33:43,426 [INFO ] W-9000-my_model_1.0 TS_METRICS - Requests2XX.Count:1.0|#Level:Host|#hostname:pupu-Stvullinc,timestamp:1746394423
2025-05-05T02:33:43,426 [INFO ] W-9000-my_model_1.0 TS_METRICS - ts_inference_latency_microseconds.Microseconds:2051329.959|#model_name:my_model,model_version:default|#hostname:pupu-Stvullinc,timestamp:1746394423
2025-05-05T02:33:43,427 [INFO ] W-9000-my_model_1.0 TS_METRICS - ts_queue_latency_microseconds.Microseconds:176.543|#model_name:my_model,model_version:default|#hostname:pupu-Stvullinc,timestamp:1746394423
2025-05-05T02:33:43,427 [DEBUG] W-9000-my_model_1.0 org.pytorch.serve.job.RestJob - Waiting time ns: 176543, Backend time ns: 2054464807
2025-05-05T02:33:43,427 [DEBUG] W-9000-my_model_1.0 org.pytorch.serve.job.RestJob - Waiting time ns: 176543, Backend time ns: 2054464807
2025-05-05T02:33:43,427 [INFO ] W-9000-my_model_1.0 TS_METRICS - QueueTime.Milliseconds:0.0|#Level:Host|#hostname:pupu-Stvullinc,timestamp:1746394423
2025-05-05T02:33:43,427 [INFO ] W-9000-my_model_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 2050
2025-05-05T02:33:43,427 [INFO ] W-9000-my_model_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 2050
2025-05-05T02:33:43,428 [INFO ] W-9000-my_model_1.0 TS_METRICS - WorkerThreadTime.Milliseconds:5.0|#Level:Host|#hostname:pupu-Stvullinc,timestamp:1746394423
2025-05-05T02:33:51,256 [INFO ] pool-3-thread-1 TS_METRICS - CPUUtilization.Percent:25.0|#Level:Host|#hostname:pupu-Stvullinc,timestamp:1746394431
2025-05-05T02:33:51,257 [INFO ] pool-3-thread-1 TS_METRICS - DiskAvailable.Gigabytes:75.25585556030273|#Level:Host|#hostname:pupu-Stvullinc,timestamp:1746394431
2025-05-05T02:33:51,257 [INFO ] pool-3-thread-1 TS_METRICS - DiskUsage.Gigabytes:62.92599105834961|#Level:Host|#hostname:pupu-Stvullinc,timestamp:1746394431
2025-05-05T02:33:51,258 [INFO ] pool-3-thread-1 TS_METRICS - DiskUtilization.Percent:45.5|#Level:Host|#hostname:pupu-Stvullinc,timestamp:1746394431
2025-05-05T02:33:51,258 [INFO ] pool-3-thread-1 TS_METRICS - GPUMemoryUtilization.Percent:8.41064453125|#Level:Host,DeviceId:0|#hostname:pupu-Stvullinc,timestamp:1746394431
2025-05-05T02:33:51,258 [INFO ] pool-3-thread-1 TS_METRICS - GPUMemoryUsed.Megabytes:689.0|#Level:Host,DeviceId:0|#hostname:pupu-Stvullinc,timestamp:1746394431
2025-05-05T02:33:51,258 [INFO ] pool-3-thread-1 TS_METRICS - GPUUtilization.Percent:0.0|#Level:Host,DeviceId:0|#hostname:pupu-Stvullinc,timestamp:1746394431
2025-05-05T02:33:51,259 [INFO ] pool-3-thread-1 TS_METRICS - MemoryAvailable.Megabytes:8492.73828125|#Level:Host|#hostname:pupu-Stvullinc,timestamp:1746394431
2025-05-05T02:33:51,259 [INFO ] pool-3-thread-1 TS_METRICS - MemoryUsed.Megabytes:6997.99609375|#Level:Host|#hostname:pupu-Stvullinc,timestamp:1746394431
2025-05-05T02:33:51,259 [INFO ] pool-3-thread-1 TS_METRICS - MemoryUtilization.Percent:46.8|#Level:Host|#hostname:pupu-Stvullinc,timestamp:1746394431
2025-05-05T02:34:51,302 [INFO ] pool-3-thread-1 TS_METRICS - CPUUtilization.Percent:0.0|#Level:Host|#hostname:pupu-Stvullinc,timestamp:1746394491
2025-05-05T02:34:51,302 [INFO ] pool-3-thread-1 TS_METRICS - DiskAvailable.Gigabytes:75.2557487487793|#Level:Host|#hostname:pupu-Stvullinc,timestamp:1746394491
2025-05-05T02:34:51,302 [INFO ] pool-3-thread-1 TS_METRICS - DiskUsage.Gigabytes:62.92609786987305|#Level:Host|#hostname:pupu-Stvullinc,timestamp:1746394491
2025-05-05T02:34:51,303 [INFO ] pool-3-thread-1 TS_METRICS - DiskUtilization.Percent:45.5|#Level:Host|#hostname:pupu-Stvullinc,timestamp:1746394491
2025-05-05T02:34:51,303 [INFO ] pool-3-thread-1 TS_METRICS - GPUMemoryUtilization.Percent:8.65478515625|#Level:Host,DeviceId:0|#hostname:pupu-Stvullinc,timestamp:1746394491
2025-05-05T02:34:51,303 [INFO ] pool-3-thread-1 TS_METRICS - GPUMemoryUsed.Megabytes:709.0|#Level:Host,DeviceId:0|#hostname:pupu-Stvullinc,timestamp:1746394491
2025-05-05T02:34:51,303 [INFO ] pool-3-thread-1 TS_METRICS - GPUUtilization.Percent:22.0|#Level:Host,DeviceId:0|#hostname:pupu-Stvullinc,timestamp:1746394491
2025-05-05T02:34:51,304 [INFO ] pool-3-thread-1 TS_METRICS - MemoryAvailable.Megabytes:8450.3671875|#Level:Host|#hostname:pupu-Stvullinc,timestamp:1746394491
2025-05-05T02:34:51,304 [INFO ] pool-3-thread-1 TS_METRICS - MemoryUsed.Megabytes:7040.45703125|#Level:Host|#hostname:pupu-Stvullinc,timestamp:1746394491
2025-05-05T02:34:51,304 [INFO ] pool-3-thread-1 TS_METRICS - MemoryUtilization.Percent:47.0|#Level:Host|#hostname:pupu-Stvullinc,timestamp:1746394491
2025-05-05T02:35:51,304 [INFO ] pool-3-thread-1 TS_METRICS - CPUUtilization.Percent:33.3|#Level:Host|#hostname:pupu-Stvullinc,timestamp:1746394551
2025-05-05T02:35:51,305 [INFO ] pool-3-thread-1 TS_METRICS - DiskAvailable.Gigabytes:75.25350952148438|#Level:Host|#hostname:pupu-Stvullinc,timestamp:1746394551
2025-05-05T02:35:51,305 [INFO ] pool-3-thread-1 TS_METRICS - DiskUsage.Gigabytes:62.92833709716797|#Level:Host|#hostname:pupu-Stvullinc,timestamp:1746394551
2025-05-05T02:35:51,305 [INFO ] pool-3-thread-1 TS_METRICS - DiskUtilization.Percent:45.5|#Level:Host|#hostname:pupu-Stvullinc,timestamp:1746394551
2025-05-05T02:35:51,305 [INFO ] pool-3-thread-1 TS_METRICS - GPUMemoryUtilization.Percent:8.642578125|#Level:Host,DeviceId:0|#hostname:pupu-Stvullinc,timestamp:1746394551
2025-05-05T02:35:51,305 [INFO ] pool-3-thread-1 TS_METRICS - GPUMemoryUsed.Megabytes:708.0|#Level:Host,DeviceId:0|#hostname:pupu-Stvullinc,timestamp:1746394551
2025-05-05T02:35:51,305 [INFO ] pool-3-thread-1 TS_METRICS - GPUUtilization.Percent:7.0|#Level:Host,DeviceId:0|#hostname:pupu-Stvullinc,timestamp:1746394551
2025-05-05T02:35:51,306 [INFO ] pool-3-thread-1 TS_METRICS - MemoryAvailable.Megabytes:8421.0703125|#Level:Host|#hostname:pupu-Stvullinc,timestamp:1746394551
2025-05-05T02:35:51,306 [INFO ] pool-3-thread-1 TS_METRICS - MemoryUsed.Megabytes:7072.2265625|#Level:Host|#hostname:pupu-Stvullinc,timestamp:1746394551
2025-05-05T02:35:51,306 [INFO ] pool-3-thread-1 TS_METRICS - MemoryUtilization.Percent:47.2|#Level:Host|#hostname:pupu-Stvullinc,timestamp:1746394551
2025-05-05T02:36:51,302 [INFO ] pool-3-thread-1 TS_METRICS - CPUUtilization.Percent:0.0|#Level:Host|#hostname:pupu-Stvullinc,timestamp:1746394611
2025-05-05T02:36:51,303 [INFO ] pool-3-thread-1 TS_METRICS - DiskAvailable.Gigabytes:75.25328826904297|#Level:Host|#hostname:pupu-Stvullinc,timestamp:1746394611
2025-05-05T02:36:51,303 [INFO ] pool-3-thread-1 TS_METRICS - DiskUsage.Gigabytes:62.928558349609375|#Level:Host|#hostname:pupu-Stvullinc,timestamp:1746394611
2025-05-05T02:36:51,304 [INFO ] pool-3-thread-1 TS_METRICS - DiskUtilization.Percent:45.5|#Level:Host|#hostname:pupu-Stvullinc,timestamp:1746394611
2025-05-05T02:36:51,304 [INFO ] pool-3-thread-1 TS_METRICS - GPUMemoryUtilization.Percent:8.85009765625|#Level:Host,DeviceId:0|#hostname:pupu-Stvullinc,timestamp:1746394611
2025-05-05T02:36:51,304 [INFO ] pool-3-thread-1 TS_METRICS - GPUMemoryUsed.Megabytes:725.0|#Level:Host,DeviceId:0|#hostname:pupu-Stvullinc,timestamp:1746394611
2025-05-05T02:36:51,304 [INFO ] pool-3-thread-1 TS_METRICS - GPUUtilization.Percent:3.0|#Level:Host,DeviceId:0|#hostname:pupu-Stvullinc,timestamp:1746394611
2025-05-05T02:36:51,305 [INFO ] pool-3-thread-1 TS_METRICS - MemoryAvailable.Megabytes:8464.328125|#Level:Host|#hostname:pupu-Stvullinc,timestamp:1746394611
2025-05-05T02:36:51,305 [INFO ] pool-3-thread-1 TS_METRICS - MemoryUsed.Megabytes:7026.8359375|#Level:Host|#hostname:pupu-Stvullinc,timestamp:1746394611
2025-05-05T02:36:51,305 [INFO ] pool-3-thread-1 TS_METRICS - MemoryUtilization.Percent:47.0|#Level:Host|#hostname:pupu-Stvullinc,timestamp:1746394611
2025-05-05T02:36:52,066 [INFO ] epollEventLoopGroup-3-2 TS_METRICS - ts_inference_requests_total.Count:1.0|#model_name:my_model,model_version:default|#hostname:pupu-Stvullinc,timestamp:1746394612
2025-05-05T02:36:52,067 [DEBUG] W-9000-my_model_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req.cmd PREDICT repeats 1 to backend at: 1746394612067
2025-05-05T02:36:52,067 [DEBUG] W-9000-my_model_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req.cmd PREDICT repeats 1 to backend at: 1746394612067
2025-05-05T02:36:52,068 [INFO ] W-9000-my_model_1.0 org.pytorch.serve.wlm.WorkerThread - Looping backend response at: 1746394612068
2025-05-05T02:36:52,068 [INFO ] W-9000-my_model_1.0 org.pytorch.serve.wlm.WorkerThread - Looping backend response at: 1746394612068
2025-05-05T02:36:52,070 [INFO ] W-9000-my_model_1.0-stdout MODEL_LOG - Backend received inference at: 1746394612
2025-05-05T02:36:52,131 [INFO ] W-9000-my_model_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - result=[METRICS]HandlerTime.Milliseconds:61.53|#ModelName:my_model,Level:Model|#type:GAUGE|#hostname:pupu-Stvullinc,1746394612,9b1b7750-a40a-4754-ab0b-b0f0ef0794ea, pattern=[METRICS]
2025-05-05T02:36:52,131 [INFO ] W-9000-my_model_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - result=[METRICS]HandlerTime.Milliseconds:61.53|#ModelName:my_model,Level:Model|#type:GAUGE|#hostname:pupu-Stvullinc,1746394612,9b1b7750-a40a-4754-ab0b-b0f0ef0794ea, pattern=[METRICS]
2025-05-05T02:36:52,132 [INFO ] W-9000-my_model_1.0-stdout MODEL_METRICS - HandlerTime.ms:61.53|#ModelName:my_model,Level:Model|#hostname:pupu-Stvullinc,requestID:9b1b7750-a40a-4754-ab0b-b0f0ef0794ea,timestamp:1746394612
2025-05-05T02:36:52,132 [INFO ] W-9000-my_model_1.0 org.pytorch.serve.wlm.BatchAggregator - Sending response for jobId 9b1b7750-a40a-4754-ab0b-b0f0ef0794ea
2025-05-05T02:36:52,132 [INFO ] W-9000-my_model_1.0 org.pytorch.serve.wlm.BatchAggregator - Sending response for jobId 9b1b7750-a40a-4754-ab0b-b0f0ef0794ea
2025-05-05T02:36:52,132 [INFO ] W-9000-my_model_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - result=[METRICS]PredictionTime.Milliseconds:61.69|#ModelName:my_model,Level:Model|#type:GAUGE|#hostname:pupu-Stvullinc,1746394612,9b1b7750-a40a-4754-ab0b-b0f0ef0794ea, pattern=[METRICS]
2025-05-05T02:36:52,132 [INFO ] W-9000-my_model_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - result=[METRICS]PredictionTime.Milliseconds:61.69|#ModelName:my_model,Level:Model|#type:GAUGE|#hostname:pupu-Stvullinc,1746394612,9b1b7750-a40a-4754-ab0b-b0f0ef0794ea, pattern=[METRICS]
2025-05-05T02:36:52,132 [INFO ] W-9000-my_model_1.0 ACCESS_LOG - /127.0.0.1:49442 "POST /predictions/my_model HTTP/1.1" 200 66
2025-05-05T02:36:52,133 [INFO ] W-9000-my_model_1.0-stdout MODEL_METRICS - PredictionTime.ms:61.69|#ModelName:my_model,Level:Model|#hostname:pupu-Stvullinc,requestID:9b1b7750-a40a-4754-ab0b-b0f0ef0794ea,timestamp:1746394612
2025-05-05T02:36:52,133 [INFO ] W-9000-my_model_1.0 TS_METRICS - Requests2XX.Count:1.0|#Level:Host|#hostname:pupu-Stvullinc,timestamp:1746394612
2025-05-05T02:36:52,133 [INFO ] W-9000-my_model_1.0 TS_METRICS - ts_inference_latency_microseconds.Microseconds:65238.319|#model_name:my_model,model_version:default|#hostname:pupu-Stvullinc,timestamp:1746394612
2025-05-05T02:36:52,133 [INFO ] W-9000-my_model_1.0 TS_METRICS - ts_queue_latency_microseconds.Microseconds:90.845|#model_name:my_model,model_version:default|#hostname:pupu-Stvullinc,timestamp:1746394612
2025-05-05T02:36:52,134 [DEBUG] W-9000-my_model_1.0 org.pytorch.serve.job.RestJob - Waiting time ns: 90845, Backend time ns: 66365780
2025-05-05T02:36:52,134 [DEBUG] W-9000-my_model_1.0 org.pytorch.serve.job.RestJob - Waiting time ns: 90845, Backend time ns: 66365780
2025-05-05T02:36:52,134 [INFO ] W-9000-my_model_1.0 TS_METRICS - QueueTime.Milliseconds:0.0|#Level:Host|#hostname:pupu-Stvullinc,timestamp:1746394612
2025-05-05T02:36:52,134 [INFO ] W-9000-my_model_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 64
2025-05-05T02:36:52,134 [INFO ] W-9000-my_model_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 64
2025-05-05T02:36:52,134 [INFO ] W-9000-my_model_1.0 TS_METRICS - WorkerThreadTime.Milliseconds:3.0|#Level:Host|#hostname:pupu-Stvullinc,timestamp:1746394612
2025-05-05T02:37:05,470 [INFO ] epollEventLoopGroup-3-3 TS_METRICS - ts_inference_requests_total.Count:1.0|#model_name:my_model,model_version:default|#hostname:pupu-Stvullinc,timestamp:1746394625
2025-05-05T02:37:05,471 [DEBUG] W-9000-my_model_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req.cmd PREDICT repeats 1 to backend at: 1746394625471
2025-05-05T02:37:05,471 [DEBUG] W-9000-my_model_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req.cmd PREDICT repeats 1 to backend at: 1746394625471
2025-05-05T02:37:05,471 [INFO ] W-9000-my_model_1.0 org.pytorch.serve.wlm.WorkerThread - Looping backend response at: 1746394625471
2025-05-05T02:37:05,471 [INFO ] W-9000-my_model_1.0 org.pytorch.serve.wlm.WorkerThread - Looping backend response at: 1746394625471
2025-05-05T02:37:05,473 [INFO ] W-9000-my_model_1.0-stdout MODEL_LOG - Backend received inference at: 1746394625
2025-05-05T02:37:05,512 [INFO ] W-9000-my_model_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - result=[METRICS]HandlerTime.Milliseconds:39.26|#ModelName:my_model,Level:Model|#type:GAUGE|#hostname:pupu-Stvullinc,1746394625,68fe00c4-cb31-473e-a4bd-6cbe6302c4cd, pattern=[METRICS]
2025-05-05T02:37:05,512 [INFO ] W-9000-my_model_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - result=[METRICS]HandlerTime.Milliseconds:39.26|#ModelName:my_model,Level:Model|#type:GAUGE|#hostname:pupu-Stvullinc,1746394625,68fe00c4-cb31-473e-a4bd-6cbe6302c4cd, pattern=[METRICS]
2025-05-05T02:37:05,513 [INFO ] W-9000-my_model_1.0 org.pytorch.serve.wlm.BatchAggregator - Sending response for jobId 68fe00c4-cb31-473e-a4bd-6cbe6302c4cd
2025-05-05T02:37:05,513 [INFO ] W-9000-my_model_1.0-stdout MODEL_METRICS - HandlerTime.ms:39.26|#ModelName:my_model,Level:Model|#hostname:pupu-Stvullinc,requestID:68fe00c4-cb31-473e-a4bd-6cbe6302c4cd,timestamp:1746394625
2025-05-05T02:37:05,513 [INFO ] W-9000-my_model_1.0 org.pytorch.serve.wlm.BatchAggregator - Sending response for jobId 68fe00c4-cb31-473e-a4bd-6cbe6302c4cd
2025-05-05T02:37:05,513 [INFO ] W-9000-my_model_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - result=[METRICS]PredictionTime.Milliseconds:39.41|#ModelName:my_model,Level:Model|#type:GAUGE|#hostname:pupu-Stvullinc,1746394625,68fe00c4-cb31-473e-a4bd-6cbe6302c4cd, pattern=[METRICS]
2025-05-05T02:37:05,513 [INFO ] W-9000-my_model_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - result=[METRICS]PredictionTime.Milliseconds:39.41|#ModelName:my_model,Level:Model|#type:GAUGE|#hostname:pupu-Stvullinc,1746394625,68fe00c4-cb31-473e-a4bd-6cbe6302c4cd, pattern=[METRICS]
2025-05-05T02:37:05,513 [INFO ] W-9000-my_model_1.0 ACCESS_LOG - /127.0.0.1:44422 "POST /predictions/my_model HTTP/1.1" 200 43
2025-05-05T02:37:05,513 [INFO ] W-9000-my_model_1.0-stdout MODEL_METRICS - PredictionTime.ms:39.41|#ModelName:my_model,Level:Model|#hostname:pupu-Stvullinc,requestID:68fe00c4-cb31-473e-a4bd-6cbe6302c4cd,timestamp:1746394625
2025-05-05T02:37:05,514 [INFO ] W-9000-my_model_1.0 TS_METRICS - Requests2XX.Count:1.0|#Level:Host|#hostname:pupu-Stvullinc,timestamp:1746394625
2025-05-05T02:37:05,514 [INFO ] W-9000-my_model_1.0 TS_METRICS - ts_inference_latency_microseconds.Microseconds:42143.376|#model_name:my_model,model_version:default|#hostname:pupu-Stvullinc,timestamp:1746394625
2025-05-05T02:37:05,514 [INFO ] W-9000-my_model_1.0 TS_METRICS - ts_queue_latency_microseconds.Microseconds:111.217|#model_name:my_model,model_version:default|#hostname:pupu-Stvullinc,timestamp:1746394625
2025-05-05T02:37:05,514 [DEBUG] W-9000-my_model_1.0 org.pytorch.serve.job.RestJob - Waiting time ns: 111217, Backend time ns: 43490569
2025-05-05T02:37:05,514 [DEBUG] W-9000-my_model_1.0 org.pytorch.serve.job.RestJob - Waiting time ns: 111217, Backend time ns: 43490569
2025-05-05T02:37:05,515 [INFO ] W-9000-my_model_1.0 TS_METRICS - QueueTime.Milliseconds:0.0|#Level:Host|#hostname:pupu-Stvullinc,timestamp:1746394625
2025-05-05T02:37:05,515 [INFO ] W-9000-my_model_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 42
2025-05-05T02:37:05,515 [INFO ] W-9000-my_model_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 42
2025-05-05T02:37:05,515 [INFO ] W-9000-my_model_1.0 TS_METRICS - WorkerThreadTime.Milliseconds:2.0|#Level:Host|#hostname:pupu-Stvullinc,timestamp:1746394625
2025-05-05T02:37:51,359 [INFO ] pool-3-thread-1 TS_METRICS - CPUUtilization.Percent:33.3|#Level:Host|#hostname:pupu-Stvullinc,timestamp:1746394671
2025-05-05T02:37:51,359 [INFO ] pool-3-thread-1 TS_METRICS - DiskAvailable.Gigabytes:75.25286102294922|#Level:Host|#hostname:pupu-Stvullinc,timestamp:1746394671
2025-05-05T02:37:51,360 [INFO ] pool-3-thread-1 TS_METRICS - DiskUsage.Gigabytes:62.928985595703125|#Level:Host|#hostname:pupu-Stvullinc,timestamp:1746394671
2025-05-05T02:37:51,360 [INFO ] pool-3-thread-1 TS_METRICS - DiskUtilization.Percent:45.5|#Level:Host|#hostname:pupu-Stvullinc,timestamp:1746394671
2025-05-05T02:37:51,360 [INFO ] pool-3-thread-1 TS_METRICS - GPUMemoryUtilization.Percent:8.92333984375|#Level:Host,DeviceId:0|#hostname:pupu-Stvullinc,timestamp:1746394671
2025-05-05T02:37:51,361 [INFO ] pool-3-thread-1 TS_METRICS - GPUMemoryUsed.Megabytes:731.0|#Level:Host,DeviceId:0|#hostname:pupu-Stvullinc,timestamp:1746394671
2025-05-05T02:37:51,361 [INFO ] pool-3-thread-1 TS_METRICS - GPUUtilization.Percent:10.0|#Level:Host,DeviceId:0|#hostname:pupu-Stvullinc,timestamp:1746394671
2025-05-05T02:37:51,361 [INFO ] pool-3-thread-1 TS_METRICS - MemoryAvailable.Megabytes:8388.3046875|#Level:Host|#hostname:pupu-Stvullinc,timestamp:1746394671
2025-05-05T02:37:51,362 [INFO ] pool-3-thread-1 TS_METRICS - MemoryUsed.Megabytes:7101.74609375|#Level:Host|#hostname:pupu-Stvullinc,timestamp:1746394671
2025-05-05T02:37:51,362 [INFO ] pool-3-thread-1 TS_METRICS - MemoryUtilization.Percent:47.4|#Level:Host|#hostname:pupu-Stvullinc,timestamp:1746394671
2025-05-05T02:38:51,307 [INFO ] pool-3-thread-1 TS_METRICS - CPUUtilization.Percent:0.0|#Level:Host|#hostname:pupu-Stvullinc,timestamp:1746394731
2025-05-05T02:38:51,307 [INFO ] pool-3-thread-1 TS_METRICS - DiskAvailable.Gigabytes:75.24770736694336|#Level:Host|#hostname:pupu-Stvullinc,timestamp:1746394731
2025-05-05T02:38:51,308 [INFO ] pool-3-thread-1 TS_METRICS - DiskUsage.Gigabytes:62.934139251708984|#Level:Host|#hostname:pupu-Stvullinc,timestamp:1746394731
2025-05-05T02:38:51,308 [INFO ] pool-3-thread-1 TS_METRICS - DiskUtilization.Percent:45.5|#Level:Host|#hostname:pupu-Stvullinc,timestamp:1746394731
2025-05-05T02:38:51,308 [INFO ] pool-3-thread-1 TS_METRICS - GPUMemoryUtilization.Percent:9.19189453125|#Level:Host,DeviceId:0|#hostname:pupu-Stvullinc,timestamp:1746394731
2025-05-05T02:38:51,308 [INFO ] pool-3-thread-1 TS_METRICS - GPUMemoryUsed.Megabytes:753.0|#Level:Host,DeviceId:0|#hostname:pupu-Stvullinc,timestamp:1746394731
2025-05-05T02:38:51,308 [INFO ] pool-3-thread-1 TS_METRICS - GPUUtilization.Percent:42.0|#Level:Host,DeviceId:0|#hostname:pupu-Stvullinc,timestamp:1746394731
2025-05-05T02:38:51,309 [INFO ] pool-3-thread-1 TS_METRICS - MemoryAvailable.Megabytes:8410.0546875|#Level:Host|#hostname:pupu-Stvullinc,timestamp:1746394731
2025-05-05T02:38:51,309 [INFO ] pool-3-thread-1 TS_METRICS - MemoryUsed.Megabytes:7075.5625|#Level:Host|#hostname:pupu-Stvullinc,timestamp:1746394731
2025-05-05T02:38:51,309 [INFO ] pool-3-thread-1 TS_METRICS - MemoryUtilization.Percent:47.3|#Level:Host|#hostname:pupu-Stvullinc,timestamp:1746394731
2025-05-05T02:39:51,376 [INFO ] pool-3-thread-1 TS_METRICS - CPUUtilization.Percent:40.0|#Level:Host|#hostname:pupu-Stvullinc,timestamp:1746394791
2025-05-05T02:39:51,376 [INFO ] pool-3-thread-1 TS_METRICS - DiskAvailable.Gigabytes:75.24562454223633|#Level:Host|#hostname:pupu-Stvullinc,timestamp:1746394791
2025-05-05T02:39:51,376 [INFO ] pool-3-thread-1 TS_METRICS - DiskUsage.Gigabytes:62.936222076416016|#Level:Host|#hostname:pupu-Stvullinc,timestamp:1746394791
2025-05-05T02:39:51,377 [INFO ] pool-3-thread-1 TS_METRICS - DiskUtilization.Percent:45.5|#Level:Host|#hostname:pupu-Stvullinc,timestamp:1746394791
2025-05-05T02:39:51,377 [INFO ] pool-3-thread-1 TS_METRICS - GPUMemoryUtilization.Percent:9.0576171875|#Level:Host,DeviceId:0|#hostname:pupu-Stvullinc,timestamp:1746394791
2025-05-05T02:39:51,377 [INFO ] pool-3-thread-1 TS_METRICS - GPUMemoryUsed.Megabytes:742.0|#Level:Host,DeviceId:0|#hostname:pupu-Stvullinc,timestamp:1746394791
2025-05-05T02:39:51,377 [INFO ] pool-3-thread-1 TS_METRICS - GPUUtilization.Percent:8.0|#Level:Host,DeviceId:0|#hostname:pupu-Stvullinc,timestamp:1746394791
2025-05-05T02:39:51,377 [INFO ] pool-3-thread-1 TS_METRICS - MemoryAvailable.Megabytes:8337.8203125|#Level:Host|#hostname:pupu-Stvullinc,timestamp:1746394791
2025-05-05T02:39:51,377 [INFO ] pool-3-thread-1 TS_METRICS - MemoryUsed.Megabytes:7127.8203125|#Level:Host|#hostname:pupu-Stvullinc,timestamp:1746394791
2025-05-05T02:39:51,378 [INFO ] pool-3-thread-1 TS_METRICS - MemoryUtilization.Percent:47.8|#Level:Host|#hostname:pupu-Stvullinc,timestamp:1746394791
2025-05-05T02:40:51,435 [INFO ] pool-3-thread-1 TS_METRICS - CPUUtilization.Percent:50.0|#Level:Host|#hostname:pupu-Stvullinc,timestamp:1746394851
2025-05-05T02:40:51,435 [INFO ] pool-3-thread-1 TS_METRICS - DiskAvailable.Gigabytes:75.2451171875|#Level:Host|#hostname:pupu-Stvullinc,timestamp:1746394851
2025-05-05T02:40:51,435 [INFO ] pool-3-thread-1 TS_METRICS - DiskUsage.Gigabytes:62.936729431152344|#Level:Host|#hostname:pupu-Stvullinc,timestamp:1746394851
2025-05-05T02:40:51,436 [INFO ] pool-3-thread-1 TS_METRICS - DiskUtilization.Percent:45.5|#Level:Host|#hostname:pupu-Stvullinc,timestamp:1746394851
2025-05-05T02:40:51,436 [INFO ] pool-3-thread-1 TS_METRICS - GPUMemoryUtilization.Percent:9.0087890625|#Level:Host,DeviceId:0|#hostname:pupu-Stvullinc,timestamp:1746394851
2025-05-05T02:40:51,436 [INFO ] pool-3-thread-1 TS_METRICS - GPUMemoryUsed.Megabytes:738.0|#Level:Host,DeviceId:0|#hostname:pupu-Stvullinc,timestamp:1746394851
2025-05-05T02:40:51,436 [INFO ] pool-3-thread-1 TS_METRICS - GPUUtilization.Percent:6.0|#Level:Host,DeviceId:0|#hostname:pupu-Stvullinc,timestamp:1746394851
2025-05-05T02:40:51,436 [INFO ] pool-3-thread-1 TS_METRICS - MemoryAvailable.Megabytes:8419.703125|#Level:Host|#hostname:pupu-Stvullinc,timestamp:1746394851
2025-05-05T02:40:51,436 [INFO ] pool-3-thread-1 TS_METRICS - MemoryUsed.Megabytes:7069.79296875|#Level:Host|#hostname:pupu-Stvullinc,timestamp:1746394851
2025-05-05T02:40:51,436 [INFO ] pool-3-thread-1 TS_METRICS - MemoryUtilization.Percent:47.2|#Level:Host|#hostname:pupu-Stvullinc,timestamp:1746394851
2025-05-05T02:41:51,311 [INFO ] pool-3-thread-1 TS_METRICS - CPUUtilization.Percent:33.3|#Level:Host|#hostname:pupu-Stvullinc,timestamp:1746394911
2025-05-05T02:41:51,311 [INFO ] pool-3-thread-1 TS_METRICS - DiskAvailable.Gigabytes:75.24491882324219|#Level:Host|#hostname:pupu-Stvullinc,timestamp:1746394911
2025-05-05T02:41:51,312 [INFO ] pool-3-thread-1 TS_METRICS - DiskUsage.Gigabytes:62.936927795410156|#Level:Host|#hostname:pupu-Stvullinc,timestamp:1746394911
2025-05-05T02:41:51,312 [INFO ] pool-3-thread-1 TS_METRICS - DiskUtilization.Percent:45.5|#Level:Host|#hostname:pupu-Stvullinc,timestamp:1746394911
2025-05-05T02:41:51,312 [INFO ] pool-3-thread-1 TS_METRICS - GPUMemoryUtilization.Percent:9.09423828125|#Level:Host,DeviceId:0|#hostname:pupu-Stvullinc,timestamp:1746394911
2025-05-05T02:41:51,313 [INFO ] pool-3-thread-1 TS_METRICS - GPUMemoryUsed.Megabytes:745.0|#Level:Host,DeviceId:0|#hostname:pupu-Stvullinc,timestamp:1746394911
2025-05-05T02:41:51,313 [INFO ] pool-3-thread-1 TS_METRICS - GPUUtilization.Percent:34.0|#Level:Host,DeviceId:0|#hostname:pupu-Stvullinc,timestamp:1746394911
2025-05-05T02:41:51,313 [INFO ] pool-3-thread-1 TS_METRICS - MemoryAvailable.Megabytes:8331.94921875|#Level:Host|#hostname:pupu-Stvullinc,timestamp:1746394911
2025-05-05T02:41:51,313 [INFO ] pool-3-thread-1 TS_METRICS - MemoryUsed.Megabytes:7146.86328125|#Level:Host|#hostname:pupu-Stvullinc,timestamp:1746394911
2025-05-05T02:41:51,313 [INFO ] pool-3-thread-1 TS_METRICS - MemoryUtilization.Percent:47.8|#Level:Host|#hostname:pupu-Stvullinc,timestamp:1746394911
2025-05-05T02:42:51,376 [INFO ] pool-3-thread-1 TS_METRICS - CPUUtilization.Percent:0.0|#Level:Host|#hostname:pupu-Stvullinc,timestamp:1746394971
2025-05-05T02:42:51,376 [INFO ] pool-3-thread-1 TS_METRICS - DiskAvailable.Gigabytes:75.24481582641602|#Level:Host|#hostname:pupu-Stvullinc,timestamp:1746394971
2025-05-05T02:42:51,378 [INFO ] pool-3-thread-1 TS_METRICS - DiskUsage.Gigabytes:62.93703079223633|#Level:Host|#hostname:pupu-Stvullinc,timestamp:1746394971
2025-05-05T02:42:51,381 [INFO ] pool-3-thread-1 TS_METRICS - DiskUtilization.Percent:45.5|#Level:Host|#hostname:pupu-Stvullinc,timestamp:1746394971
2025-05-05T02:42:51,381 [INFO ] pool-3-thread-1 TS_METRICS - GPUMemoryUtilization.Percent:8.55712890625|#Level:Host,DeviceId:0|#hostname:pupu-Stvullinc,timestamp:1746394971
2025-05-05T02:42:51,381 [INFO ] pool-3-thread-1 TS_METRICS - GPUMemoryUsed.Megabytes:701.0|#Level:Host,DeviceId:0|#hostname:pupu-Stvullinc,timestamp:1746394971
2025-05-05T02:42:51,381 [INFO ] pool-3-thread-1 TS_METRICS - GPUUtilization.Percent:23.0|#Level:Host,DeviceId:0|#hostname:pupu-Stvullinc,timestamp:1746394971
2025-05-05T02:42:51,382 [INFO ] pool-3-thread-1 TS_METRICS - MemoryAvailable.Megabytes:8345.99609375|#Level:Host|#hostname:pupu-Stvullinc,timestamp:1746394971
2025-05-05T02:42:51,384 [INFO ] pool-3-thread-1 TS_METRICS - MemoryUsed.Megabytes:7125.5078125|#Level:Host|#hostname:pupu-Stvullinc,timestamp:1746394971
2025-05-05T02:42:51,384 [INFO ] pool-3-thread-1 TS_METRICS - MemoryUtilization.Percent:47.7|#Level:Host|#hostname:pupu-Stvullinc,timestamp:1746394971
2025-05-05T02:43:51,278 [INFO ] pool-3-thread-1 TS_METRICS - CPUUtilization.Percent:33.3|#Level:Host|#hostname:pupu-Stvullinc,timestamp:1746395031
2025-05-05T02:43:51,279 [INFO ] pool-3-thread-1 TS_METRICS - DiskAvailable.Gigabytes:75.24472045898438|#Level:Host|#hostname:pupu-Stvullinc,timestamp:1746395031
2025-05-05T02:43:51,279 [INFO ] pool-3-thread-1 TS_METRICS - DiskUsage.Gigabytes:62.93712615966797|#Level:Host|#hostname:pupu-Stvullinc,timestamp:1746395031
2025-05-05T02:43:51,279 [INFO ] pool-3-thread-1 TS_METRICS - DiskUtilization.Percent:45.5|#Level:Host|#hostname:pupu-Stvullinc,timestamp:1746395031
2025-05-05T02:43:51,279 [INFO ] pool-3-thread-1 TS_METRICS - GPUMemoryUtilization.Percent:8.65478515625|#Level:Host,DeviceId:0|#hostname:pupu-Stvullinc,timestamp:1746395031
2025-05-05T02:43:51,279 [INFO ] pool-3-thread-1 TS_METRICS - GPUMemoryUsed.Megabytes:709.0|#Level:Host,DeviceId:0|#hostname:pupu-Stvullinc,timestamp:1746395031
2025-05-05T02:43:51,280 [INFO ] pool-3-thread-1 TS_METRICS - GPUUtilization.Percent:45.0|#Level:Host,DeviceId:0|#hostname:pupu-Stvullinc,timestamp:1746395031
2025-05-05T02:43:51,280 [INFO ] pool-3-thread-1 TS_METRICS - MemoryAvailable.Megabytes:8332.8828125|#Level:Host|#hostname:pupu-Stvullinc,timestamp:1746395031
2025-05-05T02:43:51,280 [INFO ] pool-3-thread-1 TS_METRICS - MemoryUsed.Megabytes:7153.63671875|#Level:Host|#hostname:pupu-Stvullinc,timestamp:1746395031
2025-05-05T02:43:51,280 [INFO ] pool-3-thread-1 TS_METRICS - MemoryUtilization.Percent:47.8|#Level:Host|#hostname:pupu-Stvullinc,timestamp:1746395031
2025-05-05T02:44:51,300 [INFO ] pool-3-thread-1 TS_METRICS - CPUUtilization.Percent:0.0|#Level:Host|#hostname:pupu-Stvullinc,timestamp:1746395091
2025-05-05T02:44:51,300 [INFO ] pool-3-thread-1 TS_METRICS - DiskAvailable.Gigabytes:75.2444839477539|#Level:Host|#hostname:pupu-Stvullinc,timestamp:1746395091
2025-05-05T02:44:51,300 [INFO ] pool-3-thread-1 TS_METRICS - DiskUsage.Gigabytes:62.93736267089844|#Level:Host|#hostname:pupu-Stvullinc,timestamp:1746395091
2025-05-05T02:44:51,301 [INFO ] pool-3-thread-1 TS_METRICS - DiskUtilization.Percent:45.5|#Level:Host|#hostname:pupu-Stvullinc,timestamp:1746395091
2025-05-05T02:44:51,301 [INFO ] pool-3-thread-1 TS_METRICS - GPUMemoryUtilization.Percent:8.6669921875|#Level:Host,DeviceId:0|#hostname:pupu-Stvullinc,timestamp:1746395091
2025-05-05T02:44:51,301 [INFO ] pool-3-thread-1 TS_METRICS - GPUMemoryUsed.Megabytes:710.0|#Level:Host,DeviceId:0|#hostname:pupu-Stvullinc,timestamp:1746395091
2025-05-05T02:44:51,301 [INFO ] pool-3-thread-1 TS_METRICS - GPUUtilization.Percent:18.0|#Level:Host,DeviceId:0|#hostname:pupu-Stvullinc,timestamp:1746395091
2025-05-05T02:44:51,301 [INFO ] pool-3-thread-1 TS_METRICS - MemoryAvailable.Megabytes:8373.171875|#Level:Host|#hostname:pupu-Stvullinc,timestamp:1746395091
2025-05-05T02:44:51,302 [INFO ] pool-3-thread-1 TS_METRICS - MemoryUsed.Megabytes:7122.43359375|#Level:Host|#hostname:pupu-Stvullinc,timestamp:1746395091
2025-05-05T02:44:51,302 [INFO ] pool-3-thread-1 TS_METRICS - MemoryUtilization.Percent:47.5|#Level:Host|#hostname:pupu-Stvullinc,timestamp:1746395091
2025-05-05T02:45:51,303 [INFO ] pool-3-thread-1 TS_METRICS - CPUUtilization.Percent:0.0|#Level:Host|#hostname:pupu-Stvullinc,timestamp:1746395151
2025-05-05T02:45:51,304 [INFO ] pool-3-thread-1 TS_METRICS - DiskAvailable.Gigabytes:75.24442672729492|#Level:Host|#hostname:pupu-Stvullinc,timestamp:1746395151
2025-05-05T02:45:51,304 [INFO ] pool-3-thread-1 TS_METRICS - DiskUsage.Gigabytes:62.93741989135742|#Level:Host|#hostname:pupu-Stvullinc,timestamp:1746395151
2025-05-05T02:45:51,304 [INFO ] pool-3-thread-1 TS_METRICS - DiskUtilization.Percent:45.5|#Level:Host|#hostname:pupu-Stvullinc,timestamp:1746395151
2025-05-05T02:45:51,305 [INFO ] pool-3-thread-1 TS_METRICS - GPUMemoryUtilization.Percent:8.60595703125|#Level:Host,DeviceId:0|#hostname:pupu-Stvullinc,timestamp:1746395151
2025-05-05T02:45:51,305 [INFO ] pool-3-thread-1 TS_METRICS - GPUMemoryUsed.Megabytes:705.0|#Level:Host,DeviceId:0|#hostname:pupu-Stvullinc,timestamp:1746395151
2025-05-05T02:45:51,305 [INFO ] pool-3-thread-1 TS_METRICS - GPUUtilization.Percent:0.0|#Level:Host,DeviceId:0|#hostname:pupu-Stvullinc,timestamp:1746395151
2025-05-05T02:45:51,305 [INFO ] pool-3-thread-1 TS_METRICS - MemoryAvailable.Megabytes:8359.484375|#Level:Host|#hostname:pupu-Stvullinc,timestamp:1746395151
2025-05-05T02:45:51,305 [INFO ] pool-3-thread-1 TS_METRICS - MemoryUsed.Megabytes:7135.59375|#Level:Host|#hostname:pupu-Stvullinc,timestamp:1746395151
2025-05-05T02:45:51,306 [INFO ] pool-3-thread-1 TS_METRICS - MemoryUtilization.Percent:47.6|#Level:Host|#hostname:pupu-Stvullinc,timestamp:1746395151
2025-05-05T02:46:51,336 [INFO ] pool-3-thread-1 TS_METRICS - CPUUtilization.Percent:33.3|#Level:Host|#hostname:pupu-Stvullinc,timestamp:1746395211
2025-05-05T02:46:51,336 [INFO ] pool-3-thread-1 TS_METRICS - DiskAvailable.Gigabytes:75.24462127685547|#Level:Host|#hostname:pupu-Stvullinc,timestamp:1746395211
2025-05-05T02:46:51,337 [INFO ] pool-3-thread-1 TS_METRICS - DiskUsage.Gigabytes:62.937225341796875|#Level:Host|#hostname:pupu-Stvullinc,timestamp:1746395211
2025-05-05T02:46:51,337 [INFO ] pool-3-thread-1 TS_METRICS - DiskUtilization.Percent:45.5|#Level:Host|#hostname:pupu-Stvullinc,timestamp:1746395211
2025-05-05T02:46:51,337 [INFO ] pool-3-thread-1 TS_METRICS - GPUMemoryUtilization.Percent:8.8134765625|#Level:Host,DeviceId:0|#hostname:pupu-Stvullinc,timestamp:1746395211
2025-05-05T02:46:51,338 [INFO ] pool-3-thread-1 TS_METRICS - GPUMemoryUsed.Megabytes:722.0|#Level:Host,DeviceId:0|#hostname:pupu-Stvullinc,timestamp:1746395211
2025-05-05T02:46:51,338 [INFO ] pool-3-thread-1 TS_METRICS - GPUUtilization.Percent:1.0|#Level:Host,DeviceId:0|#hostname:pupu-Stvullinc,timestamp:1746395211
2025-05-05T02:46:51,338 [INFO ] pool-3-thread-1 TS_METRICS - MemoryAvailable.Megabytes:8416.6796875|#Level:Host|#hostname:pupu-Stvullinc,timestamp:1746395211
2025-05-05T02:46:51,338 [INFO ] pool-3-thread-1 TS_METRICS - MemoryUsed.Megabytes:7073.8203125|#Level:Host|#hostname:pupu-Stvullinc,timestamp:1746395211
2025-05-05T02:46:51,339 [INFO ] pool-3-thread-1 TS_METRICS - MemoryUtilization.Percent:47.3|#Level:Host|#hostname:pupu-Stvullinc,timestamp:1746395211
2025-05-05T02:47:51,394 [INFO ] pool-3-thread-1 TS_METRICS - CPUUtilization.Percent:80.0|#Level:Host|#hostname:pupu-Stvullinc,timestamp:1746395271
2025-05-05T02:47:51,394 [INFO ] pool-3-thread-1 TS_METRICS - DiskAvailable.Gigabytes:75.2445182800293|#Level:Host|#hostname:pupu-Stvullinc,timestamp:1746395271
2025-05-05T02:47:51,394 [INFO ] pool-3-thread-1 TS_METRICS - DiskUsage.Gigabytes:62.93732833862305|#Level:Host|#hostname:pupu-Stvullinc,timestamp:1746395271
2025-05-05T02:47:51,395 [INFO ] pool-3-thread-1 TS_METRICS - DiskUtilization.Percent:45.5|#Level:Host|#hostname:pupu-Stvullinc,timestamp:1746395271
2025-05-05T02:47:51,395 [INFO ] pool-3-thread-1 TS_METRICS - GPUMemoryUtilization.Percent:8.8623046875|#Level:Host,DeviceId:0|#hostname:pupu-Stvullinc,timestamp:1746395271
2025-05-05T02:47:51,395 [INFO ] pool-3-thread-1 TS_METRICS - GPUMemoryUsed.Megabytes:726.0|#Level:Host,DeviceId:0|#hostname:pupu-Stvullinc,timestamp:1746395271
2025-05-05T02:47:51,395 [INFO ] pool-3-thread-1 TS_METRICS - GPUUtilization.Percent:14.0|#Level:Host,DeviceId:0|#hostname:pupu-Stvullinc,timestamp:1746395271
2025-05-05T02:47:51,395 [INFO ] pool-3-thread-1 TS_METRICS - MemoryAvailable.Megabytes:8368.8515625|#Level:Host|#hostname:pupu-Stvullinc,timestamp:1746395271
2025-05-05T02:47:51,396 [INFO ] pool-3-thread-1 TS_METRICS - MemoryUsed.Megabytes:7125.77734375|#Level:Host|#hostname:pupu-Stvullinc,timestamp:1746395271
2025-05-05T02:47:51,396 [INFO ] pool-3-thread-1 TS_METRICS - MemoryUtilization.Percent:47.6|#Level:Host|#hostname:pupu-Stvullinc,timestamp:1746395271
2025-05-05T02:48:51,312 [INFO ] pool-3-thread-1 TS_METRICS - CPUUtilization.Percent:33.3|#Level:Host|#hostname:pupu-Stvullinc,timestamp:1746395331
2025-05-05T02:48:51,312 [INFO ] pool-3-thread-1 TS_METRICS - DiskAvailable.Gigabytes:75.24436950683594|#Level:Host|#hostname:pupu-Stvullinc,timestamp:1746395331
2025-05-05T02:48:51,313 [INFO ] pool-3-thread-1 TS_METRICS - DiskUsage.Gigabytes:62.937477111816406|#Level:Host|#hostname:pupu-Stvullinc,timestamp:1746395331
2025-05-05T02:48:51,313 [INFO ] pool-3-thread-1 TS_METRICS - DiskUtilization.Percent:45.5|#Level:Host|#hostname:pupu-Stvullinc,timestamp:1746395331
2025-05-05T02:48:51,313 [INFO ] pool-3-thread-1 TS_METRICS - GPUMemoryUtilization.Percent:8.77685546875|#Level:Host,DeviceId:0|#hostname:pupu-Stvullinc,timestamp:1746395331
2025-05-05T02:48:51,313 [INFO ] pool-3-thread-1 TS_METRICS - GPUMemoryUsed.Megabytes:719.0|#Level:Host,DeviceId:0|#hostname:pupu-Stvullinc,timestamp:1746395331
2025-05-05T02:48:51,313 [INFO ] pool-3-thread-1 TS_METRICS - GPUUtilization.Percent:1.0|#Level:Host,DeviceId:0|#hostname:pupu-Stvullinc,timestamp:1746395331
2025-05-05T02:48:51,313 [INFO ] pool-3-thread-1 TS_METRICS - MemoryAvailable.Megabytes:8335.625|#Level:Host|#hostname:pupu-Stvullinc,timestamp:1746395331
2025-05-05T02:48:51,314 [INFO ] pool-3-thread-1 TS_METRICS - MemoryUsed.Megabytes:7162.9296875|#Level:Host|#hostname:pupu-Stvullinc,timestamp:1746395331
2025-05-05T02:48:51,314 [INFO ] pool-3-thread-1 TS_METRICS - MemoryUtilization.Percent:47.8|#Level:Host|#hostname:pupu-Stvullinc,timestamp:1746395331
2025-05-05T02:49:51,321 [INFO ] pool-3-thread-1 TS_METRICS - CPUUtilization.Percent:25.0|#Level:Host|#hostname:pupu-Stvullinc,timestamp:1746395391
2025-05-05T02:49:51,321 [INFO ] pool-3-thread-1 TS_METRICS - DiskAvailable.Gigabytes:75.24424743652344|#Level:Host|#hostname:pupu-Stvullinc,timestamp:1746395391
2025-05-05T02:49:51,322 [INFO ] pool-3-thread-1 TS_METRICS - DiskUsage.Gigabytes:62.937599182128906|#Level:Host|#hostname:pupu-Stvullinc,timestamp:1746395391
2025-05-05T02:49:51,322 [INFO ] pool-3-thread-1 TS_METRICS - DiskUtilization.Percent:45.5|#Level:Host|#hostname:pupu-Stvullinc,timestamp:1746395391
2025-05-05T02:49:51,322 [INFO ] pool-3-thread-1 TS_METRICS - GPUMemoryUtilization.Percent:8.72802734375|#Level:Host,DeviceId:0|#hostname:pupu-Stvullinc,timestamp:1746395391
2025-05-05T02:49:51,322 [INFO ] pool-3-thread-1 TS_METRICS - GPUMemoryUsed.Megabytes:715.0|#Level:Host,DeviceId:0|#hostname:pupu-Stvullinc,timestamp:1746395391
2025-05-05T02:49:51,322 [INFO ] pool-3-thread-1 TS_METRICS - GPUUtilization.Percent:7.0|#Level:Host,DeviceId:0|#hostname:pupu-Stvullinc,timestamp:1746395391
2025-05-05T02:49:51,323 [INFO ] pool-3-thread-1 TS_METRICS - MemoryAvailable.Megabytes:8409.6953125|#Level:Host|#hostname:pupu-Stvullinc,timestamp:1746395391
2025-05-05T02:49:51,323 [INFO ] pool-3-thread-1 TS_METRICS - MemoryUsed.Megabytes:7088.87109375|#Level:Host|#hostname:pupu-Stvullinc,timestamp:1746395391
2025-05-05T02:49:51,323 [INFO ] pool-3-thread-1 TS_METRICS - MemoryUtilization.Percent:47.3|#Level:Host|#hostname:pupu-Stvullinc,timestamp:1746395391
2025-05-05T02:50:51,329 [INFO ] pool-3-thread-1 TS_METRICS - CPUUtilization.Percent:33.3|#Level:Host|#hostname:pupu-Stvullinc,timestamp:1746395451
2025-05-05T02:50:51,329 [INFO ] pool-3-thread-1 TS_METRICS - DiskAvailable.Gigabytes:75.24419021606445|#Level:Host|#hostname:pupu-Stvullinc,timestamp:1746395451
2025-05-05T02:50:51,330 [INFO ] pool-3-thread-1 TS_METRICS - DiskUsage.Gigabytes:62.93765640258789|#Level:Host|#hostname:pupu-Stvullinc,timestamp:1746395451
2025-05-05T02:50:51,330 [INFO ] pool-3-thread-1 TS_METRICS - DiskUtilization.Percent:45.5|#Level:Host|#hostname:pupu-Stvullinc,timestamp:1746395451
2025-05-05T02:50:51,330 [INFO ] pool-3-thread-1 TS_METRICS - GPUMemoryUtilization.Percent:8.72802734375|#Level:Host,DeviceId:0|#hostname:pupu-Stvullinc,timestamp:1746395451
2025-05-05T02:50:51,330 [INFO ] pool-3-thread-1 TS_METRICS - GPUMemoryUsed.Megabytes:715.0|#Level:Host,DeviceId:0|#hostname:pupu-Stvullinc,timestamp:1746395451
2025-05-05T02:50:51,330 [INFO ] pool-3-thread-1 TS_METRICS - GPUUtilization.Percent:15.0|#Level:Host,DeviceId:0|#hostname:pupu-Stvullinc,timestamp:1746395451
2025-05-05T02:50:51,331 [INFO ] pool-3-thread-1 TS_METRICS - MemoryAvailable.Megabytes:8386.3203125|#Level:Host|#hostname:pupu-Stvullinc,timestamp:1746395451
2025-05-05T02:50:51,331 [INFO ] pool-3-thread-1 TS_METRICS - MemoryUsed.Megabytes:7112.2421875|#Level:Host|#hostname:pupu-Stvullinc,timestamp:1746395451
2025-05-05T02:50:51,331 [INFO ] pool-3-thread-1 TS_METRICS - MemoryUtilization.Percent:47.5|#Level:Host|#hostname:pupu-Stvullinc,timestamp:1746395451
2025-05-05T02:51:51,292 [INFO ] pool-3-thread-1 TS_METRICS - CPUUtilization.Percent:0.0|#Level:Host|#hostname:pupu-Stvullinc,timestamp:1746395511
2025-05-05T02:51:51,292 [INFO ] pool-3-thread-1 TS_METRICS - DiskAvailable.Gigabytes:75.24401092529297|#Level:Host|#hostname:pupu-Stvullinc,timestamp:1746395511
2025-05-05T02:51:51,293 [INFO ] pool-3-thread-1 TS_METRICS - DiskUsage.Gigabytes:62.937835693359375|#Level:Host|#hostname:pupu-Stvullinc,timestamp:1746395511
2025-05-05T02:51:51,293 [INFO ] pool-3-thread-1 TS_METRICS - DiskUtilization.Percent:45.5|#Level:Host|#hostname:pupu-Stvullinc,timestamp:1746395511
2025-05-05T02:51:51,293 [INFO ] pool-3-thread-1 TS_METRICS - GPUMemoryUtilization.Percent:8.59375|#Level:Host,DeviceId:0|#hostname:pupu-Stvullinc,timestamp:1746395511
2025-05-05T02:51:51,293 [INFO ] pool-3-thread-1 TS_METRICS - GPUMemoryUsed.Megabytes:704.0|#Level:Host,DeviceId:0|#hostname:pupu-Stvullinc,timestamp:1746395511
2025-05-05T02:51:51,293 [INFO ] pool-3-thread-1 TS_METRICS - GPUUtilization.Percent:1.0|#Level:Host,DeviceId:0|#hostname:pupu-Stvullinc,timestamp:1746395511
2025-05-05T02:51:51,294 [INFO ] pool-3-thread-1 TS_METRICS - MemoryAvailable.Megabytes:8442.76171875|#Level:Host|#hostname:pupu-Stvullinc,timestamp:1746395511
2025-05-05T02:51:51,294 [INFO ] pool-3-thread-1 TS_METRICS - MemoryUsed.Megabytes:7052.15234375|#Level:Host|#hostname:pupu-Stvullinc,timestamp:1746395511
2025-05-05T02:51:51,294 [INFO ] pool-3-thread-1 TS_METRICS - MemoryUtilization.Percent:47.1|#Level:Host|#hostname:pupu-Stvullinc,timestamp:1746395511
2025-05-05T02:52:51,365 [INFO ] pool-3-thread-1 TS_METRICS - CPUUtilization.Percent:0.0|#Level:Host|#hostname:pupu-Stvullinc,timestamp:1746395571
2025-05-05T02:52:51,366 [INFO ] pool-3-thread-1 TS_METRICS - DiskAvailable.Gigabytes:75.24392700195312|#Level:Host|#hostname:pupu-Stvullinc,timestamp:1746395571
2025-05-05T02:52:51,366 [INFO ] pool-3-thread-1 TS_METRICS - DiskUsage.Gigabytes:62.93791961669922|#Level:Host|#hostname:pupu-Stvullinc,timestamp:1746395571
2025-05-05T02:52:51,366 [INFO ] pool-3-thread-1 TS_METRICS - DiskUtilization.Percent:45.5|#Level:Host|#hostname:pupu-Stvullinc,timestamp:1746395571
2025-05-05T02:52:51,366 [INFO ] pool-3-thread-1 TS_METRICS - GPUMemoryUtilization.Percent:8.80126953125|#Level:Host,DeviceId:0|#hostname:pupu-Stvullinc,timestamp:1746395571
2025-05-05T02:52:51,366 [INFO ] pool-3-thread-1 TS_METRICS - GPUMemoryUsed.Megabytes:721.0|#Level:Host,DeviceId:0|#hostname:pupu-Stvullinc,timestamp:1746395571
2025-05-05T02:52:51,367 [INFO ] pool-3-thread-1 TS_METRICS - GPUUtilization.Percent:1.0|#Level:Host,DeviceId:0|#hostname:pupu-Stvullinc,timestamp:1746395571
2025-05-05T02:52:51,367 [INFO ] pool-3-thread-1 TS_METRICS - MemoryAvailable.Megabytes:8444.71484375|#Level:Host|#hostname:pupu-Stvullinc,timestamp:1746395571
2025-05-05T02:52:51,367 [INFO ] pool-3-thread-1 TS_METRICS - MemoryUsed.Megabytes:7048.078125|#Level:Host|#hostname:pupu-Stvullinc,timestamp:1746395571
2025-05-05T02:52:51,367 [INFO ] pool-3-thread-1 TS_METRICS - MemoryUtilization.Percent:47.1|#Level:Host|#hostname:pupu-Stvullinc,timestamp:1746395571
2025-05-05T02:53:51,370 [INFO ] pool-3-thread-1 TS_METRICS - CPUUtilization.Percent:0.0|#Level:Host|#hostname:pupu-Stvullinc,timestamp:1746395631
2025-05-05T02:53:51,370 [INFO ] pool-3-thread-1 TS_METRICS - DiskAvailable.Gigabytes:75.24259948730469|#Level:Host|#hostname:pupu-Stvullinc,timestamp:1746395631
2025-05-05T02:53:51,371 [INFO ] pool-3-thread-1 TS_METRICS - DiskUsage.Gigabytes:62.939247131347656|#Level:Host|#hostname:pupu-Stvullinc,timestamp:1746395631
2025-05-05T02:53:51,371 [INFO ] pool-3-thread-1 TS_METRICS - DiskUtilization.Percent:45.5|#Level:Host|#hostname:pupu-Stvullinc,timestamp:1746395631
2025-05-05T02:53:51,371 [INFO ] pool-3-thread-1 TS_METRICS - GPUMemoryUtilization.Percent:9.19189453125|#Level:Host,DeviceId:0|#hostname:pupu-Stvullinc,timestamp:1746395631
2025-05-05T02:53:51,371 [INFO ] pool-3-thread-1 TS_METRICS - GPUMemoryUsed.Megabytes:753.0|#Level:Host,DeviceId:0|#hostname:pupu-Stvullinc,timestamp:1746395631
2025-05-05T02:53:51,371 [INFO ] pool-3-thread-1 TS_METRICS - GPUUtilization.Percent:32.0|#Level:Host,DeviceId:0|#hostname:pupu-Stvullinc,timestamp:1746395631
2025-05-05T02:53:51,372 [INFO ] pool-3-thread-1 TS_METRICS - MemoryAvailable.Megabytes:8270.6953125|#Level:Host|#hostname:pupu-Stvullinc,timestamp:1746395631
2025-05-05T02:53:51,372 [INFO ] pool-3-thread-1 TS_METRICS - MemoryUsed.Megabytes:7188.86328125|#Level:Host|#hostname:pupu-Stvullinc,timestamp:1746395631
2025-05-05T02:53:51,372 [INFO ] pool-3-thread-1 TS_METRICS - MemoryUtilization.Percent:48.2|#Level:Host|#hostname:pupu-Stvullinc,timestamp:1746395631
2025-05-05T02:54:51,317 [INFO ] pool-3-thread-1 TS_METRICS - CPUUtilization.Percent:0.0|#Level:Host|#hostname:pupu-Stvullinc,timestamp:1746395691
2025-05-05T02:54:51,317 [INFO ] pool-3-thread-1 TS_METRICS - DiskAvailable.Gigabytes:75.24262237548828|#Level:Host|#hostname:pupu-Stvullinc,timestamp:1746395691
2025-05-05T02:54:51,318 [INFO ] pool-3-thread-1 TS_METRICS - DiskUsage.Gigabytes:62.93922424316406|#Level:Host|#hostname:pupu-Stvullinc,timestamp:1746395691
2025-05-05T02:54:51,318 [INFO ] pool-3-thread-1 TS_METRICS - DiskUtilization.Percent:45.5|#Level:Host|#hostname:pupu-Stvullinc,timestamp:1746395691
2025-05-05T02:54:51,318 [INFO ] pool-3-thread-1 TS_METRICS - GPUMemoryUtilization.Percent:9.2041015625|#Level:Host,DeviceId:0|#hostname:pupu-Stvullinc,timestamp:1746395691
2025-05-05T02:54:51,318 [INFO ] pool-3-thread-1 TS_METRICS - GPUMemoryUsed.Megabytes:754.0|#Level:Host,DeviceId:0|#hostname:pupu-Stvullinc,timestamp:1746395691
2025-05-05T02:54:51,318 [INFO ] pool-3-thread-1 TS_METRICS - GPUUtilization.Percent:2.0|#Level:Host,DeviceId:0|#hostname:pupu-Stvullinc,timestamp:1746395691
2025-05-05T02:54:51,318 [INFO ] pool-3-thread-1 TS_METRICS - MemoryAvailable.Megabytes:8214.34375|#Level:Host|#hostname:pupu-Stvullinc,timestamp:1746395691
2025-05-05T02:54:51,319 [INFO ] pool-3-thread-1 TS_METRICS - MemoryUsed.Megabytes:7229.26171875|#Level:Host|#hostname:pupu-Stvullinc,timestamp:1746395691
2025-05-05T02:54:51,319 [INFO ] pool-3-thread-1 TS_METRICS - MemoryUtilization.Percent:48.5|#Level:Host|#hostname:pupu-Stvullinc,timestamp:1746395691
2025-05-05T02:55:51,417 [INFO ] pool-3-thread-1 TS_METRICS - CPUUtilization.Percent:0.0|#Level:Host|#hostname:pupu-Stvullinc,timestamp:1746395751
2025-05-05T02:55:51,418 [INFO ] pool-3-thread-1 TS_METRICS - DiskAvailable.Gigabytes:75.24020385742188|#Level:Host|#hostname:pupu-Stvullinc,timestamp:1746395751
2025-05-05T02:55:51,419 [INFO ] pool-3-thread-1 TS_METRICS - DiskUsage.Gigabytes:62.94164276123047|#Level:Host|#hostname:pupu-Stvullinc,timestamp:1746395751
2025-05-05T02:55:51,419 [INFO ] pool-3-thread-1 TS_METRICS - DiskUtilization.Percent:45.5|#Level:Host|#hostname:pupu-Stvullinc,timestamp:1746395751
2025-05-05T02:55:51,419 [INFO ] pool-3-thread-1 TS_METRICS - GPUMemoryUtilization.Percent:9.24072265625|#Level:Host,DeviceId:0|#hostname:pupu-Stvullinc,timestamp:1746395751
2025-05-05T02:55:51,419 [INFO ] pool-3-thread-1 TS_METRICS - GPUMemoryUsed.Megabytes:757.0|#Level:Host,DeviceId:0|#hostname:pupu-Stvullinc,timestamp:1746395751
2025-05-05T02:55:51,419 [INFO ] pool-3-thread-1 TS_METRICS - GPUUtilization.Percent:34.0|#Level:Host,DeviceId:0|#hostname:pupu-Stvullinc,timestamp:1746395751
2025-05-05T02:55:51,419 [INFO ] pool-3-thread-1 TS_METRICS - MemoryAvailable.Megabytes:8061.23046875|#Level:Host|#hostname:pupu-Stvullinc,timestamp:1746395751
2025-05-05T02:55:51,419 [INFO ] pool-3-thread-1 TS_METRICS - MemoryUsed.Megabytes:7385.78125|#Level:Host|#hostname:pupu-Stvullinc,timestamp:1746395751
2025-05-05T02:55:51,419 [INFO ] pool-3-thread-1 TS_METRICS - MemoryUtilization.Percent:49.5|#Level:Host|#hostname:pupu-Stvullinc,timestamp:1746395751
2025-05-05T02:56:51,267 [INFO ] pool-3-thread-1 TS_METRICS - CPUUtilization.Percent:33.3|#Level:Host|#hostname:pupu-Stvullinc,timestamp:1746395811
2025-05-05T02:56:51,268 [INFO ] pool-3-thread-1 TS_METRICS - DiskAvailable.Gigabytes:75.24010848999023|#Level:Host|#hostname:pupu-Stvullinc,timestamp:1746395811
2025-05-05T02:56:51,268 [INFO ] pool-3-thread-1 TS_METRICS - DiskUsage.Gigabytes:62.94173812866211|#Level:Host|#hostname:pupu-Stvullinc,timestamp:1746395811
2025-05-05T02:56:51,268 [INFO ] pool-3-thread-1 TS_METRICS - DiskUtilization.Percent:45.5|#Level:Host|#hostname:pupu-Stvullinc,timestamp:1746395811
2025-05-05T02:56:51,268 [INFO ] pool-3-thread-1 TS_METRICS - GPUMemoryUtilization.Percent:9.521484375|#Level:Host,DeviceId:0|#hostname:pupu-Stvullinc,timestamp:1746395811
2025-05-05T02:56:51,268 [INFO ] pool-3-thread-1 TS_METRICS - GPUMemoryUsed.Megabytes:780.0|#Level:Host,DeviceId:0|#hostname:pupu-Stvullinc,timestamp:1746395811
2025-05-05T02:56:51,269 [INFO ] pool-3-thread-1 TS_METRICS - GPUUtilization.Percent:40.0|#Level:Host,DeviceId:0|#hostname:pupu-Stvullinc,timestamp:1746395811
2025-05-05T02:56:51,269 [INFO ] pool-3-thread-1 TS_METRICS - MemoryAvailable.Megabytes:8179.46875|#Level:Host|#hostname:pupu-Stvullinc,timestamp:1746395811
2025-05-05T02:56:51,269 [INFO ] pool-3-thread-1 TS_METRICS - MemoryUsed.Megabytes:7296.0078125|#Level:Host|#hostname:pupu-Stvullinc,timestamp:1746395811
2025-05-05T02:56:51,269 [INFO ] pool-3-thread-1 TS_METRICS - MemoryUtilization.Percent:48.7|#Level:Host|#hostname:pupu-Stvullinc,timestamp:1746395811
2025-05-05T02:57:51,334 [INFO ] pool-3-thread-1 TS_METRICS - CPUUtilization.Percent:50.0|#Level:Host|#hostname:pupu-Stvullinc,timestamp:1746395871
2025-05-05T02:57:51,334 [INFO ] pool-3-thread-1 TS_METRICS - DiskAvailable.Gigabytes:75.23791122436523|#Level:Host|#hostname:pupu-Stvullinc,timestamp:1746395871
2025-05-05T02:57:51,334 [INFO ] pool-3-thread-1 TS_METRICS - DiskUsage.Gigabytes:62.94393539428711|#Level:Host|#hostname:pupu-Stvullinc,timestamp:1746395871
2025-05-05T02:57:51,334 [INFO ] pool-3-thread-1 TS_METRICS - DiskUtilization.Percent:45.6|#Level:Host|#hostname:pupu-Stvullinc,timestamp:1746395871
2025-05-05T02:57:51,334 [INFO ] pool-3-thread-1 TS_METRICS - GPUMemoryUtilization.Percent:9.50927734375|#Level:Host,DeviceId:0|#hostname:pupu-Stvullinc,timestamp:1746395871
2025-05-05T02:57:51,334 [INFO ] pool-3-thread-1 TS_METRICS - GPUMemoryUsed.Megabytes:779.0|#Level:Host,DeviceId:0|#hostname:pupu-Stvullinc,timestamp:1746395871
2025-05-05T02:57:51,335 [INFO ] pool-3-thread-1 TS_METRICS - GPUUtilization.Percent:20.0|#Level:Host,DeviceId:0|#hostname:pupu-Stvullinc,timestamp:1746395871
2025-05-05T02:57:51,335 [INFO ] pool-3-thread-1 TS_METRICS - MemoryAvailable.Megabytes:8183.46875|#Level:Host|#hostname:pupu-Stvullinc,timestamp:1746395871
2025-05-05T02:57:51,335 [INFO ] pool-3-thread-1 TS_METRICS - MemoryUsed.Megabytes:7281.86328125|#Level:Host|#hostname:pupu-Stvullinc,timestamp:1746395871
2025-05-05T02:57:51,335 [INFO ] pool-3-thread-1 TS_METRICS - MemoryUtilization.Percent:48.7|#Level:Host|#hostname:pupu-Stvullinc,timestamp:1746395871
2025-05-05T02:58:51,316 [INFO ] pool-3-thread-1 TS_METRICS - CPUUtilization.Percent:0.0|#Level:Host|#hostname:pupu-Stvullinc,timestamp:1746395931
2025-05-05T02:58:51,316 [INFO ] pool-3-thread-1 TS_METRICS - DiskAvailable.Gigabytes:75.23564910888672|#Level:Host|#hostname:pupu-Stvullinc,timestamp:1746395931
2025-05-05T02:58:51,316 [INFO ] pool-3-thread-1 TS_METRICS - DiskUsage.Gigabytes:62.946197509765625|#Level:Host|#hostname:pupu-Stvullinc,timestamp:1746395931
2025-05-05T02:58:51,317 [INFO ] pool-3-thread-1 TS_METRICS - DiskUtilization.Percent:45.6|#Level:Host|#hostname:pupu-Stvullinc,timestamp:1746395931
2025-05-05T02:58:51,317 [INFO ] pool-3-thread-1 TS_METRICS - GPUMemoryUtilization.Percent:9.46044921875|#Level:Host,DeviceId:0|#hostname:pupu-Stvullinc,timestamp:1746395931
2025-05-05T02:58:51,317 [INFO ] pool-3-thread-1 TS_METRICS - GPUMemoryUsed.Megabytes:775.0|#Level:Host,DeviceId:0|#hostname:pupu-Stvullinc,timestamp:1746395931
2025-05-05T02:58:51,317 [INFO ] pool-3-thread-1 TS_METRICS - GPUUtilization.Percent:2.0|#Level:Host,DeviceId:0|#hostname:pupu-Stvullinc,timestamp:1746395931
2025-05-05T02:58:51,317 [INFO ] pool-3-thread-1 TS_METRICS - MemoryAvailable.Megabytes:8151.76171875|#Level:Host|#hostname:pupu-Stvullinc,timestamp:1746395931
2025-05-05T02:58:51,318 [INFO ] pool-3-thread-1 TS_METRICS - MemoryUsed.Megabytes:7301.203125|#Level:Host|#hostname:pupu-Stvullinc,timestamp:1746395931
2025-05-05T02:58:51,318 [INFO ] pool-3-thread-1 TS_METRICS - MemoryUtilization.Percent:48.9|#Level:Host|#hostname:pupu-Stvullinc,timestamp:1746395931
2025-05-05T02:59:51,296 [INFO ] pool-3-thread-1 TS_METRICS - CPUUtilization.Percent:33.3|#Level:Host|#hostname:pupu-Stvullinc,timestamp:1746395991
2025-05-05T02:59:51,296 [INFO ] pool-3-thread-1 TS_METRICS - DiskAvailable.Gigabytes:75.23564910888672|#Level:Host|#hostname:pupu-Stvullinc,timestamp:1746395991
2025-05-05T02:59:51,296 [INFO ] pool-3-thread-1 TS_METRICS - DiskUsage.Gigabytes:62.946197509765625|#Level:Host|#hostname:pupu-Stvullinc,timestamp:1746395991
2025-05-05T02:59:51,296 [INFO ] pool-3-thread-1 TS_METRICS - DiskUtilization.Percent:45.6|#Level:Host|#hostname:pupu-Stvullinc,timestamp:1746395991
2025-05-05T02:59:51,297 [INFO ] pool-3-thread-1 TS_METRICS - GPUMemoryUtilization.Percent:9.46044921875|#Level:Host,DeviceId:0|#hostname:pupu-Stvullinc,timestamp:1746395991
2025-05-05T02:59:51,297 [INFO ] pool-3-thread-1 TS_METRICS - GPUMemoryUsed.Megabytes:775.0|#Level:Host,DeviceId:0|#hostname:pupu-Stvullinc,timestamp:1746395991
2025-05-05T02:59:51,297 [INFO ] pool-3-thread-1 TS_METRICS - GPUUtilization.Percent:2.0|#Level:Host,DeviceId:0|#hostname:pupu-Stvullinc,timestamp:1746395991
2025-05-05T02:59:51,297 [INFO ] pool-3-thread-1 TS_METRICS - MemoryAvailable.Megabytes:8145.1875|#Level:Host|#hostname:pupu-Stvullinc,timestamp:1746395991
2025-05-05T02:59:51,297 [INFO ] pool-3-thread-1 TS_METRICS - MemoryUsed.Megabytes:7313.0|#Level:Host|#hostname:pupu-Stvullinc,timestamp:1746395991
2025-05-05T02:59:51,297 [INFO ] pool-3-thread-1 TS_METRICS - MemoryUtilization.Percent:49.0|#Level:Host|#hostname:pupu-Stvullinc,timestamp:1746395991
